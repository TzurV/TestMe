{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Building_an_E2E_Speech_Recognition_model_using_BERT_4decoding",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46d79c556c194349b4a41e5bdf291779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d3627ad36030446a892b6f058a25922f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7b4fcf8cbe11429a8724ad597c3b85eb",
              "IPY_MODEL_7ad2bb9b6ed448a1ab709cd9617853a7"
            ]
          }
        },
        "d3627ad36030446a892b6f058a25922f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b4fcf8cbe11429a8724ad597c3b85eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23f4bf35395c46658c303699eb72aae0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 346663984,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 346663984,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_964711bc8b7e4d4da643dfb9205d596a"
          }
        },
        "7ad2bb9b6ed448a1ab709cd9617853a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a31035dc9b34d1da02595beea74c66c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 331M/331M [00:40&lt;00:00, 8.58MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47a84ced24cc4969a4229d4485e03329"
          }
        },
        "23f4bf35395c46658c303699eb72aae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "964711bc8b7e4d4da643dfb9205d596a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a31035dc9b34d1da02595beea74c66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47a84ced24cc4969a4229d4485e03329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TzurV/TestMe/blob/BERT-based-Decoder/Building_an_E2E_Speech_Recognition_model_using_BERT_4decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ibD6bsRPl8Qu"
      },
      "source": [
        "# Building an end-to-end Speech Recognition model in PyTorch - [AssemblyAI](https://www.assemblyai.com/)\n",
        "\n",
        "Adding BERT based Decoder.\n",
        "Using Private Google drive as Drive for storing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVfrfGVmwR20",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhWH6jX-we8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "02b52c06-4768-4c7a-f56b-fce82e1d89e0"
      },
      "source": [
        "# mount Google Drive\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re \n",
        "\n",
        "# mounting is required once every time you start this notebook  \n",
        "drive.mount('/content/gdrive')\n",
        "filesRootDirectory = \"/content/gdrive/My Drive/Colab_files/CTCmodels/\"\n",
        "\n",
        "# Set to True if you want to check that you have access to google drive\n",
        "if True:\n",
        "    # output file \n",
        "    # on PC it is located in <Google Drive directory>\\Colab_files\\CTCmodels\n",
        "    testOutputFileName = filesRootDirectory + \"testOutputFile.txt\"\n",
        "    testOutputFile = open(testOutputFileName,\"w\") \n",
        "    FORMAT = '%Y%m%d%H%M%S'\n",
        "    TodayDate = datetime.now().strftime(FORMAT)\n",
        "    print(f\" look for {TodayDate} in output file {testOutputFileName}.\")\n",
        "    testOutputFile.write(TodayDate+'\\n')\n",
        "    testOutputFile.close()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            " look for 20200923165347 in output file /content/gdrive/My Drive/Colab_files/CTCmodels/testOutputFile.txt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q1fXgsDQmK09"
      },
      "source": [
        "## installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwfN8o17Bdp2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "5de2d6ad-6d17-4d43-bb1c-83787a6d50e8"
      },
      "source": [
        "!pip install torchaudio==0.4.0 torch==1.4.0 comet-ml==3.0.2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio==0.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/6e/bc/3ebc127162d27bed33dc914606f10117d106680baae7ce83603ea09985fd/torchaudio-0.4.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 753.4MB 15kB/s \n",
            "\u001b[?25hCollecting comet-ml==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/c6/fac88f43f2aa61a09fee4ffb769c73fe93fe7de75764246e70967d31da09/comet_ml-3.0.2-py3-none-any.whl (170kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 174kB 83kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (2.6.0)\n",
            "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\n",
            "Collecting comet-git-pure>=0.19.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/7a/483413046e48908986a0f9a1d8a917e1da46ae58e6ba16b2ac71b3adf8d7/comet_git_pure-0.19.16-py3-none-any.whl (409kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419kB 54.0MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.55.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 204kB 58.2MB/s \n",
            "\u001b[?25hCollecting netifaces>=0.10.7\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (7.352.0)\n",
            "Collecting wurlitzer>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/1e/52f4effa64a447c4ec0fb71222799e2ac32c55b4b6c1725fccdf6123146e/wurlitzer-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (2020.6.20)\n",
            "Collecting configobj; extra == \"ini\"\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Building wheels for collected packages: configobj\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34547 sha256=9ecf55a67ab4ec08a107adfcb7d67a7e075f721fa491430bb74c3d04a01322d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built configobj\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio, configobj, everett, comet-git-pure, websocket-client, netifaces, wurlitzer, comet-ml\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "Successfully installed comet-git-pure-0.19.16 comet-ml-3.0.2 configobj-5.0.6 everett-1.0.2 netifaces-0.10.9 torch-1.4.0 torchaudio-0.4.0 websocket-client-0.57.0 wurlitzer-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSKHvy8DmOCQ"
      },
      "source": [
        "## Setting up your data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RVJs4Bk8FjjO",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# The core class of Comet.ml is an Experiment, a specific run of a script that \n",
        "# generated a result such as training a model on a single set of hyper parameters. \n",
        "# An Experiment will automatically log scripts output (stdout/stderr), code, \n",
        "# and command line arguments on any script and for the supported libraries will\n",
        "#  also log hyper parameters, metrics and model configuration.\n",
        "from comet_ml import Experiment\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
        "    between two sequences. Informally, the levenshtein disctance is defined as\n",
        "    the minimum number of single-character edits (substitutions, insertions or\n",
        "    deletions) required to change one word into the other. We can naturally\n",
        "    extend the edits to word level when calculate levenshtein disctance for\n",
        "    two sentences.\n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in word-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Levenshtein distance and word number of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in char-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Levenshtein distance and length of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
        "    hypothesis text in word-level. WER is defined as:\n",
        "    .. math::\n",
        "        WER = (Sw + Dw + Iw) / Nw\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sw is the number of words subsituted,\n",
        "        Dw is the number of words deleted,\n",
        "        Iw is the number of words inserted,\n",
        "        Nw is the number of words in the reference\n",
        "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
        "    that empty items will be removed when splitting sentences by delimiter.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Word error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If word number of reference is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
        "    hypothesis text in char-level. CER is defined as:\n",
        "    .. math::\n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
        "    encoded to unicode. Please draw an attention that the leading and tailing\n",
        "    space characters will be truncated and multiple consecutive space\n",
        "    characters in a sentence will be replaced by one space character.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Character error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If the reference length is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        q 18\n",
        "        r 19\n",
        "        s 20\n",
        "        t 21\n",
        "        u 22\n",
        "        v 23\n",
        "        w 24\n",
        "        x 25\n",
        "        y 26\n",
        "        z 27\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html#torch-nn-utils-rnn-pad-sequence\n",
        "    # pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. \n",
        "    # batch_first (bool, optional) â€“ output will be in B x T x * if True, or in T x B x * otherwise\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4XdSlhAQnDEA"
      },
      "source": [
        "## The Model\n",
        "Base of of Deep Speech 2 with some personal improvements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "65H1-PCjm-FB",
        "colab": {}
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CuguNEzKnMOn"
      },
      "source": [
        "## The Training and Evaluating Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ydkqGeOwnPGY",
        "colab": {}
      },
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    with experiment.train():\n",
        "        for batch_idx, _data in enumerate(train_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "\n",
        "            experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
        "            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            iter_meter.step()\n",
        "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(spectrograms), data_len,\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with experiment.test():\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data \n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
        "    experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n",
        "    experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def main(learning_rate=5e-4, batch_size=20, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\",\n",
        "        experiment=Experiment(api_key='dummy_key', disabled=True)):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    experiment.log_parameters(hparams)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28).to(device)\n",
        "    # torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs\n",
        "    #Sets the learning rate of each parameter group according to the 1cycle learning rate policy. \n",
        "    #The 1cycle policy anneals the learning rate from an initial learning rate to \n",
        "    # some maximum learning rate and then from that maximum learning rate to some \n",
        "    # minimum learning rate much lower than the initial learning rate. \n",
        "    # This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    \n",
        "    iter_meter = IterMeter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
        "        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)\n",
        "\n",
        "    # return the final model\n",
        "    return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4qBGdkQSmW3a"
      },
      "source": [
        "## Setting up Comet\n",
        "If you have a comet account, fill in teh api key, project name and experiment name below. You can create an account at [comet.ml](comet.ml)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "edo8shRBFt4V",
        "colab": {}
      },
      "source": [
        "comet_api_key = \"\" # add your api key here\n",
        "project_name = \"speechrecognition\"\n",
        "experiment_name = \"speechrecognition-colab\"\n",
        "\n",
        "if comet_api_key:\n",
        "  experiment = Experiment(api_key=comet_api_key, project_name=project_name, parse_args=False)\n",
        "  experiment.set_name(experiment_name)\n",
        "  experiment.display()\n",
        "else:\n",
        "  experiment = Experiment(api_key='dummy_key', disabled=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HxRIb_WempDq"
      },
      "source": [
        "## GPU runtime\n",
        "If you are using a GPU runtime, this will let you know what GPU and how much memory is available. Adjust your batch_size depending on which GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nlUSuAJwlzo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "1b6759e7-556b-4630-ca10-c3026e05c456"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Sep 23 16:59:24 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vBT776XweV8R"
      },
      "source": [
        "## ORIGINAL code sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1LsgT-m7HA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saveModel(model, outputFileName):\n",
        "    ''' Save NN model into  a file ''' \n",
        "    print(f\"saving {outputFileName}\")\n",
        "    torch.save(model, outputFileName)\n",
        "\n",
        "# update save/load as recommended here  \n",
        "# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hXvlWZeVpXfX"
      },
      "source": [
        "## Train\n",
        "this will download the data on first run and may take a while. \n",
        "\n",
        "If you have Comet.ml setup, you can start seeing your progress in the comet cell above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rXAaegPIe6TF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7sRfxHNjiGw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "46d79c556c194349b4a41e5bdf291779",
            "d3627ad36030446a892b6f058a25922f",
            "7b4fcf8cbe11429a8724ad597c3b85eb",
            "7ad2bb9b6ed448a1ab709cd9617853a7",
            "23f4bf35395c46658c303699eb72aae0",
            "964711bc8b7e4d4da643dfb9205d596a",
            "2a31035dc9b34d1da02595beea74c66c",
            "47a84ced24cc4969a4229d4485e03329"
          ]
        },
        "outputId": "f2ebc22f-3413-476c-9d44-58b32143f842"
      },
      "source": [
        "# Phase I - to make sure things are working\n",
        "# short training cycle using the the test set for training and \n",
        "# evaluation \n",
        "learning_rate = 5e-4\n",
        "batch_size = 3\n",
        "epochs = 3\n",
        "libri_train_set = \"test-clean\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "model = main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set, experiment)\n",
        "\n",
        "modelFileName = filesRootDirectory + \"PhaseImodel.pt\"\n",
        "saveModel(model, modelFileName)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46d79c556c194349b4a41e5bdf291779",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=346663984.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "SpeechRecognitionModel(\n",
            "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (rescnn_layers): Sequential(\n",
            "    (0): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (2): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (birnn_layers): Sequential(\n",
            "    (0): BidirectionalGRU(\n",
            "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (2): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (3): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (4): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): GELU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "  )\n",
            ")\n",
            "Num Model Parameters 23705373\n",
            "Train Epoch: 1 [0/2620 (0%)]\tLoss: 11.188392\n",
            "Train Epoch: 1 [300/2620 (11%)]\tLoss: 2.930324\n",
            "Train Epoch: 1 [600/2620 (23%)]\tLoss: 3.043006\n",
            "Train Epoch: 1 [900/2620 (34%)]\tLoss: 2.895102\n",
            "Train Epoch: 1 [1200/2620 (46%)]\tLoss: 2.961194\n",
            "Train Epoch: 1 [1500/2620 (57%)]\tLoss: 2.829447\n",
            "Train Epoch: 1 [1800/2620 (69%)]\tLoss: 2.902519\n",
            "Train Epoch: 1 [2100/2620 (80%)]\tLoss: 2.889657\n",
            "Train Epoch: 1 [2400/2620 (92%)]\tLoss: 2.934847\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.9171, Average CER: 1.000000 Average WER: 1.0000\n",
            "\n",
            "Train Epoch: 2 [0/2620 (0%)]\tLoss: 2.992191\n",
            "Train Epoch: 2 [300/2620 (11%)]\tLoss: 2.918222\n",
            "Train Epoch: 2 [600/2620 (23%)]\tLoss: 2.842262\n",
            "Train Epoch: 2 [900/2620 (34%)]\tLoss: 2.899186\n",
            "Train Epoch: 2 [1200/2620 (46%)]\tLoss: 2.849150\n",
            "Train Epoch: 2 [1500/2620 (57%)]\tLoss: 2.861015\n",
            "Train Epoch: 2 [1800/2620 (69%)]\tLoss: 2.916411\n",
            "Train Epoch: 2 [2100/2620 (80%)]\tLoss: 2.935502\n",
            "Train Epoch: 2 [2400/2620 (92%)]\tLoss: 2.982823\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.9002, Average CER: 1.000000 Average WER: 1.0000\n",
            "\n",
            "Train Epoch: 3 [0/2620 (0%)]\tLoss: 2.922109\n",
            "Train Epoch: 3 [300/2620 (11%)]\tLoss: 2.937934\n",
            "Train Epoch: 3 [600/2620 (23%)]\tLoss: 3.011184\n",
            "Train Epoch: 3 [900/2620 (34%)]\tLoss: 2.882180\n",
            "Train Epoch: 3 [1200/2620 (46%)]\tLoss: 2.873926\n",
            "Train Epoch: 3 [1500/2620 (57%)]\tLoss: 2.859894\n",
            "Train Epoch: 3 [1800/2620 (69%)]\tLoss: 2.906968\n",
            "Train Epoch: 3 [2100/2620 (80%)]\tLoss: 2.776203\n",
            "Train Epoch: 3 [2400/2620 (92%)]\tLoss: 2.910212\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.8937, Average CER: 1.000000 Average WER: 1.0000\n",
            "\n",
            "saving /content/gdrive/My Drive/Colab_files/CTCmodels/PhaseImodel.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type SpeechRecognitionModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ResidualCNN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CNNLayerNorm. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type BidirectionalGRU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp4CICMOB4Sr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6889a44-3114-4cf0-ab19-bf9990f552ea"
      },
      "source": [
        "print(model.state_dict())\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('cnn.weight', tensor([[[[ 0.0084, -0.2196,  0.0903],\n",
            "          [ 0.0842, -0.2075, -0.0735],\n",
            "          [-0.2051,  0.0700, -0.1075]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2224,  0.2238,  0.0320],\n",
            "          [-0.1585, -0.2213, -0.0528],\n",
            "          [-0.1281,  0.1208, -0.3071]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3382, -0.2334,  0.2703],\n",
            "          [-0.0357,  0.1909,  0.1509],\n",
            "          [ 0.0084,  0.0515,  0.2456]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1130,  0.2005, -0.2184],\n",
            "          [ 0.2808,  0.1597,  0.1462],\n",
            "          [ 0.0674,  0.1826,  0.3353]]],\n",
            "\n",
            "\n",
            "        [[[-0.1104, -0.2125, -0.0998],\n",
            "          [-0.2318,  0.1414, -0.1868],\n",
            "          [-0.2262,  0.2307,  0.2675]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0431, -0.0517, -0.0604],\n",
            "          [ 0.1267, -0.3485, -0.3126],\n",
            "          [ 0.2877,  0.3457, -0.0085]]],\n",
            "\n",
            "\n",
            "        [[[-0.2720,  0.0278, -0.1200],\n",
            "          [ 0.0146,  0.1847,  0.1796],\n",
            "          [-0.0560,  0.0250,  0.1088]]],\n",
            "\n",
            "\n",
            "        [[[-0.2907, -0.1493, -0.0110],\n",
            "          [-0.2057, -0.1642,  0.1050],\n",
            "          [ 0.1779, -0.0323, -0.1618]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1982, -0.1690, -0.0601],\n",
            "          [ 0.3407,  0.0847,  0.0603],\n",
            "          [ 0.1252, -0.0602,  0.3071]]],\n",
            "\n",
            "\n",
            "        [[[-0.2977, -0.1161, -0.0851],\n",
            "          [-0.1494, -0.0993,  0.2009],\n",
            "          [ 0.1502, -0.0258,  0.2492]]],\n",
            "\n",
            "\n",
            "        [[[-0.2799, -0.2215,  0.1445],\n",
            "          [-0.0504, -0.3365, -0.2476],\n",
            "          [-0.2349,  0.2313,  0.1323]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2290, -0.2886, -0.3667],\n",
            "          [-0.2296,  0.0291, -0.0748],\n",
            "          [-0.2452,  0.2199, -0.1374]]],\n",
            "\n",
            "\n",
            "        [[[-0.2026,  0.1321, -0.1376],\n",
            "          [ 0.2244,  0.0763, -0.0110],\n",
            "          [ 0.0659, -0.2453, -0.2047]]],\n",
            "\n",
            "\n",
            "        [[[-0.1928, -0.2158,  0.0417],\n",
            "          [ 0.0601, -0.1388,  0.2263],\n",
            "          [-0.2258,  0.0393, -0.0986]]],\n",
            "\n",
            "\n",
            "        [[[-0.3294, -0.2483,  0.1227],\n",
            "          [ 0.1079, -0.2285, -0.1178],\n",
            "          [-0.3386,  0.1587,  0.2778]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1006, -0.1936,  0.1874],\n",
            "          [ 0.2755, -0.0266, -0.0419],\n",
            "          [ 0.0880,  0.0327, -0.1102]]],\n",
            "\n",
            "\n",
            "        [[[-0.3727, -0.2987, -0.0433],\n",
            "          [-0.0262,  0.2587, -0.0005],\n",
            "          [-0.1700, -0.4050, -0.3859]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1446,  0.2932, -0.0118],\n",
            "          [-0.2936,  0.1279,  0.2904],\n",
            "          [-0.2581, -0.1377, -0.0061]]],\n",
            "\n",
            "\n",
            "        [[[-0.1855, -0.3103,  0.1399],\n",
            "          [-0.3264, -0.1507, -0.2360],\n",
            "          [ 0.2550,  0.0568,  0.1536]]],\n",
            "\n",
            "\n",
            "        [[[-0.2164,  0.2804,  0.0382],\n",
            "          [-0.2316, -0.0815, -0.2758],\n",
            "          [-0.0218,  0.1782, -0.1531]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2588,  0.0921, -0.0557],\n",
            "          [ 0.0977, -0.0633, -0.0764],\n",
            "          [-0.2372,  0.1621,  0.1881]]],\n",
            "\n",
            "\n",
            "        [[[ 0.1597,  0.2478,  0.3663],\n",
            "          [ 0.3256, -0.2936,  0.0979],\n",
            "          [-0.0061,  0.1513, -0.0427]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0691, -0.3365,  0.1211],\n",
            "          [ 0.3169, -0.0671, -0.2701],\n",
            "          [-0.3151, -0.1160, -0.0505]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0571,  0.0691,  0.2760],\n",
            "          [-0.0561,  0.1121, -0.1550],\n",
            "          [ 0.3258,  0.1279, -0.0285]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2192,  0.0964,  0.0438],\n",
            "          [-0.0773,  0.0439,  0.0115],\n",
            "          [-0.1678,  0.0147,  0.0732]]],\n",
            "\n",
            "\n",
            "        [[[-0.1856,  0.0667, -0.1075],\n",
            "          [-0.3206, -0.2479, -0.3093],\n",
            "          [ 0.0808,  0.2142,  0.3063]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3071,  0.1241,  0.1711],\n",
            "          [-0.0526,  0.2893, -0.1134],\n",
            "          [-0.1056, -0.1185, -0.2243]]],\n",
            "\n",
            "\n",
            "        [[[-0.2660, -0.0802,  0.2612],\n",
            "          [-0.1894, -0.3268, -0.0901],\n",
            "          [ 0.2916,  0.0708, -0.0252]]],\n",
            "\n",
            "\n",
            "        [[[-0.0795,  0.2902, -0.3318],\n",
            "          [ 0.2389,  0.1338,  0.2482],\n",
            "          [-0.1846, -0.1099, -0.1600]]],\n",
            "\n",
            "\n",
            "        [[[ 0.3040, -0.0653, -0.3362],\n",
            "          [ 0.1816,  0.0575,  0.0447],\n",
            "          [-0.2512, -0.3492,  0.2808]]],\n",
            "\n",
            "\n",
            "        [[[-0.3204, -0.1380, -0.1230],\n",
            "          [ 0.1710, -0.3102, -0.2119],\n",
            "          [-0.2696, -0.2606,  0.1572]]],\n",
            "\n",
            "\n",
            "        [[[ 0.2027,  0.1439,  0.2560],\n",
            "          [-0.2544, -0.0389, -0.2986],\n",
            "          [-0.0408, -0.3231, -0.3585]]]], device='cuda:0')), ('cnn.bias', tensor([ 0.2088, -0.1463, -0.0261, -0.0974, -0.2624, -0.0328,  0.0487,  0.2648,\n",
            "        -0.1411,  0.3122, -0.1536,  0.2122,  0.2895,  0.0345, -0.1140,  0.2447,\n",
            "        -0.0963, -0.1415, -0.1157, -0.2703, -0.0328, -0.2691,  0.2043, -0.3187,\n",
            "        -0.2463,  0.0729,  0.2046,  0.3165, -0.2681,  0.2718,  0.2323, -0.2175],\n",
            "       device='cuda:0')), ('rescnn_layers.0.cnn1.weight', tensor([[[[ 0.0206, -0.0123,  0.0198],\n",
            "          [-0.0508, -0.0306, -0.0332],\n",
            "          [-0.0027, -0.0024,  0.0006]],\n",
            "\n",
            "         [[-0.0200,  0.0326,  0.0308],\n",
            "          [-0.0455, -0.0569, -0.0172],\n",
            "          [-0.0090, -0.0667, -0.0372]],\n",
            "\n",
            "         [[ 0.0192, -0.0378,  0.0656],\n",
            "          [ 0.0398,  0.0500, -0.0371],\n",
            "          [-0.0661, -0.0067,  0.0189]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0216, -0.0199,  0.0179],\n",
            "          [ 0.0127, -0.0337, -0.0580],\n",
            "          [-0.0258,  0.0235, -0.0209]],\n",
            "\n",
            "         [[-0.0322, -0.0362, -0.0427],\n",
            "          [-0.0140, -0.0205,  0.0153],\n",
            "          [ 0.0597, -0.0366, -0.0085]],\n",
            "\n",
            "         [[ 0.0314, -0.0302, -0.0031],\n",
            "          [-0.0216,  0.0381, -0.0142],\n",
            "          [-0.0148,  0.0156, -0.0708]]],\n",
            "\n",
            "\n",
            "        [[[-0.0215,  0.0566,  0.0358],\n",
            "          [-0.0016,  0.0661,  0.0588],\n",
            "          [-0.0470, -0.0322,  0.0084]],\n",
            "\n",
            "         [[-0.0451, -0.0553, -0.0165],\n",
            "          [-0.0580,  0.0506,  0.0330],\n",
            "          [-0.0713, -0.0149,  0.0476]],\n",
            "\n",
            "         [[-0.0305, -0.0293,  0.0293],\n",
            "          [-0.0310, -0.0482,  0.0335],\n",
            "          [ 0.0113,  0.0394, -0.0106]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0160,  0.0341,  0.0444],\n",
            "          [ 0.0210, -0.0319, -0.0313],\n",
            "          [-0.0461,  0.0185, -0.0723]],\n",
            "\n",
            "         [[-0.0162,  0.0743,  0.0177],\n",
            "          [ 0.0446, -0.0144, -0.0015],\n",
            "          [-0.0584, -0.0051, -0.0541]],\n",
            "\n",
            "         [[-0.0344,  0.0196,  0.0058],\n",
            "          [-0.0350,  0.0088, -0.0143],\n",
            "          [-0.0966,  0.0387, -0.0683]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0202,  0.0314, -0.0322],\n",
            "          [-0.0186,  0.0458,  0.0378],\n",
            "          [-0.0207, -0.0377, -0.0464]],\n",
            "\n",
            "         [[ 0.0430, -0.0202,  0.0402],\n",
            "          [ 0.0026,  0.0461,  0.0022],\n",
            "          [ 0.0171, -0.0400, -0.0314]],\n",
            "\n",
            "         [[-0.0241, -0.0427, -0.0448],\n",
            "          [ 0.0210,  0.0380, -0.0124],\n",
            "          [-0.0753,  0.0259,  0.0320]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0562,  0.0131,  0.0402],\n",
            "          [ 0.0155, -0.0583, -0.0416],\n",
            "          [-0.0468, -0.0806, -0.0150]],\n",
            "\n",
            "         [[ 0.0671, -0.0389, -0.0336],\n",
            "          [-0.0461,  0.0039, -0.0109],\n",
            "          [ 0.0208, -0.0485, -0.0706]],\n",
            "\n",
            "         [[ 0.0407,  0.0545, -0.0412],\n",
            "          [-0.0439, -0.0261, -0.0519],\n",
            "          [-0.0667,  0.0311,  0.0301]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0053, -0.0043, -0.0286],\n",
            "          [-0.0137,  0.0403,  0.0454],\n",
            "          [ 0.0068, -0.0467,  0.0333]],\n",
            "\n",
            "         [[-0.0112,  0.0005, -0.0242],\n",
            "          [ 0.0119,  0.0675, -0.0482],\n",
            "          [-0.0123, -0.0249,  0.0590]],\n",
            "\n",
            "         [[ 0.0194,  0.0577, -0.0127],\n",
            "          [-0.0043, -0.0423, -0.0449],\n",
            "          [-0.0543,  0.0028, -0.0207]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0229, -0.0365,  0.0296],\n",
            "          [-0.0338,  0.0163, -0.0301],\n",
            "          [-0.0223, -0.0534, -0.0230]],\n",
            "\n",
            "         [[-0.0454,  0.0158,  0.0304],\n",
            "          [-0.0328, -0.0415,  0.0194],\n",
            "          [ 0.0210, -0.0001, -0.0061]],\n",
            "\n",
            "         [[-0.0165, -0.0644,  0.0298],\n",
            "          [ 0.0512, -0.0091,  0.0534],\n",
            "          [-0.0147,  0.0635,  0.0557]]],\n",
            "\n",
            "\n",
            "        [[[-0.0224, -0.0114,  0.0400],\n",
            "          [-0.0104, -0.0557, -0.0290],\n",
            "          [ 0.0143,  0.0462,  0.0318]],\n",
            "\n",
            "         [[-0.0474, -0.0507,  0.0004],\n",
            "          [ 0.0524, -0.0496, -0.0297],\n",
            "          [ 0.0258, -0.0629,  0.0261]],\n",
            "\n",
            "         [[-0.0280,  0.0084,  0.0309],\n",
            "          [ 0.0677, -0.0475, -0.0702],\n",
            "          [ 0.0367, -0.0266,  0.0412]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0574, -0.0034,  0.0297],\n",
            "          [ 0.0617,  0.0248,  0.0346],\n",
            "          [ 0.0648, -0.0284, -0.0421]],\n",
            "\n",
            "         [[ 0.0217,  0.0391,  0.0210],\n",
            "          [-0.0478,  0.0293,  0.0314],\n",
            "          [-0.0044, -0.0637,  0.0086]],\n",
            "\n",
            "         [[ 0.0321, -0.0026,  0.0273],\n",
            "          [ 0.0086, -0.0500,  0.0289],\n",
            "          [-0.0635, -0.0369, -0.0285]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0093,  0.0026, -0.0234],\n",
            "          [ 0.0414, -0.0173, -0.0297],\n",
            "          [-0.0468, -0.0338, -0.0140]],\n",
            "\n",
            "         [[ 0.0140,  0.0531, -0.0531],\n",
            "          [ 0.0163,  0.0494, -0.0128],\n",
            "          [ 0.0332, -0.0336,  0.0025]],\n",
            "\n",
            "         [[-0.0245, -0.0036,  0.0428],\n",
            "          [-0.0318,  0.0499,  0.0409],\n",
            "          [-0.0219,  0.0072,  0.0150]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0598, -0.0076,  0.0080],\n",
            "          [-0.0597, -0.0305, -0.0043],\n",
            "          [ 0.0562,  0.0170, -0.0363]],\n",
            "\n",
            "         [[-0.0313,  0.0776, -0.0293],\n",
            "          [ 0.0039,  0.0325,  0.0175],\n",
            "          [-0.0461,  0.0437, -0.0413]],\n",
            "\n",
            "         [[ 0.0278, -0.0317,  0.0089],\n",
            "          [ 0.0312,  0.0056, -0.0256],\n",
            "          [-0.0284, -0.0091, -0.0084]]]], device='cuda:0')), ('rescnn_layers.0.cnn1.bias', tensor([-0.0554,  0.0455, -0.0085, -0.0378, -0.0059,  0.0435, -0.0070, -0.0382,\n",
            "         0.0297,  0.0327,  0.0237,  0.0022, -0.0397, -0.0294,  0.0479,  0.0338,\n",
            "        -0.0402,  0.0503, -0.0189, -0.0053,  0.0142, -0.0519,  0.0262, -0.0177,\n",
            "        -0.0462,  0.0137, -0.0364,  0.0276, -0.0324, -0.0310,  0.0218,  0.0099],\n",
            "       device='cuda:0')), ('rescnn_layers.0.cnn2.weight', tensor([[[[ 1.3343e-03, -5.8213e-02, -5.4904e-02],\n",
            "          [ 7.9445e-03, -1.0229e-02,  3.1213e-02],\n",
            "          [-3.7141e-02, -7.6503e-03,  1.1047e-02]],\n",
            "\n",
            "         [[-7.7079e-03, -3.5413e-02, -1.8445e-02],\n",
            "          [-3.7408e-02,  3.3309e-02, -7.3338e-02],\n",
            "          [-6.0324e-02,  3.4232e-02, -4.9968e-02]],\n",
            "\n",
            "         [[-2.5629e-02,  1.5328e-02,  4.8247e-02],\n",
            "          [-3.5379e-02,  2.5309e-02, -1.6005e-02],\n",
            "          [-2.1671e-02,  5.8985e-03, -1.5765e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.7191e-02, -1.2844e-02, -2.4366e-02],\n",
            "          [ 5.0628e-02, -2.5410e-02, -3.3114e-02],\n",
            "          [-1.6052e-02, -5.3166e-02,  5.2432e-02]],\n",
            "\n",
            "         [[-6.3435e-02,  3.0819e-02,  5.9449e-03],\n",
            "          [-6.7107e-02, -5.6506e-02, -4.8760e-02],\n",
            "          [ 7.2512e-03, -3.4247e-03, -2.4025e-02]],\n",
            "\n",
            "         [[-3.1394e-02, -2.5758e-02, -5.6329e-02],\n",
            "          [ 2.9729e-02,  3.6470e-02,  4.3170e-02],\n",
            "          [-4.9758e-02, -2.6492e-03,  3.3064e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.5534e-02,  2.1966e-02,  1.0181e-02],\n",
            "          [ 2.3486e-02, -2.3906e-02, -2.5429e-02],\n",
            "          [-2.8827e-02,  2.1536e-02, -2.5086e-02]],\n",
            "\n",
            "         [[-4.7158e-02, -5.0926e-04,  1.2202e-02],\n",
            "          [ 3.6007e-03, -2.5287e-03,  3.9814e-02],\n",
            "          [-1.2353e-02,  4.2408e-02, -3.1060e-02]],\n",
            "\n",
            "         [[-1.9800e-02,  5.0626e-02,  5.0434e-02],\n",
            "          [ 5.8305e-02,  7.4466e-02,  2.9055e-02],\n",
            "          [-3.3766e-02,  4.3495e-03,  6.2611e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.9548e-02, -4.4688e-02,  1.5107e-02],\n",
            "          [-5.5648e-02,  4.7510e-03, -3.7470e-02],\n",
            "          [-1.9818e-02,  2.7558e-02, -3.7618e-02]],\n",
            "\n",
            "         [[ 3.6777e-02, -3.3449e-02,  4.7127e-02],\n",
            "          [-5.0450e-02, -5.0166e-02,  5.2500e-02],\n",
            "          [ 6.0852e-03, -4.1647e-03, -3.1303e-02]],\n",
            "\n",
            "         [[-6.4393e-03,  4.9704e-02,  3.0446e-02],\n",
            "          [ 5.7850e-02, -6.7828e-03,  4.9070e-02],\n",
            "          [-3.1902e-02,  1.1460e-02, -3.2283e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.7347e-03, -6.3781e-02,  4.0868e-02],\n",
            "          [ 3.8992e-02, -1.2618e-02,  3.4500e-02],\n",
            "          [-5.7682e-02,  2.4125e-02,  4.9345e-03]],\n",
            "\n",
            "         [[ 5.8840e-02, -4.3478e-02, -1.1721e-02],\n",
            "          [ 5.8504e-02, -3.7306e-03,  3.7663e-02],\n",
            "          [-3.6443e-02,  1.9458e-03,  9.7095e-03]],\n",
            "\n",
            "         [[ 1.5034e-02,  4.8082e-03,  5.1344e-02],\n",
            "          [ 8.1925e-02,  1.6001e-02,  2.5379e-02],\n",
            "          [-1.6158e-02,  1.0305e-02,  3.5210e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.0769e-03, -1.1662e-02, -1.5252e-03],\n",
            "          [-5.2763e-03,  1.4317e-02, -5.4786e-02],\n",
            "          [-4.6101e-02, -7.9235e-03,  3.8816e-02]],\n",
            "\n",
            "         [[ 1.9378e-02,  9.3489e-03,  3.5471e-02],\n",
            "          [-1.4879e-02, -6.5165e-02, -3.3418e-02],\n",
            "          [-2.6310e-02, -2.6846e-02,  2.6765e-02]],\n",
            "\n",
            "         [[-3.4372e-03,  2.3094e-02, -9.1123e-03],\n",
            "          [-3.6312e-02,  8.5110e-02, -3.2165e-02],\n",
            "          [-4.3978e-02, -2.6634e-02,  5.5524e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.1974e-02, -2.4621e-02,  5.3749e-02],\n",
            "          [-2.1483e-02, -5.2193e-02,  2.3753e-02],\n",
            "          [ 1.7960e-02,  3.4157e-02, -3.9130e-02]],\n",
            "\n",
            "         [[-3.3481e-02, -4.9085e-02, -2.8296e-02],\n",
            "          [ 3.5636e-02,  3.4647e-03, -4.6753e-02],\n",
            "          [-6.0807e-02,  1.1526e-02, -4.4659e-02]],\n",
            "\n",
            "         [[ 1.3691e-03, -1.1049e-03, -2.4977e-02],\n",
            "          [-3.3071e-03, -3.0423e-02,  4.6090e-02],\n",
            "          [-3.5088e-02,  5.0654e-02, -4.3139e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0100e-03,  4.7778e-02, -4.0438e-02],\n",
            "          [ 3.6191e-02,  5.5983e-02,  9.7933e-03],\n",
            "          [ 8.2928e-02,  2.8027e-02, -3.1619e-02]],\n",
            "\n",
            "         [[ 2.2938e-02, -6.2685e-02,  2.8061e-02],\n",
            "          [ 2.5478e-03, -3.9644e-02,  8.7098e-03],\n",
            "          [-2.5077e-02, -8.2827e-03,  3.4963e-02]],\n",
            "\n",
            "         [[-4.3793e-02,  5.5903e-02, -1.0630e-02],\n",
            "          [ 2.2553e-02, -8.9708e-03,  3.8584e-02],\n",
            "          [-4.9996e-02,  1.2422e-02,  3.9285e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1724e-03, -4.7254e-02, -1.1377e-02],\n",
            "          [-5.6317e-02,  1.7750e-02,  1.5131e-02],\n",
            "          [-1.9365e-02, -1.7224e-04,  3.1734e-02]],\n",
            "\n",
            "         [[-1.5448e-02, -2.6460e-02, -2.8843e-02],\n",
            "          [ 5.2002e-02,  3.3656e-02,  5.5008e-02],\n",
            "          [-1.3527e-02, -4.0413e-02,  4.5004e-02]],\n",
            "\n",
            "         [[ 4.0597e-02, -3.6356e-02, -6.5471e-02],\n",
            "          [ 1.7287e-02,  4.1847e-02,  7.0452e-02],\n",
            "          [-2.7054e-02,  3.5176e-02, -2.1679e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3276e-02,  1.8081e-02, -6.5310e-02],\n",
            "          [-6.4925e-02, -5.4971e-02, -1.8627e-02],\n",
            "          [ 6.5839e-02, -4.5582e-03,  3.2214e-02]],\n",
            "\n",
            "         [[ 2.9878e-03, -1.8594e-02,  5.3130e-02],\n",
            "          [ 2.3717e-02,  2.2563e-02, -5.6334e-02],\n",
            "          [-5.8269e-02,  3.3498e-03, -4.9313e-02]],\n",
            "\n",
            "         [[ 2.9773e-02,  1.2711e-02, -1.9062e-02],\n",
            "          [-4.3224e-02,  3.6566e-02, -4.5604e-02],\n",
            "          [ 1.8607e-02,  3.1248e-02,  5.2956e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.3779e-02,  2.8113e-03,  2.4648e-02],\n",
            "          [-4.5090e-02, -2.9920e-02, -3.6031e-02],\n",
            "          [-2.6961e-02,  5.0344e-02,  5.0074e-02]],\n",
            "\n",
            "         [[ 1.8383e-02,  7.2345e-03, -5.0071e-02],\n",
            "          [-1.5546e-03,  5.2677e-02, -3.2863e-02],\n",
            "          [-1.4984e-02, -4.0159e-02,  3.9476e-02]],\n",
            "\n",
            "         [[ 2.8120e-02,  1.7428e-02, -1.0416e-03],\n",
            "          [-1.5621e-02, -5.0752e-03,  4.5157e-02],\n",
            "          [-3.5563e-02, -4.7862e-02,  2.4855e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.2658e-02, -6.1279e-02,  2.5249e-02],\n",
            "          [ 1.9618e-02, -2.3033e-02, -1.3408e-02],\n",
            "          [-3.4093e-02,  2.7377e-02, -4.3545e-02]],\n",
            "\n",
            "         [[ 2.2600e-02,  3.2585e-02, -9.5454e-05],\n",
            "          [-4.7868e-02,  2.7073e-02,  3.6437e-02],\n",
            "          [ 8.1182e-03,  4.2682e-03, -6.7576e-02]],\n",
            "\n",
            "         [[-6.0174e-03, -2.7597e-02,  1.8315e-02],\n",
            "          [ 2.2944e-03,  2.1662e-02,  3.4336e-02],\n",
            "          [ 9.8675e-03,  5.2522e-02,  1.4267e-02]]]], device='cuda:0')), ('rescnn_layers.0.cnn2.bias', tensor([ 0.0520,  0.0380, -0.0697,  0.0421, -0.0313, -0.0584, -0.0327,  0.0198,\n",
            "         0.0527,  0.0369, -0.0162,  0.0246, -0.0579, -0.0562,  0.0510, -0.0282,\n",
            "        -0.0260,  0.0450, -0.0038,  0.0264, -0.0181,  0.0249,  0.0155,  0.0170,\n",
            "        -0.0479,  0.0303,  0.0166,  0.0393,  0.0516,  0.0041,  0.0438, -0.0344],\n",
            "       device='cuda:0')), ('rescnn_layers.0.layer_norm1.layer_norm.weight', tensor([0.9774, 0.9908, 0.9900, 0.9977, 0.9448, 0.9780, 1.0018, 1.0059, 0.9876,\n",
            "        0.9858, 0.9842, 1.0167, 1.0312, 1.0089, 1.0122, 1.0344, 1.0314, 1.0449,\n",
            "        0.9919, 0.9764, 0.9660, 0.9662, 1.0259, 1.0561, 1.0329, 1.0106, 0.9734,\n",
            "        0.9656, 0.9539, 0.9888, 0.9750, 1.0108, 0.9627, 0.9807, 0.9898, 1.0315,\n",
            "        1.0265, 1.0007, 0.9813, 0.9939, 1.0297, 0.9933, 0.9990, 0.9865, 1.0090,\n",
            "        1.0252, 1.0290, 1.0498, 1.0115, 0.9939, 1.0115, 0.9919, 0.9918, 1.0296,\n",
            "        1.0445, 1.0336, 1.0143, 0.9846, 0.9972, 0.9899, 1.0069, 0.9885, 1.0133,\n",
            "        0.9946], device='cuda:0')), ('rescnn_layers.0.layer_norm1.layer_norm.bias', tensor([-0.0002, -0.0002, -0.0002, -0.0002, -0.0002, -0.0002,  0.0002,  0.0002,\n",
            "         0.0002, -0.0002,  0.0002, -0.0002,  0.0002, -0.0002,  0.0002,  0.0002,\n",
            "        -0.0002,  0.0002,  0.0002,  0.0002,  0.0002, -0.0002,  0.0002, -0.0002,\n",
            "         0.0002, -0.0002, -0.0002,  0.0002,  0.0002, -0.0002,  0.0002,  0.0002,\n",
            "         0.0002, -0.0002,  0.0002,  0.0002,  0.0002,  0.0002,  0.0002,  0.0002,\n",
            "        -0.0002, -0.0002,  0.0002, -0.0002, -0.0002, -0.0002, -0.0002, -0.0002,\n",
            "        -0.0002,  0.0002, -0.0002, -0.0002, -0.0002, -0.0002,  0.0002,  0.0002,\n",
            "         0.0002,  0.0002,  0.0002,  0.0002,  0.0002,  0.0002,  0.0002, -0.0002],\n",
            "       device='cuda:0')), ('rescnn_layers.0.layer_norm2.layer_norm.weight', tensor([0.9890, 0.9874, 0.9789, 0.9775, 0.9429, 0.9502, 0.9845, 0.9999, 0.9917,\n",
            "        0.9784, 0.9826, 0.9923, 1.0018, 1.0031, 1.0064, 1.0294, 1.0271, 1.0091,\n",
            "        0.9937, 0.9738, 0.9663, 0.9753, 0.9904, 1.0151, 1.0354, 1.0141, 0.9938,\n",
            "        0.9883, 0.9758, 0.9853, 0.9883, 0.9882, 0.9825, 0.9784, 0.9743, 0.9960,\n",
            "        1.0128, 1.0052, 0.9796, 0.9746, 0.9899, 0.9912, 0.9819, 0.9869, 0.9928,\n",
            "        1.0074, 1.0221, 1.0302, 1.0020, 1.0024, 0.9963, 0.9990, 0.9948, 1.0130,\n",
            "        1.0288, 1.0211, 1.0086, 0.9973, 0.9948, 0.9952, 1.0021, 1.0029, 1.0039,\n",
            "        1.0082], device='cuda:0')), ('rescnn_layers.0.layer_norm2.layer_norm.bias', tensor([ 0.0002,  0.0002,  0.0002, -0.0002, -0.0002, -0.0002, -0.0002,  0.0002,\n",
            "        -0.0002, -0.0002, -0.0002, -0.0002,  0.0002, -0.0002,  0.0002, -0.0002,\n",
            "         0.0002,  0.0002,  0.0002, -0.0002,  0.0002, -0.0002,  0.0002, -0.0002,\n",
            "        -0.0002,  0.0002, -0.0002,  0.0002,  0.0002, -0.0002, -0.0002,  0.0002,\n",
            "         0.0002, -0.0002, -0.0002, -0.0002, -0.0002,  0.0002, -0.0002,  0.0002,\n",
            "        -0.0002, -0.0002,  0.0002,  0.0002, -0.0002, -0.0002,  0.0002,  0.0002,\n",
            "        -0.0002,  0.0002,  0.0002, -0.0002,  0.0002, -0.0002,  0.0002,  0.0002,\n",
            "         0.0002, -0.0002,  0.0002,  0.0002,  0.0002,  0.0002,  0.0002, -0.0002],\n",
            "       device='cuda:0')), ('rescnn_layers.1.cnn1.weight', tensor([[[[ 0.0035,  0.0389, -0.0548],\n",
            "          [-0.0509,  0.0525, -0.0531],\n",
            "          [-0.0595, -0.0557,  0.0143]],\n",
            "\n",
            "         [[ 0.0025, -0.0406, -0.0106],\n",
            "          [-0.0487,  0.0332, -0.0378],\n",
            "          [-0.0245,  0.0045, -0.0280]],\n",
            "\n",
            "         [[-0.0271, -0.0552,  0.0321],\n",
            "          [ 0.0014,  0.0560, -0.0235],\n",
            "          [ 0.0149, -0.0499, -0.0386]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0427, -0.0146, -0.0373],\n",
            "          [-0.0132, -0.0555,  0.0044],\n",
            "          [ 0.0451, -0.0376, -0.0449]],\n",
            "\n",
            "         [[ 0.0010,  0.0080, -0.0337],\n",
            "          [ 0.0114,  0.0567, -0.0020],\n",
            "          [ 0.0050, -0.0595,  0.0063]],\n",
            "\n",
            "         [[-0.0216,  0.0394, -0.0087],\n",
            "          [ 0.0194, -0.0093, -0.0085],\n",
            "          [ 0.0448, -0.0215,  0.0266]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0622,  0.0045,  0.0525],\n",
            "          [ 0.0347, -0.0289,  0.0208],\n",
            "          [-0.0273, -0.0147, -0.0058]],\n",
            "\n",
            "         [[ 0.0444,  0.0177,  0.0117],\n",
            "          [ 0.0447,  0.0613, -0.0251],\n",
            "          [ 0.0671,  0.0077, -0.0472]],\n",
            "\n",
            "         [[ 0.0189,  0.0097,  0.0574],\n",
            "          [-0.0316, -0.0222,  0.0407],\n",
            "          [-0.0196,  0.0268,  0.0386]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0139, -0.0389,  0.0182],\n",
            "          [ 0.0232,  0.0410, -0.0372],\n",
            "          [ 0.0227,  0.0360,  0.0195]],\n",
            "\n",
            "         [[ 0.0098, -0.0164,  0.0036],\n",
            "          [ 0.0191, -0.0224,  0.0561],\n",
            "          [ 0.0460,  0.0373,  0.0319]],\n",
            "\n",
            "         [[ 0.0541,  0.0374,  0.0140],\n",
            "          [-0.0153, -0.0091,  0.0547],\n",
            "          [-0.0479,  0.0454,  0.0340]]],\n",
            "\n",
            "\n",
            "        [[[-0.0155,  0.0032, -0.0127],\n",
            "          [-0.0307,  0.0572,  0.0488],\n",
            "          [ 0.0301,  0.0019, -0.0058]],\n",
            "\n",
            "         [[ 0.0201,  0.0509, -0.0400],\n",
            "          [-0.0396,  0.0413, -0.0490],\n",
            "          [-0.0351,  0.0289, -0.0168]],\n",
            "\n",
            "         [[ 0.0641,  0.0370,  0.0186],\n",
            "          [ 0.0497,  0.0312, -0.0504],\n",
            "          [-0.0625, -0.0468, -0.0065]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0570,  0.0081, -0.0688],\n",
            "          [ 0.0040,  0.0552,  0.0322],\n",
            "          [ 0.0005, -0.0583, -0.0140]],\n",
            "\n",
            "         [[-0.0369,  0.0280,  0.0101],\n",
            "          [-0.0171,  0.0082, -0.0257],\n",
            "          [ 0.0534, -0.0071, -0.0522]],\n",
            "\n",
            "         [[ 0.0497,  0.0671, -0.0075],\n",
            "          [ 0.0387,  0.0040, -0.0076],\n",
            "          [-0.0299,  0.0160,  0.0378]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0325, -0.0228, -0.0438],\n",
            "          [ 0.0699,  0.0061,  0.0068],\n",
            "          [-0.0178, -0.0032, -0.0509]],\n",
            "\n",
            "         [[-0.0519, -0.0477, -0.0116],\n",
            "          [ 0.0461, -0.0015, -0.0289],\n",
            "          [ 0.0384, -0.0276, -0.0629]],\n",
            "\n",
            "         [[ 0.0357,  0.0483,  0.0254],\n",
            "          [-0.0649, -0.0097, -0.0057],\n",
            "          [-0.0592, -0.0454,  0.0400]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0331,  0.0092, -0.0616],\n",
            "          [-0.0539, -0.0246,  0.0401],\n",
            "          [ 0.0289, -0.0509,  0.0396]],\n",
            "\n",
            "         [[ 0.0731,  0.0020,  0.0250],\n",
            "          [ 0.0153,  0.0313,  0.0195],\n",
            "          [-0.0311,  0.0513, -0.0020]],\n",
            "\n",
            "         [[ 0.0572,  0.0271,  0.0106],\n",
            "          [ 0.0079, -0.0177,  0.0468],\n",
            "          [ 0.0399, -0.0414, -0.0244]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0146,  0.0127,  0.0142],\n",
            "          [ 0.0330, -0.0464, -0.0015],\n",
            "          [-0.0330,  0.0463,  0.0175]],\n",
            "\n",
            "         [[-0.0334, -0.0156,  0.0069],\n",
            "          [ 0.0302, -0.0345, -0.0217],\n",
            "          [ 0.0342,  0.0466,  0.0121]],\n",
            "\n",
            "         [[ 0.0142,  0.0300, -0.0324],\n",
            "          [ 0.0177,  0.0164,  0.0252],\n",
            "          [ 0.0440, -0.0179,  0.0199]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0103, -0.0420, -0.0291],\n",
            "          [ 0.0171, -0.0455,  0.0153],\n",
            "          [ 0.0538,  0.0577,  0.0275]],\n",
            "\n",
            "         [[ 0.0198,  0.0566, -0.0142],\n",
            "          [ 0.0166, -0.0541, -0.0076],\n",
            "          [ 0.0468, -0.0471, -0.0047]],\n",
            "\n",
            "         [[ 0.0289, -0.0120,  0.0064],\n",
            "          [ 0.0704,  0.0629, -0.0220],\n",
            "          [-0.0059, -0.0522, -0.0578]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0324,  0.0340, -0.0028],\n",
            "          [-0.0183,  0.0263, -0.0183],\n",
            "          [-0.0445, -0.0359, -0.0280]],\n",
            "\n",
            "         [[-0.0448,  0.0193, -0.0378],\n",
            "          [ 0.0439, -0.0320,  0.0383],\n",
            "          [-0.0045,  0.0436,  0.0405]],\n",
            "\n",
            "         [[-0.0341, -0.0284,  0.0007],\n",
            "          [-0.0556,  0.0336, -0.0238],\n",
            "          [-0.0112, -0.0106,  0.0082]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0048,  0.0501,  0.0562],\n",
            "          [-0.0119,  0.0469, -0.0107],\n",
            "          [-0.0763, -0.0381, -0.0006]],\n",
            "\n",
            "         [[-0.0463, -0.0190,  0.0012],\n",
            "          [-0.0025, -0.0732, -0.0359],\n",
            "          [-0.0673,  0.0124,  0.0328]],\n",
            "\n",
            "         [[-0.0626,  0.0049,  0.0062],\n",
            "          [ 0.0156,  0.0635, -0.0346],\n",
            "          [-0.0477,  0.0558, -0.0476]]]], device='cuda:0')), ('rescnn_layers.1.cnn1.bias', tensor([-0.0360, -0.0327, -0.0479, -0.0431, -0.0171, -0.0319, -0.0237,  0.0101,\n",
            "        -0.0202, -0.0481,  0.0547,  0.0097,  0.0331,  0.0093, -0.0519, -0.0149,\n",
            "        -0.0104,  0.0194,  0.0079, -0.0140, -0.0462, -0.0460, -0.0293, -0.0240,\n",
            "         0.0394, -0.0352, -0.0387,  0.0393, -0.0586,  0.0282,  0.0413,  0.0475],\n",
            "       device='cuda:0')), ('rescnn_layers.1.cnn2.weight', tensor([[[[ 0.0309,  0.0172,  0.0608],\n",
            "          [ 0.0290,  0.0185, -0.0471],\n",
            "          [ 0.0323,  0.0323, -0.0298]],\n",
            "\n",
            "         [[ 0.0479, -0.0417, -0.0128],\n",
            "          [-0.0218,  0.0468,  0.0323],\n",
            "          [ 0.0285,  0.0516,  0.0246]],\n",
            "\n",
            "         [[-0.0333,  0.0274, -0.0189],\n",
            "          [ 0.0318, -0.0097, -0.0191],\n",
            "          [ 0.0509, -0.0053, -0.0335]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0115,  0.0206,  0.0371],\n",
            "          [ 0.0431, -0.0136,  0.0413],\n",
            "          [ 0.0065, -0.0420, -0.0484]],\n",
            "\n",
            "         [[ 0.0076, -0.0018,  0.0500],\n",
            "          [-0.0122,  0.0424,  0.0272],\n",
            "          [ 0.0046,  0.0143, -0.0044]],\n",
            "\n",
            "         [[ 0.0561,  0.0566,  0.0284],\n",
            "          [-0.0359, -0.0145, -0.0186],\n",
            "          [ 0.0244, -0.0571, -0.0620]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0570, -0.0113, -0.0091],\n",
            "          [-0.0116, -0.0359, -0.0164],\n",
            "          [-0.0527, -0.0028,  0.0033]],\n",
            "\n",
            "         [[-0.0353, -0.0243,  0.0333],\n",
            "          [ 0.0039, -0.0377,  0.0526],\n",
            "          [ 0.0276, -0.0485, -0.0111]],\n",
            "\n",
            "         [[ 0.0436,  0.0063, -0.0502],\n",
            "          [ 0.0098,  0.0593,  0.0440],\n",
            "          [ 0.0393, -0.0174, -0.0465]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0130, -0.0094, -0.0568],\n",
            "          [ 0.0247,  0.0299, -0.0329],\n",
            "          [ 0.0511,  0.0432, -0.0330]],\n",
            "\n",
            "         [[ 0.0117,  0.0603, -0.0083],\n",
            "          [-0.0087, -0.0416,  0.0143],\n",
            "          [ 0.0014,  0.0513, -0.0509]],\n",
            "\n",
            "         [[-0.0565, -0.0430,  0.0183],\n",
            "          [-0.0552, -0.0381,  0.0129],\n",
            "          [ 0.0374, -0.0425, -0.0407]]],\n",
            "\n",
            "\n",
            "        [[[-0.0297,  0.0518, -0.0534],\n",
            "          [-0.0163,  0.0014,  0.0123],\n",
            "          [-0.0428, -0.0548,  0.0201]],\n",
            "\n",
            "         [[-0.0259, -0.0286, -0.0563],\n",
            "          [ 0.0316, -0.0565, -0.0140],\n",
            "          [-0.0272, -0.0018, -0.0473]],\n",
            "\n",
            "         [[-0.0431,  0.0388,  0.0089],\n",
            "          [ 0.0018,  0.0159,  0.0121],\n",
            "          [-0.0351,  0.0262,  0.0087]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0277, -0.0585,  0.0126],\n",
            "          [-0.0270, -0.0183,  0.0379],\n",
            "          [-0.0398, -0.0043, -0.0288]],\n",
            "\n",
            "         [[ 0.0470, -0.0527,  0.0047],\n",
            "          [ 0.0565,  0.0107, -0.0499],\n",
            "          [-0.0373, -0.0299,  0.0475]],\n",
            "\n",
            "         [[ 0.0484,  0.0058, -0.0145],\n",
            "          [-0.0021, -0.0201, -0.0233],\n",
            "          [-0.0245, -0.0296, -0.0127]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0460,  0.0404,  0.0196],\n",
            "          [ 0.0326,  0.0310, -0.0131],\n",
            "          [-0.0170,  0.0349,  0.0097]],\n",
            "\n",
            "         [[ 0.0432, -0.0035,  0.0370],\n",
            "          [ 0.0004,  0.0047,  0.0067],\n",
            "          [-0.0369,  0.0597, -0.0011]],\n",
            "\n",
            "         [[-0.0557,  0.0519,  0.0051],\n",
            "          [ 0.0126,  0.0400,  0.0430],\n",
            "          [ 0.0560, -0.0285, -0.0430]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0455,  0.0464,  0.0468],\n",
            "          [ 0.0051,  0.0232,  0.0243],\n",
            "          [ 0.0023, -0.0239, -0.0627]],\n",
            "\n",
            "         [[-0.0088,  0.0575, -0.0073],\n",
            "          [ 0.0158, -0.0083,  0.0238],\n",
            "          [ 0.0134,  0.0600, -0.0044]],\n",
            "\n",
            "         [[-0.0386,  0.0527, -0.0415],\n",
            "          [-0.0159, -0.0227, -0.0024],\n",
            "          [-0.0167,  0.0278, -0.0167]]],\n",
            "\n",
            "\n",
            "        [[[-0.0203,  0.0006, -0.0277],\n",
            "          [ 0.0468,  0.0670,  0.0540],\n",
            "          [-0.0325,  0.0096, -0.0440]],\n",
            "\n",
            "         [[ 0.0063, -0.0046,  0.0653],\n",
            "          [-0.0422, -0.0190,  0.0482],\n",
            "          [-0.0124,  0.0057,  0.0153]],\n",
            "\n",
            "         [[-0.0425,  0.0555, -0.0023],\n",
            "          [ 0.0481, -0.0528, -0.0233],\n",
            "          [ 0.0350, -0.0287,  0.0502]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0328, -0.0103, -0.0337],\n",
            "          [ 0.0266,  0.0117, -0.0534],\n",
            "          [-0.0131, -0.0395,  0.0381]],\n",
            "\n",
            "         [[-0.0426, -0.0124, -0.0156],\n",
            "          [-0.0432,  0.0626,  0.0263],\n",
            "          [ 0.0544,  0.0262, -0.0415]],\n",
            "\n",
            "         [[-0.0319,  0.0283,  0.0255],\n",
            "          [-0.0659, -0.0091,  0.0406],\n",
            "          [ 0.0344, -0.0184,  0.0105]]],\n",
            "\n",
            "\n",
            "        [[[-0.0096, -0.0503, -0.0518],\n",
            "          [ 0.0251, -0.0516,  0.0571],\n",
            "          [-0.0275,  0.0164, -0.0402]],\n",
            "\n",
            "         [[-0.0267, -0.0321, -0.0366],\n",
            "          [ 0.0683,  0.0643,  0.0126],\n",
            "          [-0.0214,  0.0404, -0.0210]],\n",
            "\n",
            "         [[-0.0219, -0.0333, -0.0009],\n",
            "          [ 0.0542,  0.0208,  0.0433],\n",
            "          [ 0.0369, -0.0281, -0.0305]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0620,  0.0045,  0.0108],\n",
            "          [ 0.0334,  0.0455,  0.0092],\n",
            "          [ 0.0225,  0.0380,  0.0268]],\n",
            "\n",
            "         [[ 0.0104,  0.0571,  0.0369],\n",
            "          [-0.0152,  0.0571,  0.0035],\n",
            "          [-0.0206,  0.0135,  0.0124]],\n",
            "\n",
            "         [[ 0.0289, -0.0237, -0.0159],\n",
            "          [-0.0242,  0.0012,  0.0290],\n",
            "          [ 0.0396, -0.0063, -0.0493]]]], device='cuda:0')), ('rescnn_layers.1.cnn2.bias', tensor([-0.0585, -0.0238,  0.0341,  0.0235,  0.0147,  0.0524, -0.0497, -0.0362,\n",
            "        -0.0473,  0.0406,  0.0141, -0.0310,  0.0320,  0.0271,  0.0528, -0.0109,\n",
            "        -0.0201, -0.0296, -0.0196,  0.0217,  0.0403, -0.0071, -0.0643,  0.0378,\n",
            "        -0.0284, -0.0291,  0.0118, -0.0065,  0.0323, -0.0514, -0.0399,  0.0236],\n",
            "       device='cuda:0')), ('rescnn_layers.1.layer_norm1.layer_norm.weight', tensor([1.0011, 0.9886, 0.9872, 0.9930, 0.9753, 0.9529, 0.9868, 1.0039, 0.9881,\n",
            "        0.9875, 0.9943, 1.0003, 1.0227, 1.0102, 1.0192, 1.0302, 1.0215, 1.0140,\n",
            "        0.9884, 0.9829, 0.9702, 0.9800, 0.9956, 1.0097, 1.0158, 1.0168, 1.0030,\n",
            "        0.9840, 0.9870, 0.9905, 0.9893, 0.9802, 0.9886, 0.9823, 0.9839, 0.9957,\n",
            "        1.0108, 1.0054, 0.9848, 0.9779, 0.9936, 0.9907, 0.9906, 0.9838, 0.9859,\n",
            "        1.0098, 1.0075, 1.0106, 1.0054, 1.0067, 0.9920, 0.9943, 0.9920, 1.0088,\n",
            "        1.0113, 1.0060, 1.0000, 1.0011, 0.9972, 0.9959, 1.0008, 0.9983, 0.9998,\n",
            "        1.0011], device='cuda:0')), ('rescnn_layers.1.layer_norm1.layer_norm.bias', tensor([ 0.0002, -0.0002, -0.0002,  0.0002,  0.0002, -0.0002,  0.0002, -0.0002,\n",
            "        -0.0002,  0.0002, -0.0002,  0.0002, -0.0002,  0.0002, -0.0002, -0.0002,\n",
            "        -0.0002, -0.0002,  0.0002,  0.0002, -0.0002,  0.0002, -0.0002, -0.0002,\n",
            "         0.0002, -0.0002,  0.0002,  0.0002, -0.0002, -0.0002, -0.0002, -0.0002,\n",
            "         0.0002,  0.0002, -0.0002, -0.0002,  0.0002,  0.0002,  0.0002,  0.0002,\n",
            "         0.0002,  0.0002,  0.0002,  0.0002, -0.0002,  0.0002, -0.0002, -0.0002,\n",
            "        -0.0002, -0.0002, -0.0002,  0.0002, -0.0002,  0.0002, -0.0002, -0.0002,\n",
            "        -0.0002, -0.0002,  0.0002, -0.0002,  0.0002, -0.0002, -0.0002,  0.0002],\n",
            "       device='cuda:0')), ('rescnn_layers.1.layer_norm2.layer_norm.weight', tensor([0.9926, 0.9937, 0.9901, 0.9879, 0.9652, 0.9561, 0.9607, 0.9788, 0.9818,\n",
            "        0.9796, 0.9871, 0.9783, 0.9850, 0.9926, 0.9996, 1.0070, 1.0029, 0.9840,\n",
            "        0.9748, 0.9688, 0.9572, 0.9584, 0.9867, 0.9888, 1.0059, 1.0091, 0.9910,\n",
            "        0.9760, 0.9747, 0.9716, 0.9774, 0.9758, 0.9729, 0.9713, 0.9610, 0.9772,\n",
            "        0.9913, 0.9817, 0.9647, 0.9628, 0.9823, 0.9841, 0.9761, 0.9662, 0.9766,\n",
            "        0.9846, 0.9978, 0.9897, 0.9795, 0.9947, 0.9895, 0.9802, 0.9852, 0.9941,\n",
            "        0.9999, 0.9914, 1.0001, 0.9916, 0.9959, 0.9944, 0.9888, 0.9912, 0.9820,\n",
            "        0.9642], device='cuda:0')), ('rescnn_layers.1.layer_norm2.layer_norm.bias', tensor([-2.6218e-04, -2.5295e-04,  3.5533e-04,  3.4744e-04, -2.5539e-04,\n",
            "        -2.7700e-04,  2.2723e-04,  1.9321e-04, -3.4835e-04, -2.9224e-04,\n",
            "         2.3446e-04,  2.1564e-04, -2.6666e-04,  2.3865e-04, -2.5695e-04,\n",
            "        -3.1495e-04,  2.1865e-04, -2.8763e-04,  2.3660e-04, -4.3100e-04,\n",
            "        -2.6715e-04, -3.1512e-04, -2.5560e-04,  2.3419e-04, -2.5650e-04,\n",
            "         4.3733e-05,  2.3844e-04, -2.6696e-04, -2.7737e-04,  2.0415e-04,\n",
            "         2.3926e-04,  2.0913e-04, -2.5112e-04,  2.2786e-04, -2.6815e-04,\n",
            "         2.4013e-04,  2.3808e-04,  2.3448e-04, -2.5976e-04, -2.7124e-04,\n",
            "         2.1267e-04,  2.0365e-04, -2.8126e-04, -2.7227e-04,  2.0422e-04,\n",
            "         2.0672e-04, -3.3819e-04,  1.5880e-04, -2.5755e-04, -2.7408e-04,\n",
            "        -3.4855e-03, -3.1449e-04,  2.2757e-04,  2.2717e-04, -2.5264e-04,\n",
            "        -2.6745e-04, -2.5047e-04,  2.5040e-04,  2.4820e-04,  5.4586e-04,\n",
            "        -2.4803e-04, -1.1408e-04, -2.4853e-04, -2.5381e-04], device='cuda:0')), ('rescnn_layers.2.cnn1.weight', tensor([[[[-1.2928e-02, -9.8057e-03,  3.8686e-02],\n",
            "          [-2.6192e-02, -6.6955e-02,  3.6736e-02],\n",
            "          [ 1.3316e-04,  2.2825e-02, -3.2121e-02]],\n",
            "\n",
            "         [[-2.6989e-03,  4.9853e-02,  3.0478e-02],\n",
            "          [-2.5177e-02, -2.5304e-02,  2.9055e-02],\n",
            "          [ 2.4059e-02,  3.2100e-02,  2.1934e-02]],\n",
            "\n",
            "         [[-2.2152e-02, -6.1172e-02, -3.1257e-02],\n",
            "          [-3.4324e-02, -5.5294e-02,  1.0789e-02],\n",
            "          [-3.5714e-02, -4.2994e-02,  2.1370e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.9805e-02, -4.5642e-02,  4.5986e-02],\n",
            "          [ 2.6474e-02, -4.6967e-02, -8.2765e-03],\n",
            "          [ 3.8003e-02, -3.3399e-02, -7.7604e-03]],\n",
            "\n",
            "         [[-5.8985e-03,  7.1688e-03, -1.0569e-02],\n",
            "          [-3.0523e-02, -6.0247e-03,  2.4932e-02],\n",
            "          [ 3.3462e-02,  2.5429e-02,  4.8993e-03]],\n",
            "\n",
            "         [[-2.8317e-02, -4.0018e-03, -2.7032e-02],\n",
            "          [-3.2476e-02,  2.1721e-03, -3.0921e-02],\n",
            "          [ 2.1194e-02, -1.9521e-02, -4.1031e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.6252e-02,  2.2225e-02,  3.4745e-02],\n",
            "          [ 1.6277e-02,  1.2064e-02,  5.8232e-02],\n",
            "          [ 4.8280e-02,  4.2389e-02,  5.7166e-02]],\n",
            "\n",
            "         [[-4.3318e-02,  5.9870e-02,  3.0910e-02],\n",
            "          [-5.2642e-02,  1.2793e-02,  3.7354e-02],\n",
            "          [ 4.6768e-02, -3.5779e-02, -2.0320e-02]],\n",
            "\n",
            "         [[ 3.9999e-02,  1.8933e-03,  1.7472e-02],\n",
            "          [-3.1284e-03,  1.5492e-02,  1.0344e-02],\n",
            "          [-4.9871e-02,  2.4337e-02,  4.2628e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.7722e-03,  2.5403e-02,  7.1779e-02],\n",
            "          [-4.1839e-02,  5.9261e-02,  2.0855e-02],\n",
            "          [-4.2720e-02, -3.2770e-02, -1.8859e-02]],\n",
            "\n",
            "         [[-2.4480e-02,  4.8706e-02,  6.5059e-02],\n",
            "          [ 2.5206e-02,  5.0122e-02, -2.0681e-02],\n",
            "          [ 5.3330e-03,  4.3715e-02,  7.0464e-02]],\n",
            "\n",
            "         [[-4.3538e-02,  2.7830e-02,  5.2115e-02],\n",
            "          [ 3.2421e-02,  3.6421e-02,  6.7939e-02],\n",
            "          [-7.7436e-03, -2.0525e-02,  6.7826e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8041e-02, -3.1027e-05, -1.0573e-02],\n",
            "          [ 1.0320e-02, -4.2811e-02, -3.1373e-02],\n",
            "          [ 3.7961e-02,  1.9403e-02, -2.9407e-02]],\n",
            "\n",
            "         [[ 1.2676e-02, -7.2635e-02, -4.7056e-02],\n",
            "          [-2.3210e-02,  4.4434e-02, -1.4471e-02],\n",
            "          [ 2.7365e-02, -3.8581e-02, -4.6813e-02]],\n",
            "\n",
            "         [[-2.0987e-02,  1.3871e-02, -2.3553e-02],\n",
            "          [ 1.6342e-02,  7.3378e-04,  4.4558e-02],\n",
            "          [-2.8009e-02,  1.4496e-03,  2.8701e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.9769e-02,  5.3329e-02,  2.0352e-02],\n",
            "          [-1.7805e-03, -4.9698e-02, -5.3164e-03],\n",
            "          [-6.6613e-02,  6.4120e-03, -1.7136e-02]],\n",
            "\n",
            "         [[-1.2591e-02, -2.0291e-02, -2.5279e-02],\n",
            "          [-2.2115e-02, -1.4261e-02,  2.0391e-02],\n",
            "          [ 5.5102e-02,  2.5811e-02, -2.2296e-02]],\n",
            "\n",
            "         [[-1.3319e-02, -5.3456e-02,  1.5399e-02],\n",
            "          [ 2.8002e-02,  3.9891e-02,  4.5099e-02],\n",
            "          [ 2.1557e-03, -3.2474e-02,  2.5097e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 5.0756e-02,  3.9245e-02, -1.1501e-02],\n",
            "          [-3.6462e-02,  4.6554e-02,  3.2192e-02],\n",
            "          [-5.5461e-03, -3.6654e-02,  2.2501e-03]],\n",
            "\n",
            "         [[ 4.9533e-03,  2.9058e-02, -9.2916e-03],\n",
            "          [ 4.9914e-02, -4.1384e-02, -2.9739e-02],\n",
            "          [ 7.8450e-03,  4.3412e-02, -3.3814e-02]],\n",
            "\n",
            "         [[-2.3266e-03,  4.8932e-02,  1.0403e-02],\n",
            "          [-4.6887e-02,  2.1001e-02, -1.6376e-02],\n",
            "          [-4.9741e-02, -3.0532e-02,  5.1150e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.6019e-02, -4.2767e-02,  2.7581e-02],\n",
            "          [-2.8017e-02, -4.0254e-02,  1.1303e-02],\n",
            "          [-1.9133e-02, -3.2280e-02, -1.4205e-02]],\n",
            "\n",
            "         [[ 3.2689e-02, -3.5258e-02,  4.0186e-02],\n",
            "          [-5.5877e-02,  3.7844e-02,  5.1286e-04],\n",
            "          [-6.1594e-02,  1.9648e-02, -5.6411e-02]],\n",
            "\n",
            "         [[ 9.2605e-04,  4.0482e-02,  2.1291e-03],\n",
            "          [ 4.9939e-02,  1.2819e-02,  1.0016e-02],\n",
            "          [-4.5422e-02,  2.3802e-02,  4.1573e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.3542e-02, -2.1986e-02, -6.4828e-03],\n",
            "          [ 2.2611e-02,  2.4274e-02,  8.0346e-03],\n",
            "          [ 3.9471e-02,  1.5965e-03, -2.9226e-02]],\n",
            "\n",
            "         [[ 4.9379e-02,  1.3359e-02, -6.6547e-03],\n",
            "          [-7.1712e-02,  2.7488e-02, -3.5626e-02],\n",
            "          [ 4.4674e-03,  3.7546e-02, -5.6345e-02]],\n",
            "\n",
            "         [[ 6.3472e-02,  5.8002e-02, -1.7863e-02],\n",
            "          [-2.2905e-02,  4.2436e-02, -7.3833e-02],\n",
            "          [ 2.0012e-02, -5.9767e-03, -3.9305e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8096e-02,  4.7539e-02,  5.1946e-02],\n",
            "          [-4.4421e-02,  4.9771e-02,  1.2026e-02],\n",
            "          [-6.2152e-02, -7.8864e-02, -5.6614e-02]],\n",
            "\n",
            "         [[ 6.3087e-02,  4.0352e-02,  5.2108e-02],\n",
            "          [ 1.3388e-02, -3.4611e-02, -4.0924e-02],\n",
            "          [-2.5290e-02, -2.7211e-02,  2.8781e-02]],\n",
            "\n",
            "         [[ 1.2301e-02, -3.7356e-02,  1.4689e-02],\n",
            "          [-7.1619e-03,  1.1848e-03, -5.1895e-02],\n",
            "          [ 6.1308e-02,  2.6610e-02,  4.9632e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6884e-02,  2.9471e-02,  1.1114e-02],\n",
            "          [-3.6071e-02, -4.5159e-02,  3.1789e-02],\n",
            "          [-4.7644e-02, -4.2547e-02,  1.6155e-02]],\n",
            "\n",
            "         [[ 3.4886e-02, -4.8803e-02, -4.0202e-02],\n",
            "          [ 3.9081e-02, -2.9099e-02, -2.0290e-02],\n",
            "          [-2.8145e-02, -1.5878e-02,  9.6092e-03]],\n",
            "\n",
            "         [[-3.4989e-02,  4.7151e-02, -3.2522e-02],\n",
            "          [-1.0962e-02,  6.7920e-02, -4.5670e-02],\n",
            "          [ 1.2069e-02,  3.7364e-03,  2.5696e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.2198e-02, -7.7252e-02,  5.8221e-03],\n",
            "          [ 1.7908e-02, -3.8182e-02,  2.0903e-02],\n",
            "          [ 1.2008e-02,  1.6381e-03, -5.4482e-02]],\n",
            "\n",
            "         [[-4.3509e-02,  3.0353e-02, -8.8490e-03],\n",
            "          [ 6.7147e-03, -4.5102e-02, -1.0788e-02],\n",
            "          [ 5.2631e-02,  1.3208e-02, -3.5879e-02]],\n",
            "\n",
            "         [[-4.6044e-02,  1.1123e-02, -6.7823e-02],\n",
            "          [ 3.4964e-02,  2.9674e-03, -5.2241e-02],\n",
            "          [-3.1138e-02,  4.9982e-04,  5.6311e-02]]]], device='cuda:0')), ('rescnn_layers.2.cnn1.bias', tensor([ 0.0233,  0.0355, -0.0512, -0.0166,  0.0051,  0.0087,  0.0434,  0.0229,\n",
            "        -0.0094, -0.0355,  0.0019, -0.0322,  0.0486, -0.0401,  0.0428,  0.0301,\n",
            "         0.0422,  0.0020,  0.0062, -0.0321, -0.0237,  0.0576, -0.0060, -0.0284,\n",
            "        -0.0498, -0.0387,  0.0386,  0.0019,  0.0279, -0.0462, -0.0068,  0.0199],\n",
            "       device='cuda:0')), ('rescnn_layers.2.cnn2.weight', tensor([[[[ 1.8086e-02,  5.6246e-03, -2.1275e-02],\n",
            "          [-3.8390e-02, -1.4449e-02, -1.3335e-02],\n",
            "          [-2.2483e-02,  2.2276e-02, -2.0888e-02]],\n",
            "\n",
            "         [[-4.5530e-02, -2.6136e-02, -5.6333e-02],\n",
            "          [ 1.6894e-02, -5.0334e-02, -3.3677e-03],\n",
            "          [-2.7800e-02, -3.1232e-02,  4.1380e-02]],\n",
            "\n",
            "         [[ 2.5624e-02, -4.9519e-03, -1.8038e-02],\n",
            "          [-1.6311e-02,  4.0766e-02,  5.0710e-02],\n",
            "          [ 4.0109e-02,  1.6299e-02, -4.2100e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4560e-03,  1.2885e-02, -1.8778e-02],\n",
            "          [ 1.2425e-02,  3.1966e-02, -4.9978e-02],\n",
            "          [ 5.2032e-02,  4.0300e-02, -1.3234e-02]],\n",
            "\n",
            "         [[ 3.1135e-02,  2.5389e-02,  3.9690e-03],\n",
            "          [-1.4777e-03, -3.6251e-02,  1.9289e-02],\n",
            "          [-1.7777e-02,  5.0397e-02,  8.7769e-04]],\n",
            "\n",
            "         [[-3.4957e-02, -8.5637e-03, -5.3422e-03],\n",
            "          [ 2.2352e-02,  3.2987e-02, -1.6083e-02],\n",
            "          [ 4.4425e-02, -1.6794e-03,  2.6889e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.6689e-02, -2.3363e-02, -3.8275e-02],\n",
            "          [ 4.7142e-02, -1.9060e-02,  1.9305e-02],\n",
            "          [-2.5831e-02, -5.0621e-02,  5.6689e-02]],\n",
            "\n",
            "         [[-9.7730e-03, -5.7227e-02, -3.6966e-02],\n",
            "          [ 3.1718e-02,  4.8977e-02,  3.7567e-02],\n",
            "          [-6.8995e-03, -3.0132e-02, -3.5911e-02]],\n",
            "\n",
            "         [[ 3.4211e-02, -4.2666e-02,  3.7686e-02],\n",
            "          [ 5.0180e-02, -4.6277e-02,  2.9971e-02],\n",
            "          [-5.3904e-02, -4.8650e-02, -1.5171e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.3682e-02,  2.4393e-03,  3.7165e-02],\n",
            "          [-1.4369e-02, -3.4378e-02, -8.0773e-03],\n",
            "          [-6.1346e-03,  3.6864e-02,  1.4493e-02]],\n",
            "\n",
            "         [[-1.1510e-02,  4.0129e-02, -5.6784e-02],\n",
            "          [-1.9727e-02,  2.8035e-02,  5.2951e-03],\n",
            "          [-2.6369e-03,  6.3416e-02,  4.9198e-02]],\n",
            "\n",
            "         [[ 2.3355e-02, -2.2957e-02,  1.0720e-02],\n",
            "          [-2.4606e-02,  5.8754e-02,  2.9551e-02],\n",
            "          [-1.9480e-02,  1.6534e-02, -4.1543e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2908e-02, -4.2333e-02,  3.3792e-02],\n",
            "          [-3.0975e-02, -2.3974e-02,  3.9030e-02],\n",
            "          [ 1.2040e-02, -4.5184e-02,  3.1041e-02]],\n",
            "\n",
            "         [[ 3.6804e-02,  4.4014e-02, -8.4513e-03],\n",
            "          [ 5.7333e-02,  2.8442e-02, -7.6742e-02],\n",
            "          [ 2.6092e-02, -5.4251e-03,  1.8855e-02]],\n",
            "\n",
            "         [[-4.1647e-02, -3.1848e-03, -2.5033e-02],\n",
            "          [ 2.8067e-02, -1.5390e-02,  5.6550e-03],\n",
            "          [ 2.9157e-02, -1.5845e-02, -4.1956e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.3776e-02, -4.4503e-02,  2.4395e-02],\n",
            "          [-4.4618e-02,  5.1036e-02, -2.4504e-02],\n",
            "          [-3.6022e-02,  4.2829e-03,  3.1571e-02]],\n",
            "\n",
            "         [[-9.2701e-03, -2.6147e-02, -2.2450e-02],\n",
            "          [-7.9975e-03, -1.6973e-03,  3.8439e-02],\n",
            "          [ 3.0468e-02,  1.1886e-02,  1.4058e-03]],\n",
            "\n",
            "         [[-5.1890e-03,  3.0075e-02,  6.7571e-03],\n",
            "          [ 1.0698e-02, -2.4934e-03, -2.0318e-02],\n",
            "          [ 2.0126e-02,  2.9829e-03,  3.2000e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-3.0762e-02,  3.2292e-02, -3.0541e-02],\n",
            "          [-4.6930e-02,  2.5249e-02,  1.3714e-02],\n",
            "          [ 3.8547e-02,  4.3364e-02, -5.3578e-02]],\n",
            "\n",
            "         [[ 3.1052e-04,  4.7383e-02,  4.4030e-02],\n",
            "          [-6.0808e-02, -4.8006e-02, -3.2017e-02],\n",
            "          [-1.4945e-02,  3.1142e-02, -6.2399e-02]],\n",
            "\n",
            "         [[ 5.7588e-03,  4.3016e-02, -1.2874e-02],\n",
            "          [-2.7894e-02,  3.7197e-03,  1.1253e-02],\n",
            "          [ 8.3069e-03, -5.6915e-02, -5.3142e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0945e-02, -6.3563e-02, -4.2413e-02],\n",
            "          [-6.0241e-02, -2.3974e-02, -1.6155e-02],\n",
            "          [-5.9398e-03,  2.3113e-02, -4.2301e-02]],\n",
            "\n",
            "         [[-5.1040e-02,  4.1556e-02,  1.7844e-02],\n",
            "          [ 2.0595e-02,  2.3553e-02, -1.6352e-02],\n",
            "          [-2.0340e-02, -3.5019e-02,  6.5199e-03]],\n",
            "\n",
            "         [[-2.5782e-02,  4.0777e-02, -1.2755e-02],\n",
            "          [-4.4593e-03,  2.2440e-03, -3.8863e-02],\n",
            "          [-2.6507e-02, -6.7756e-02, -2.0099e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4068e-02,  1.1151e-02, -4.1842e-02],\n",
            "          [-5.0548e-02,  3.3191e-02, -5.2166e-02],\n",
            "          [-1.6298e-02, -5.4788e-02, -2.5586e-02]],\n",
            "\n",
            "         [[-4.3620e-02,  7.5348e-05,  6.2853e-02],\n",
            "          [ 2.7447e-03,  6.1396e-02,  2.3585e-02],\n",
            "          [-2.5475e-02, -4.4057e-02,  4.8194e-02]],\n",
            "\n",
            "         [[ 5.1981e-02,  5.6107e-03,  2.5289e-02],\n",
            "          [-4.9985e-02, -1.2070e-02,  6.2568e-02],\n",
            "          [-5.7801e-02,  3.8014e-02, -1.5218e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3915e-02,  2.2887e-02,  5.2440e-02],\n",
            "          [ 1.4098e-02, -2.4344e-02, -4.9823e-02],\n",
            "          [-1.9358e-02, -4.7720e-02, -4.7495e-02]],\n",
            "\n",
            "         [[-2.0866e-03, -5.2747e-02,  2.6210e-02],\n",
            "          [-1.9985e-02, -5.0947e-02, -4.3909e-02],\n",
            "          [-5.6757e-02,  1.3888e-02, -3.3856e-02]],\n",
            "\n",
            "         [[ 8.9020e-03,  6.5771e-02,  4.8685e-02],\n",
            "          [ 8.3632e-03, -8.8754e-03,  6.4175e-03],\n",
            "          [-4.7259e-02, -3.0999e-02, -6.4841e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 4.5923e-02,  5.4379e-02,  3.6497e-02],\n",
            "          [ 1.0987e-02, -2.8073e-02,  2.0647e-02],\n",
            "          [ 2.3819e-02,  6.4283e-02,  3.1356e-02]],\n",
            "\n",
            "         [[-3.5310e-02,  1.3135e-02, -1.1034e-02],\n",
            "          [-1.0225e-02,  7.2331e-03, -1.4775e-02],\n",
            "          [ 6.1411e-02,  6.7468e-02,  3.9273e-02]],\n",
            "\n",
            "         [[ 7.3195e-05,  1.0977e-02,  3.6658e-02],\n",
            "          [ 4.0626e-02, -2.3885e-03,  2.7893e-02],\n",
            "          [ 6.6381e-03, -1.3543e-02, -2.2681e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.6814e-02, -2.8658e-05,  5.4385e-02],\n",
            "          [ 4.1741e-02,  4.7467e-02, -8.2838e-03],\n",
            "          [ 2.5092e-02,  2.1877e-02,  2.2051e-02]],\n",
            "\n",
            "         [[-5.5876e-02,  6.1601e-03, -3.0832e-02],\n",
            "          [-1.4797e-02, -4.3782e-02,  4.7875e-02],\n",
            "          [ 9.4531e-03, -3.9817e-02,  3.6400e-02]],\n",
            "\n",
            "         [[ 1.0529e-02,  5.1683e-02,  1.3726e-02],\n",
            "          [-5.2151e-03,  2.6127e-02, -1.9446e-02],\n",
            "          [-1.0284e-02,  4.5549e-02, -3.7944e-03]]]], device='cuda:0')), ('rescnn_layers.2.cnn2.bias', tensor([-0.0309,  0.0287, -0.0271, -0.0569, -0.0093, -0.0223,  0.0565,  0.0402,\n",
            "         0.0296, -0.0562,  0.0361,  0.0537,  0.0267, -0.0269, -0.0287,  0.0539,\n",
            "         0.0375,  0.0313,  0.0358, -0.0119, -0.0341, -0.0478, -0.0515, -0.0512,\n",
            "         0.0239, -0.0178,  0.0125,  0.0450,  0.0441, -0.0150, -0.0050,  0.0444],\n",
            "       device='cuda:0')), ('rescnn_layers.2.layer_norm1.layer_norm.weight', tensor([1.0058, 0.9867, 0.9943, 0.9843, 0.9771, 0.9647, 0.9809, 0.9967, 0.9891,\n",
            "        0.9933, 0.9892, 0.9905, 1.0116, 1.0086, 1.0174, 1.0180, 1.0114, 0.9971,\n",
            "        1.0036, 0.9936, 0.9842, 0.9794, 1.0000, 1.0098, 1.0147, 1.0072, 1.0031,\n",
            "        0.9917, 0.9838, 0.9898, 0.9856, 0.9887, 0.9951, 0.9773, 0.9730, 0.9909,\n",
            "        1.0152, 0.9984, 0.9850, 0.9880, 0.9906, 0.9977, 0.9878, 0.9823, 0.9863,\n",
            "        1.0030, 1.0199, 1.0087, 0.9880, 0.9966, 0.9918, 1.0009, 0.9903, 1.0061,\n",
            "        1.0027, 0.9999, 1.0032, 1.0045, 0.9949, 0.9977, 1.0091, 0.9964, 0.9960,\n",
            "        1.0152], device='cuda:0')), ('rescnn_layers.2.layer_norm1.layer_norm.bias', tensor([ 9.4111e-04, -1.2620e-03,  4.7962e-05,  1.8600e-03,  6.0134e-04,\n",
            "        -4.7919e-03, -4.3897e-04, -6.0016e-04, -1.6443e-03, -6.0973e-04,\n",
            "        -2.3575e-05,  7.1106e-05,  6.0287e-05,  1.2466e-03,  2.2218e-03,\n",
            "         9.4657e-04, -4.4483e-04, -5.0300e-04,  2.0117e-04, -9.6919e-06,\n",
            "        -4.4256e-05, -9.5820e-04, -1.1432e-04,  3.9400e-04, -2.0516e-04,\n",
            "        -5.0076e-04,  1.0790e-04, -3.7843e-04, -1.3598e-03, -2.7550e-04,\n",
            "        -3.8973e-04, -6.2494e-04,  1.3852e-03,  1.9617e-04, -1.1061e-03,\n",
            "         1.6191e-04,  6.9138e-04, -4.9185e-04, -1.1021e-03, -2.9167e-03,\n",
            "        -1.4761e-04, -1.1361e-04, -1.3026e-04, -1.9703e-04, -6.0470e-04,\n",
            "        -9.3017e-04, -2.7566e-04, -5.6358e-04, -4.7667e-04, -4.6209e-04,\n",
            "        -1.3153e-03, -1.8654e-03, -2.6998e-03, -1.9369e-04, -2.8624e-04,\n",
            "         2.8967e-03,  3.1177e-04,  7.3592e-04, -9.0186e-05,  4.9086e-04,\n",
            "         2.4316e-03, -2.3838e-04,  4.9265e-04, -6.2742e-05], device='cuda:0')), ('rescnn_layers.2.layer_norm2.layer_norm.weight', tensor([0.9918, 0.9916, 0.9846, 0.9792, 0.9617, 0.9384, 0.9592, 0.9763, 0.9716,\n",
            "        0.9814, 0.9765, 0.9677, 0.9632, 0.9818, 0.9925, 0.9886, 0.9721, 0.9716,\n",
            "        0.9697, 0.9750, 0.9448, 0.9599, 0.9710, 0.9807, 0.9866, 0.9989, 0.9813,\n",
            "        0.9716, 0.9740, 0.9663, 0.9740, 0.9735, 0.9789, 0.9633, 0.9654, 0.9706,\n",
            "        0.9859, 0.9827, 0.9705, 0.9782, 0.9786, 0.9705, 0.9717, 0.9661, 0.9606,\n",
            "        0.9814, 0.9837, 0.9821, 0.9725, 0.9790, 0.9775, 0.9718, 0.9789, 0.9858,\n",
            "        0.9860, 0.9802, 0.9843, 0.9890, 0.9884, 0.9947, 0.9813, 0.9784, 0.9775,\n",
            "        0.9708], device='cuda:0')), ('rescnn_layers.2.layer_norm2.layer_norm.bias', tensor([-0.0041, -0.0082, -0.0133, -0.0181, -0.0312, -0.0432, -0.0336, -0.0219,\n",
            "        -0.0249, -0.0182, -0.0273, -0.0283, -0.0325, -0.0204, -0.0139, -0.0186,\n",
            "        -0.0251, -0.0311, -0.0293, -0.0203, -0.0411, -0.0277, -0.0266, -0.0241,\n",
            "        -0.0187, -0.0209, -0.0206, -0.0172, -0.0136, -0.0191, -0.0130, -0.0137,\n",
            "        -0.0121, -0.0180, -0.0159, -0.0124, -0.0124, -0.0183, -0.0290, -0.0189,\n",
            "        -0.0193, -0.0266, -0.0256, -0.0297, -0.0343, -0.0266, -0.0234, -0.0174,\n",
            "        -0.0221, -0.0212, -0.0206, -0.0287, -0.0230, -0.0247, -0.0229, -0.0252,\n",
            "        -0.0145, -0.0039, -0.0042, -0.0010, -0.0077, -0.0149, -0.0135, -0.0119],\n",
            "       device='cuda:0')), ('fully_connected.weight', tensor([[-1.4737e-02,  2.7334e-03,  7.9943e-04,  ...,  1.5375e-02,\n",
            "         -2.8173e-02, -1.8421e-02],\n",
            "        [-2.2356e-02,  4.2328e-04, -2.3683e-02,  ..., -2.8509e-03,\n",
            "          2.5418e-02,  1.0865e-02],\n",
            "        [-7.0356e-03,  1.7248e-02, -1.7431e-02,  ..., -1.4401e-02,\n",
            "         -5.2410e-04,  6.4193e-03],\n",
            "        ...,\n",
            "        [ 1.1621e-02,  1.4744e-02, -8.2842e-03,  ...,  1.0729e-02,\n",
            "          1.7943e-02, -1.6361e-04],\n",
            "        [ 8.1757e-03,  2.1556e-03, -1.8258e-02,  ...,  3.8417e-03,\n",
            "          1.0044e-02,  4.0557e-03],\n",
            "        [-2.7168e-02, -5.4171e-03, -2.2333e-02,  ...,  1.0591e-02,\n",
            "         -1.4780e-02,  6.2063e-05]], device='cuda:0')), ('fully_connected.bias', tensor([ 4.6958e-03,  1.7450e-02,  1.2604e-04, -3.1243e-03, -1.9802e-02,\n",
            "         5.7704e-03, -9.7020e-03,  1.9114e-02, -1.5377e-02, -1.2990e-02,\n",
            "        -6.8649e-03, -2.2394e-03,  1.1077e-02,  8.9405e-03, -1.1224e-02,\n",
            "        -1.9276e-03,  4.3996e-03, -2.4839e-03,  1.1779e-03,  8.3979e-03,\n",
            "        -1.7166e-02, -1.9562e-02,  1.7200e-02, -1.0345e-02, -1.7201e-02,\n",
            "         1.2246e-02,  1.0609e-02, -1.1540e-02,  2.1507e-02, -3.5008e-03,\n",
            "         6.1188e-03, -3.8085e-03, -2.0912e-02,  1.4932e-02, -1.8301e-02,\n",
            "        -1.1205e-02, -4.5470e-03,  8.3072e-03, -6.7208e-03, -1.4281e-02,\n",
            "        -1.6003e-02, -1.2557e-02,  4.2231e-03, -1.4313e-02, -1.9617e-02,\n",
            "        -7.3885e-03, -2.3761e-05, -1.8051e-02,  9.0689e-03,  1.5101e-02,\n",
            "        -1.5743e-02,  6.7950e-03, -6.2671e-03, -9.1860e-04,  7.3432e-03,\n",
            "         1.9538e-02,  2.0720e-02, -1.0778e-02,  7.4542e-03, -1.5221e-02,\n",
            "        -1.0386e-02, -8.6209e-03, -5.3415e-04, -1.0026e-02,  1.6995e-02,\n",
            "        -6.8368e-03,  2.0905e-02,  1.1127e-02, -1.7172e-02,  2.1884e-03,\n",
            "        -1.9695e-02,  1.6539e-02,  1.6477e-02,  6.7396e-03,  1.4726e-02,\n",
            "        -1.3106e-02,  1.1498e-02,  8.7706e-03, -1.3443e-02,  6.0775e-03,\n",
            "        -6.4271e-03, -6.7837e-03,  4.2094e-03, -1.0740e-02, -1.2328e-02,\n",
            "        -1.7452e-02, -5.8574e-03, -1.4132e-02,  1.8608e-02, -1.6116e-02,\n",
            "         1.5279e-03,  5.4376e-03,  2.9098e-02, -7.4731e-03, -1.8102e-02,\n",
            "        -1.3697e-02,  1.1911e-02,  1.5000e-02,  1.3206e-02, -1.0896e-02,\n",
            "         1.0160e-02,  7.0610e-03, -1.0036e-02, -1.6074e-02,  8.7245e-03,\n",
            "        -1.2455e-02, -4.0866e-03, -2.3426e-02, -5.2589e-04,  1.8125e-02,\n",
            "         2.7958e-03, -3.6095e-03, -1.5202e-02,  8.9519e-03,  4.1632e-03,\n",
            "         1.4143e-02,  1.3289e-02,  9.5432e-03, -7.4265e-03,  1.2685e-02,\n",
            "        -1.2083e-02, -7.6017e-03,  1.8484e-02, -1.1020e-03, -4.4122e-03,\n",
            "        -2.8601e-03,  1.6637e-02, -7.8251e-03,  9.0612e-03, -6.6178e-03,\n",
            "         9.0997e-03,  9.4406e-03, -1.8597e-02,  1.2434e-02,  1.5293e-02,\n",
            "         6.2602e-03,  1.2552e-02, -7.6918e-03, -4.2832e-03,  6.3749e-03,\n",
            "         5.0903e-03,  1.4496e-02,  7.1445e-03, -2.1036e-02,  2.1738e-02,\n",
            "        -4.7945e-03, -2.6487e-03,  5.0184e-03, -1.6037e-02, -4.3539e-03,\n",
            "         8.8793e-03,  1.3183e-02, -1.0018e-02,  1.9314e-02,  1.0955e-03,\n",
            "         1.5392e-02,  9.7055e-03, -3.6863e-03,  3.1278e-03, -1.9829e-02,\n",
            "         5.3038e-03,  5.8933e-03, -1.0357e-03, -1.1246e-02, -3.0511e-03,\n",
            "        -5.9683e-03,  8.9974e-03,  5.7145e-03, -1.5749e-02,  6.7901e-03,\n",
            "        -1.3006e-02,  1.7273e-03,  1.1138e-02, -1.5518e-02,  1.4418e-02,\n",
            "         1.0485e-02, -1.7135e-02,  1.7656e-02,  3.9526e-04, -5.8453e-03,\n",
            "        -1.8066e-02,  6.2626e-03,  4.7249e-03, -2.0846e-02, -1.7366e-02,\n",
            "         7.8473e-03,  2.9057e-03,  9.6009e-03,  1.3262e-02,  1.9897e-02,\n",
            "         5.8889e-03, -2.9399e-02,  1.2888e-02,  3.6282e-04,  2.0531e-03,\n",
            "        -1.3047e-02,  6.1252e-03,  2.0340e-02,  5.6999e-03,  9.6632e-03,\n",
            "        -1.6575e-02, -2.4734e-02,  4.7179e-03,  1.2849e-02,  6.5817e-03,\n",
            "        -2.0906e-02,  2.2112e-02,  2.0134e-02, -1.4686e-02, -9.9787e-03,\n",
            "         9.8798e-03, -1.3266e-02, -1.1768e-02,  1.2257e-02, -2.0643e-02,\n",
            "        -1.0259e-02, -2.1286e-02, -2.2935e-02, -1.3256e-02,  6.5809e-03,\n",
            "         1.6719e-02,  9.2987e-04,  1.8004e-02, -1.4201e-02, -5.1479e-03,\n",
            "        -1.5577e-02, -2.0667e-02, -1.7720e-02,  1.2911e-03, -1.9039e-02,\n",
            "        -4.4809e-03, -1.7510e-03,  1.7610e-03, -1.5708e-02,  1.6933e-02,\n",
            "        -1.9844e-02,  2.2555e-02,  6.1816e-03,  2.4164e-03, -2.0794e-02,\n",
            "         7.9039e-03, -1.7456e-02,  1.6389e-02,  1.0264e-02, -1.3325e-02,\n",
            "        -6.1012e-03, -8.2414e-03,  1.6699e-02,  3.6914e-03, -2.0410e-02,\n",
            "        -1.8370e-02,  8.4429e-03,  3.2393e-03, -3.0947e-03, -2.5718e-02,\n",
            "        -1.1190e-02, -8.1137e-03,  9.5423e-03, -1.4116e-02,  5.2648e-05,\n",
            "         8.1532e-03, -1.5400e-02, -1.1551e-02, -1.2812e-02, -2.9964e-03,\n",
            "        -6.9435e-03,  1.5412e-02,  1.6032e-02,  9.6282e-03,  1.1654e-02,\n",
            "         7.7482e-03, -6.1476e-03,  2.2096e-03,  1.0660e-02, -6.2224e-03,\n",
            "        -5.2384e-05, -2.0041e-02,  1.5893e-02, -5.6557e-03, -5.2698e-03,\n",
            "        -2.0291e-02, -3.8289e-03,  1.4317e-02,  3.8648e-03, -2.0338e-02,\n",
            "        -2.1816e-02, -5.4421e-03, -1.4110e-02,  1.5692e-02,  1.9859e-02,\n",
            "         3.1101e-03,  2.2824e-02, -2.2635e-02,  1.9576e-02, -9.5463e-03,\n",
            "         5.9247e-03, -8.0995e-03, -1.4411e-02, -5.6120e-03,  2.7364e-03,\n",
            "        -1.8894e-02, -1.2309e-02,  7.1505e-03, -2.5767e-02, -3.8737e-03,\n",
            "        -7.9520e-03,  5.1162e-03,  7.6178e-03,  4.5600e-03, -1.6748e-02,\n",
            "        -9.7549e-03, -8.9657e-03,  4.0465e-03, -4.6258e-03, -1.9384e-02,\n",
            "        -7.2352e-04, -9.4071e-03,  1.5345e-02, -2.2747e-02, -1.0046e-02,\n",
            "         6.8048e-03, -1.9460e-02,  1.1329e-02,  7.4006e-03,  6.3267e-03,\n",
            "        -2.0372e-03, -2.0676e-02,  1.9002e-02,  1.3614e-02,  8.9194e-03,\n",
            "         9.7869e-03,  1.2995e-02, -1.5163e-02,  8.2224e-03,  4.2822e-03,\n",
            "         1.4236e-02,  9.1840e-04, -9.7508e-03, -1.7298e-02, -5.8858e-03,\n",
            "         1.6261e-02,  1.0173e-02, -4.2739e-03, -2.2056e-02,  1.1649e-02,\n",
            "         1.1851e-02, -2.5997e-02, -1.9526e-02,  2.7768e-04,  5.4265e-03,\n",
            "        -8.7643e-03,  4.5924e-03,  8.2369e-03,  2.1919e-03, -2.0316e-02,\n",
            "        -2.4579e-02,  6.9061e-03, -4.2720e-03, -4.0157e-03,  2.6045e-03,\n",
            "        -1.2294e-02,  1.9990e-02,  8.9857e-03,  2.1571e-02, -4.1147e-03,\n",
            "        -1.4346e-02, -1.9635e-02,  2.1019e-02,  7.2152e-04,  1.3359e-02,\n",
            "         1.7029e-02, -8.1318e-03, -1.4816e-02,  1.0212e-02,  5.3413e-03,\n",
            "        -1.2697e-02, -1.9129e-02, -9.8797e-03,  1.6365e-02,  2.0250e-03,\n",
            "        -4.8531e-03, -1.9555e-02,  1.7378e-02,  1.1326e-02, -1.9818e-02,\n",
            "         9.8165e-03,  1.0717e-03, -5.3942e-03, -1.6306e-02, -1.7786e-02,\n",
            "        -2.0975e-02,  2.6138e-04,  1.0790e-02,  7.1192e-03,  9.7345e-03,\n",
            "        -5.7473e-03,  1.7380e-03,  2.1272e-02,  2.2967e-02,  3.8196e-03,\n",
            "        -7.3381e-03, -1.4875e-02, -2.2036e-02,  4.1672e-03,  1.2691e-02,\n",
            "         2.7718e-03,  2.0577e-02, -7.2204e-03,  9.8749e-03,  4.2514e-03,\n",
            "        -1.5936e-02,  1.9181e-02, -7.1112e-03, -1.6476e-02,  1.6623e-02,\n",
            "        -8.6238e-03,  6.7602e-03, -2.7661e-03, -1.2517e-02,  1.0092e-02,\n",
            "         7.0801e-03, -4.6361e-03, -2.9870e-02,  1.1486e-02, -1.5018e-02,\n",
            "        -1.6436e-02, -1.2587e-02, -1.2371e-02,  1.1706e-02, -1.4873e-02,\n",
            "        -8.4659e-03, -8.9946e-03,  1.1281e-02, -1.6451e-02, -1.1783e-02,\n",
            "        -9.7028e-03,  2.2709e-02,  1.3763e-02,  1.6030e-02, -2.1694e-02,\n",
            "        -2.4776e-02, -1.0907e-02,  1.5183e-02,  5.3219e-03, -1.9622e-02,\n",
            "         1.6943e-02,  4.1561e-03, -1.4241e-02, -1.6766e-02, -7.0104e-03,\n",
            "         1.4659e-02, -2.3316e-02,  1.4946e-03,  8.1074e-03,  1.0017e-02,\n",
            "         1.3519e-02, -9.4446e-03,  3.9111e-03,  5.0520e-04,  1.3892e-02,\n",
            "         1.5318e-02,  7.2831e-03,  2.0834e-03, -2.2590e-02,  1.3212e-02,\n",
            "         1.7905e-02,  2.3534e-04,  8.7919e-03, -6.2030e-04,  8.2533e-03,\n",
            "         1.7695e-02, -2.4040e-02,  2.4148e-02, -6.9073e-03, -1.4568e-02,\n",
            "        -1.3511e-02,  1.9918e-02, -1.9012e-02,  1.1254e-02, -5.4071e-03,\n",
            "        -1.2354e-02, -1.5884e-02,  1.0366e-03,  4.9921e-03, -4.7484e-03,\n",
            "         5.4301e-03, -9.9670e-04, -1.1004e-02, -2.9894e-03,  8.2848e-03,\n",
            "         1.6122e-02,  2.1787e-02,  1.6801e-02, -5.4893e-03, -8.6220e-04,\n",
            "        -9.8537e-03, -8.6298e-03, -7.1956e-04, -1.9575e-02, -2.1446e-02,\n",
            "         1.8879e-02,  3.8636e-03, -6.0593e-03,  1.5569e-02, -1.0820e-03,\n",
            "        -8.9284e-03, -5.7281e-03,  5.0825e-03, -7.4243e-03,  1.8902e-02,\n",
            "        -1.5408e-02, -2.5984e-03], device='cuda:0')), ('birnn_layers.0.BiGRU.weight_ih_l0', tensor([[ 0.0120, -0.0442,  0.0305,  ..., -0.0297, -0.0157,  0.0166],\n",
            "        [-0.0363, -0.0603,  0.0021,  ..., -0.0103, -0.0367, -0.0350],\n",
            "        [-0.0395, -0.0081,  0.0026,  ...,  0.0235,  0.0079, -0.0049],\n",
            "        ...,\n",
            "        [-0.0015,  0.0014, -0.0024,  ..., -0.0042,  0.0266,  0.0315],\n",
            "        [ 0.0386, -0.0229,  0.0142,  ...,  0.0446,  0.0322,  0.0003],\n",
            "        [-0.0417,  0.0409,  0.0479,  ..., -0.0210,  0.0442,  0.0268]],\n",
            "       device='cuda:0')), ('birnn_layers.0.BiGRU.weight_hh_l0', tensor([[-0.0212,  0.0335,  0.0061,  ..., -0.0335, -0.0041,  0.0209],\n",
            "        [ 0.0094,  0.0515, -0.0216,  ..., -0.0303, -0.0121, -0.0507],\n",
            "        [-0.0041,  0.0085,  0.0211,  ...,  0.0067, -0.0236,  0.0146],\n",
            "        ...,\n",
            "        [-0.0262, -0.0328,  0.0269,  ...,  0.0104,  0.0144, -0.0290],\n",
            "        [-0.0255, -0.0188,  0.0099,  ...,  0.0447, -0.0172, -0.0267],\n",
            "        [-0.0345,  0.0294,  0.0291,  ...,  0.0436, -0.0190,  0.0142]],\n",
            "       device='cuda:0')), ('birnn_layers.0.BiGRU.bias_ih_l0', tensor([-0.0221, -0.0297, -0.0500,  ...,  0.0185,  0.0277,  0.0247],\n",
            "       device='cuda:0')), ('birnn_layers.0.BiGRU.bias_hh_l0', tensor([-0.0132,  0.0092, -0.0464,  ..., -0.0394, -0.0201, -0.0350],\n",
            "       device='cuda:0')), ('birnn_layers.0.BiGRU.weight_ih_l0_reverse', tensor([[-2.3377e-02, -2.3494e-02, -3.6729e-03,  ..., -1.6044e-02,\n",
            "          2.4977e-02, -5.1766e-02],\n",
            "        [ 2.1435e-02,  4.4135e-02, -2.0533e-02,  ...,  3.6313e-02,\n",
            "         -2.6984e-03,  3.4844e-02],\n",
            "        [-1.4096e-02,  6.7932e-03, -3.8218e-02,  ...,  7.3628e-05,\n",
            "         -2.2745e-02, -1.2593e-02],\n",
            "        ...,\n",
            "        [-2.5732e-02, -1.3757e-02,  1.3358e-02,  ..., -3.0697e-03,\n",
            "          2.4399e-02, -3.1208e-02],\n",
            "        [ 2.5560e-02,  3.6509e-02,  2.4294e-03,  ..., -1.1511e-03,\n",
            "         -9.3123e-03, -3.0327e-02],\n",
            "        [ 3.8952e-02,  1.6120e-02,  1.6726e-02,  ..., -7.7845e-03,\n",
            "          5.2815e-02, -2.9117e-02]], device='cuda:0')), ('birnn_layers.0.BiGRU.weight_hh_l0_reverse', tensor([[ 0.0330,  0.0233, -0.0486,  ...,  0.0233,  0.0253,  0.0685],\n",
            "        [-0.0023, -0.0232, -0.0395,  ...,  0.0435, -0.0104,  0.0409],\n",
            "        [-0.0356, -0.0409, -0.0240,  ..., -0.0176,  0.0056,  0.0363],\n",
            "        ...,\n",
            "        [-0.0160,  0.0293, -0.0258,  ..., -0.0388, -0.0322, -0.0152],\n",
            "        [ 0.0311,  0.0018, -0.0095,  ..., -0.0295,  0.0195, -0.0346],\n",
            "        [ 0.0253,  0.0087, -0.0314,  ..., -0.0104,  0.0445, -0.0077]],\n",
            "       device='cuda:0')), ('birnn_layers.0.BiGRU.bias_ih_l0_reverse', tensor([ 0.0151,  0.0399, -0.0337,  ..., -0.0219, -0.0084,  0.0146],\n",
            "       device='cuda:0')), ('birnn_layers.0.BiGRU.bias_hh_l0_reverse', tensor([-0.0549, -0.0396, -0.0366,  ...,  0.0181,  0.0158, -0.0067],\n",
            "       device='cuda:0')), ('birnn_layers.0.layer_norm.weight', tensor([0.9937, 0.9919, 0.9988, 0.9931, 0.9934, 0.9916, 0.9897, 0.9912, 0.9736,\n",
            "        0.9914, 0.9897, 0.9890, 0.9983, 0.9955, 0.9955, 0.9968, 0.9969, 0.9862,\n",
            "        0.9900, 0.9850, 0.9864, 0.9972, 0.9990, 0.9911, 0.9904, 0.9937, 1.0036,\n",
            "        0.9782, 0.9950, 0.9904, 0.9920, 0.9571, 0.9855, 0.9917, 0.9930, 0.9844,\n",
            "        1.0124, 0.9870, 0.9842, 0.9851, 0.9989, 0.9918, 1.0051, 0.9931, 1.0072,\n",
            "        1.0032, 0.9936, 1.0013, 1.0002, 0.9902, 0.9995, 0.9985, 0.9871, 0.9898,\n",
            "        0.9956, 0.9835, 1.0039, 0.9924, 0.9955, 1.0011, 0.9834, 0.9932, 1.0068,\n",
            "        0.9852, 0.9934, 0.9920, 0.9910, 0.9912, 0.9880, 0.9997, 0.9786, 0.9866,\n",
            "        0.9894, 0.9888, 0.9872, 0.9921, 0.9927, 1.0030, 0.9945, 1.0047, 1.0029,\n",
            "        0.9895, 0.9996, 0.9930, 0.9865, 0.9924, 0.9869, 0.9926, 0.9932, 0.9914,\n",
            "        1.0020, 0.9983, 0.9911, 0.9910, 0.9982, 0.9962, 0.9930, 1.0087, 0.9922,\n",
            "        0.9859, 0.9949, 1.0058, 0.9947, 0.9876, 0.9925, 0.9903, 0.9962, 0.9933,\n",
            "        0.9891, 0.9888, 1.0025, 1.0031, 1.0014, 0.9931, 0.9992, 0.9878, 0.9925,\n",
            "        0.9892, 1.0004, 0.9911, 1.0033, 0.9920, 1.0023, 0.9974, 0.9953, 0.9913,\n",
            "        0.9971, 0.9852, 0.9876, 0.9917, 0.9889, 0.9776, 0.9894, 0.9874, 0.9852,\n",
            "        0.9975, 0.9889, 0.9919, 0.9862, 0.9932, 0.9891, 0.9835, 0.9910, 0.9908,\n",
            "        0.9972, 0.9960, 1.0008, 0.9841, 0.9860, 0.9886, 0.9836, 0.9878, 0.9890,\n",
            "        0.9908, 0.9897, 0.9893, 0.9913, 0.9931, 1.0154, 0.9903, 0.9871, 0.9904,\n",
            "        0.9900, 0.9815, 0.9918, 0.9907, 0.9921, 1.0019, 0.9925, 0.9974, 0.9926,\n",
            "        0.9968, 0.9822, 0.9918, 0.9906, 0.9900, 0.9904, 0.9922, 1.0011, 0.9940,\n",
            "        1.0202, 1.0006, 0.9888, 0.9899, 0.9814, 0.9937, 0.9874, 0.9931, 1.0043,\n",
            "        0.9979, 0.9969, 1.0015, 0.9915, 0.9802, 0.9953, 0.9884, 0.9881, 0.9952,\n",
            "        0.9936, 0.9879, 0.9875, 0.9805, 0.9989, 0.9961, 0.9877, 0.9969, 0.9831,\n",
            "        1.0024, 0.9966, 0.9922, 0.9914, 0.9841, 0.9895, 0.9929, 0.9836, 0.9809,\n",
            "        0.9893, 0.9922, 1.0071, 0.9925, 0.9910, 0.9817, 0.9929, 0.9898, 1.0017,\n",
            "        0.9856, 0.9917, 0.9904, 0.9865, 0.9833, 0.9883, 0.9905, 0.9903, 0.9923,\n",
            "        0.9931, 0.9897, 0.9866, 0.9895, 0.9934, 0.9909, 0.9889, 0.9931, 0.9883,\n",
            "        0.9891, 0.9890, 0.9851, 0.9910, 0.9933, 1.0086, 0.9880, 0.9911, 0.9892,\n",
            "        0.9907, 0.9903, 0.9812, 0.9877, 0.9828, 0.9857, 0.9925, 0.9838, 0.9928,\n",
            "        0.9953, 0.9871, 0.9894, 0.9922, 0.9941, 0.9946, 0.9916, 0.9917, 0.9860,\n",
            "        0.9860, 0.9929, 0.9913, 0.9888, 0.9766, 0.9796, 0.9928, 0.9958, 0.9847,\n",
            "        0.9915, 0.9879, 0.9939, 0.9963, 0.9913, 0.9894, 0.9983, 0.9925, 0.9919,\n",
            "        0.9946, 1.0010, 0.9876, 0.9923, 0.9935, 0.9921, 0.9942, 0.9869, 0.9972,\n",
            "        0.9930, 0.9906, 0.9810, 0.9868, 0.9844, 0.9931, 0.9910, 0.9916, 0.9847,\n",
            "        0.9903, 0.9904, 0.9909, 0.9886, 0.9882, 0.9947, 0.9926, 0.9872, 0.9938,\n",
            "        0.9952, 0.9909, 0.9917, 0.9920, 0.9901, 1.0008, 0.9899, 0.9833, 0.9952,\n",
            "        0.9897, 0.9952, 0.9888, 0.9919, 0.9901, 0.9933, 0.9822, 0.9965, 0.9877,\n",
            "        0.9917, 0.9899, 0.9931, 0.9883, 0.9884, 0.9852, 0.9857, 0.9768, 0.9997,\n",
            "        0.9886, 0.9864, 0.9918, 0.9911, 0.9891, 1.0023, 0.9974, 0.9866, 0.9922,\n",
            "        0.9956, 0.9933, 0.9829, 0.9971, 0.9843, 0.9870, 0.9897, 0.9917, 0.9916,\n",
            "        0.9879, 0.9908, 0.9903, 0.9920, 0.9961, 0.9907, 0.9861, 0.9957, 0.9911,\n",
            "        0.9992, 0.9952, 0.9833, 0.9901, 0.9925, 0.9792, 0.9902, 0.9927, 0.9937,\n",
            "        0.9773, 0.9929, 0.9972, 0.9932, 0.9975, 0.9922, 0.9892, 0.9910, 0.9836,\n",
            "        0.9908, 0.9907, 0.9910, 0.9890, 0.9911, 0.9929, 0.9839, 0.9885, 0.9930,\n",
            "        0.9925, 0.9936, 0.9938, 0.9832, 0.9963, 0.9951, 0.9908, 0.9993, 0.9868,\n",
            "        0.9887, 0.9913, 0.9853, 0.9877, 0.9958, 0.9854, 0.9883, 0.9906, 1.0042,\n",
            "        1.0031, 0.9945, 0.9813, 0.9904, 0.9997, 1.0019, 0.9890, 0.9927, 0.9956,\n",
            "        0.9954, 0.9982, 0.9936, 0.9853, 0.9848, 0.9928, 0.9923, 0.9849, 0.9980,\n",
            "        0.9877, 0.9973, 0.9834, 0.9909, 0.9932, 0.9888, 0.9965, 0.9847, 0.9968,\n",
            "        0.9937, 0.9994, 0.9941, 0.9916, 0.9912, 0.9908, 0.9760, 0.9930, 1.0108,\n",
            "        0.9921, 0.9957, 1.0015, 0.9911, 0.9914, 0.9924, 0.9891, 0.9945, 0.9930,\n",
            "        0.9838, 1.0014, 0.9993, 0.9965, 0.9902, 0.9858, 0.9978, 0.9993, 1.0020,\n",
            "        0.9880, 0.9892, 0.9973, 0.9859, 0.9969, 0.9910, 0.9937, 0.9863, 0.9864,\n",
            "        0.9943, 1.0079, 0.9935, 0.9903, 0.9936, 0.9904, 0.9907, 0.9941, 0.9940,\n",
            "        0.9875, 1.0017, 0.9879, 0.9893, 0.9875, 0.9858, 0.9907, 0.9914, 0.9957,\n",
            "        0.9930, 0.9875, 0.9943, 0.9857, 0.9814, 1.0020, 0.9961, 0.9951, 0.9891,\n",
            "        0.9935, 0.9923, 0.9869, 0.9961, 0.9869, 1.0062, 0.9946, 0.9861],\n",
            "       device='cuda:0')), ('birnn_layers.0.layer_norm.bias', tensor([-1.8139e-03,  1.2834e-03,  1.5455e-03,  3.6868e-03,  2.7778e-03,\n",
            "        -4.7272e-04, -3.1207e-03, -4.0757e-04, -1.9659e-03,  6.2760e-04,\n",
            "        -1.2057e-02, -3.2531e-03,  2.0146e-03, -8.9288e-04,  2.5265e-03,\n",
            "         1.1541e-04,  4.1024e-03, -8.3262e-03, -7.0728e-03, -3.7025e-03,\n",
            "        -4.3653e-03,  1.4328e-03, -8.5218e-04,  3.6069e-04, -1.6014e-03,\n",
            "         7.0645e-04, -5.3183e-06, -6.8801e-03,  3.4047e-03, -4.2301e-03,\n",
            "         5.1981e-03, -1.7484e-02, -4.1962e-03, -3.1464e-05, -5.2806e-04,\n",
            "        -1.7803e-03,  1.7017e-03, -1.9212e-03, -8.6942e-03, -4.2343e-03,\n",
            "        -1.6204e-04, -3.0271e-03,  5.0945e-03, -4.3643e-03,  6.6156e-03,\n",
            "         6.2457e-03, -2.5054e-03,  4.3316e-04,  5.0010e-03, -3.0978e-03,\n",
            "         4.5032e-03,  4.3219e-03,  8.9466e-04, -1.6537e-03,  2.0986e-03,\n",
            "        -6.8189e-04, -6.8104e-03, -3.7942e-03,  4.9947e-04,  5.3650e-03,\n",
            "        -9.7902e-03, -2.4907e-03,  1.0422e-02, -2.8347e-03,  2.2807e-04,\n",
            "         9.5379e-04,  2.2911e-03,  2.2264e-03,  1.0741e-04,  3.6454e-03,\n",
            "        -9.1981e-03, -6.2451e-03, -2.0335e-03, -3.8337e-03, -4.3843e-03,\n",
            "         5.1916e-04,  3.2480e-04,  5.6204e-03, -2.5549e-03, -6.1796e-03,\n",
            "         7.0801e-03, -1.1760e-03,  7.1688e-03,  2.1625e-03, -6.6764e-03,\n",
            "        -1.1450e-03, -7.2167e-03,  7.1718e-04, -1.4427e-03, -6.7735e-03,\n",
            "         3.0952e-03,  3.5804e-03,  2.6419e-03, -1.8242e-03,  2.5869e-03,\n",
            "         5.5614e-03, -2.2283e-03,  7.5620e-03, -5.2047e-03, -2.7231e-03,\n",
            "        -4.8519e-03,  6.5136e-03, -2.9463e-03, -9.0071e-05, -3.3367e-05,\n",
            "        -4.3590e-03, -1.0183e-03, -3.6398e-03, -3.8975e-03, -2.3563e-03,\n",
            "         7.1665e-03,  2.9018e-03,  1.0709e-03, -2.5940e-03,  3.7039e-03,\n",
            "        -4.2198e-03, -1.4077e-04, -2.0739e-05, -5.8857e-04, -4.2492e-03,\n",
            "         5.1310e-03, -2.1483e-03,  5.6305e-03, -1.8985e-04,  1.3839e-03,\n",
            "        -1.0606e-03,  9.9942e-04, -7.9178e-03, -8.0950e-04,  1.7466e-04,\n",
            "         1.6277e-03, -5.0929e-03, -4.0509e-04,  1.4544e-03, -9.3653e-04,\n",
            "         6.0668e-04, -8.1586e-04,  2.2093e-03, -2.6039e-03, -2.2316e-03,\n",
            "        -1.0205e-03, -2.1193e-03, -5.6155e-03,  5.3182e-05,  6.4333e-03,\n",
            "         5.0253e-03,  5.6130e-04, -6.2105e-03, -7.3426e-03,  3.9569e-04,\n",
            "        -1.5049e-03, -1.1013e-02, -1.3746e-02, -1.5004e-03, -3.4498e-03,\n",
            "         1.0133e-03, -2.4855e-03, -1.9239e-03,  6.7555e-03, -9.6630e-04,\n",
            "        -7.5241e-04, -3.2529e-03, -4.8238e-04, -9.4566e-03, -1.2291e-03,\n",
            "        -1.0052e-02, -1.5224e-03, -6.3187e-03, -2.6693e-03,  3.1685e-03,\n",
            "        -1.4034e-03,  3.1027e-03, -1.0692e-02, -2.0584e-03, -4.2303e-03,\n",
            "         2.4737e-03, -4.9780e-03, -2.0983e-03,  2.9516e-03,  4.0600e-03,\n",
            "         1.4193e-02,  1.0074e-03, -1.5867e-03, -3.7708e-03, -4.1456e-03,\n",
            "         4.8569e-03, -5.5749e-04, -1.4073e-03,  2.6154e-03,  4.0563e-03,\n",
            "         3.5491e-03, -8.8368e-04, -1.9063e-03, -8.2274e-03, -5.6023e-04,\n",
            "        -5.4869e-03, -3.1582e-04,  1.8612e-03,  2.6794e-03, -1.4537e-03,\n",
            "        -5.9175e-03, -4.2889e-03, -4.6199e-04, -5.8939e-03, -1.0151e-02,\n",
            "         1.9895e-03, -1.8956e-04,  8.5233e-03,  5.2699e-03, -3.6606e-04,\n",
            "        -2.0191e-03, -6.0849e-03, -1.5733e-03, -3.7006e-03, -1.9716e-03,\n",
            "        -2.8201e-03, -8.5981e-03, -1.6250e-03, -5.8732e-03,  7.5990e-04,\n",
            "         7.8448e-04, -5.4494e-03, -7.7613e-04,  6.8604e-03,  1.7168e-03,\n",
            "        -2.9238e-03, -3.6019e-03, -2.5941e-03, -4.9905e-03, -7.6481e-03,\n",
            "         4.3629e-04,  6.1328e-04, -2.2353e-03, -3.2808e-03,  6.0048e-03,\n",
            "        -1.3440e-03, -4.3736e-03, -7.7114e-03, -9.5800e-04, -1.6229e-03,\n",
            "         2.7555e-03,  2.1369e-03, -3.6007e-04, -2.9156e-03, -5.9475e-03,\n",
            "        -4.7633e-03, -1.5656e-04,  5.7242e-03,  7.3594e-03, -2.3818e-03,\n",
            "         1.8427e-03, -2.0414e-03, -4.2409e-03,  2.3337e-03, -1.5723e-02,\n",
            "        -1.1791e-04, -4.9767e-03, -1.8288e-04, -7.8335e-04, -1.0496e-03,\n",
            "        -1.1743e-03,  3.1007e-03, -5.0542e-04,  1.8743e-03, -6.4815e-04,\n",
            "         1.7263e-03,  2.2619e-03, -5.8526e-04,  3.2991e-03,  1.4350e-03,\n",
            "        -4.4627e-04, -3.8857e-03, -1.7956e-03, -3.8723e-03, -1.1887e-02,\n",
            "        -7.1583e-04,  7.5979e-04,  2.0469e-03, -6.9255e-03, -1.7583e-03,\n",
            "        -9.8929e-03,  4.3793e-03,  4.0485e-03,  1.2906e-03, -2.5538e-03,\n",
            "         1.2288e-03, -3.2505e-03,  1.2549e-03,  4.6862e-03,  1.1824e-02,\n",
            "        -9.1981e-04, -1.0354e-03,  1.8390e-03,  1.2009e-03,  1.0696e-05,\n",
            "        -4.4677e-03, -3.7517e-03, -3.5263e-03,  7.0021e-04, -3.2143e-03,\n",
            "        -7.3518e-03, -5.7592e-03,  2.0277e-03, -9.1580e-03, -4.2698e-03,\n",
            "        -2.0623e-03, -1.1087e-03, -2.8917e-04,  1.9936e-03, -4.6065e-03,\n",
            "        -6.4735e-03,  6.2574e-03,  1.6056e-03, -4.3572e-03,  6.9990e-04,\n",
            "        -4.0195e-03,  7.8041e-03, -1.9537e-03, -9.6583e-03, -5.4437e-03,\n",
            "         2.8882e-03,  1.4838e-03, -2.3737e-03,  1.1846e-02, -5.1677e-03,\n",
            "         5.4367e-03, -2.5658e-03, -2.1881e-04, -3.2294e-03,  3.6842e-04,\n",
            "        -5.7310e-03,  4.2490e-03, -1.0194e-02,  6.8015e-03, -3.6929e-03,\n",
            "         1.2427e-03, -2.4325e-03, -1.0493e-03, -1.2639e-02, -6.3299e-03,\n",
            "         2.4203e-03,  8.2842e-03, -7.5508e-03, -7.9665e-03,  3.4435e-04,\n",
            "        -3.8527e-03, -5.0022e-03,  5.7679e-04, -1.0428e-03, -1.3254e-02,\n",
            "         1.1596e-03,  5.3942e-04, -5.5887e-04, -1.0314e-02,  5.7850e-04,\n",
            "        -5.7884e-03, -6.3946e-03,  9.7207e-04, -1.4021e-03,  8.0607e-04,\n",
            "        -2.7041e-03,  7.9694e-04, -5.8173e-03, -2.2339e-03, -1.5426e-03,\n",
            "        -2.6465e-03, -5.3344e-03, -1.3966e-03, -5.0198e-03,  3.2832e-03,\n",
            "         3.3192e-03, -2.5913e-03,  6.6445e-04,  1.1967e-03, -5.3866e-03,\n",
            "        -2.4775e-03, -3.0071e-03, -6.9649e-04, -1.1124e-02, -1.1587e-03,\n",
            "        -1.3352e-04, -1.7071e-03, -3.9410e-03, -3.6595e-03, -2.0291e-03,\n",
            "        -2.4210e-03, -7.1530e-03, -3.4778e-03, -2.3750e-03, -1.1752e-03,\n",
            "        -5.1296e-03, -2.3462e-03,  6.2589e-04, -8.0929e-03, -3.3339e-03,\n",
            "         1.5938e-04, -8.8308e-04,  1.3810e-03,  2.9003e-03,  8.5307e-04,\n",
            "         3.7806e-03, -4.3093e-04,  1.2634e-03, -2.6054e-03, -1.1788e-02,\n",
            "         4.5200e-03, -1.7903e-03, -4.1422e-03, -1.1074e-02,  2.0037e-03,\n",
            "        -2.0715e-03, -2.1199e-03, -4.3271e-05,  4.3048e-03,  1.0432e-02,\n",
            "        -6.4371e-03, -2.1030e-02,  1.2554e-03,  5.3578e-03,  5.1514e-04,\n",
            "        -4.5188e-03, -1.9234e-03, -9.1658e-03, -5.6492e-04,  7.6410e-03,\n",
            "        -1.1779e-03, -4.3667e-03, -7.7446e-03,  3.2459e-03, -2.1754e-04,\n",
            "         4.6776e-04,  5.5025e-03, -1.8990e-03,  1.7510e-03, -3.6078e-03,\n",
            "         5.1690e-04,  9.8829e-04, -4.8617e-03,  1.5843e-03, -3.6595e-03,\n",
            "         4.7484e-04,  4.3609e-05, -5.1497e-03, -1.7805e-03,  1.9446e-04,\n",
            "        -5.0674e-03, -2.2530e-03, -1.3840e-02,  3.5582e-03,  3.6619e-03,\n",
            "         2.3320e-03, -1.8600e-03,  3.4285e-04, -2.5778e-03, -3.0155e-03,\n",
            "        -1.5912e-03,  2.7509e-03, -9.0628e-05,  4.0450e-04, -1.1651e-03,\n",
            "         1.3139e-03,  7.2859e-03, -1.3468e-04, -4.9777e-03, -4.3009e-03,\n",
            "         6.4454e-04,  1.1716e-02,  9.1988e-03,  1.3328e-03,  1.1610e-03,\n",
            "         3.6705e-03, -4.8291e-03,  7.9440e-03, -3.9465e-04,  4.7894e-03,\n",
            "        -2.8335e-03, -4.0090e-03, -6.7939e-04,  3.9054e-03, -5.5109e-03,\n",
            "        -2.1013e-03, -2.7587e-03, -4.6542e-03, -2.1511e-03,  5.4901e-04,\n",
            "        -8.2956e-04, -3.0712e-03,  1.6981e-02,  6.5965e-04, -1.2505e-03,\n",
            "        -6.4603e-03, -2.1888e-03, -2.1449e-03, -2.9044e-03,  2.5600e-03,\n",
            "        -3.5401e-03, -7.5772e-03,  2.2314e-03, -5.8100e-03, -4.8556e-03,\n",
            "         3.3779e-03,  6.9538e-03,  4.7414e-04, -3.2630e-03,  1.3249e-03,\n",
            "        -1.1967e-03, -9.3088e-05,  6.7741e-03, -1.0369e-02,  7.6314e-03,\n",
            "        -4.0357e-04, -1.3199e-02], device='cuda:0')), ('birnn_layers.1.BiGRU.weight_ih_l0', tensor([[-0.0363,  0.0363,  0.0399,  ...,  0.0158,  0.0148, -0.0335],\n",
            "        [ 0.0253, -0.0059, -0.0229,  ..., -0.0113, -0.0310,  0.0195],\n",
            "        [-0.0382, -0.0335, -0.0180,  ..., -0.0420,  0.0478,  0.0184],\n",
            "        ...,\n",
            "        [ 0.0276,  0.0128, -0.0229,  ...,  0.0161,  0.0005, -0.0024],\n",
            "        [ 0.0364,  0.0292, -0.0441,  ..., -0.0006,  0.0291,  0.0194],\n",
            "        [ 0.0251,  0.0131,  0.0316,  ...,  0.0236, -0.0191, -0.0208]],\n",
            "       device='cuda:0')), ('birnn_layers.1.BiGRU.weight_hh_l0', tensor([[-0.0061,  0.0368,  0.0080,  ...,  0.0252,  0.0234, -0.0190],\n",
            "        [ 0.0163,  0.0528, -0.0144,  ..., -0.0311,  0.0336,  0.0246],\n",
            "        [ 0.0291,  0.0152, -0.0319,  ...,  0.0109, -0.0390,  0.0120],\n",
            "        ...,\n",
            "        [ 0.0025,  0.0231,  0.0167,  ..., -0.0328,  0.0311, -0.0008],\n",
            "        [ 0.0142, -0.0243,  0.0331,  ..., -0.0259, -0.0252, -0.0102],\n",
            "        [-0.0016,  0.0087,  0.0281,  ..., -0.0313, -0.0348,  0.0097]],\n",
            "       device='cuda:0')), ('birnn_layers.1.BiGRU.bias_ih_l0', tensor([-0.0373,  0.0373,  0.0226,  ...,  0.0167,  0.0060,  0.0324],\n",
            "       device='cuda:0')), ('birnn_layers.1.BiGRU.bias_hh_l0', tensor([-0.0200, -0.0116,  0.0210,  ..., -0.0420,  0.0355,  0.0239],\n",
            "       device='cuda:0')), ('birnn_layers.1.BiGRU.weight_ih_l0_reverse', tensor([[-0.0220,  0.0251, -0.0089,  ..., -0.0271,  0.0173, -0.0056],\n",
            "        [ 0.0124, -0.0522,  0.0168,  ..., -0.0423,  0.0111, -0.0509],\n",
            "        [-0.0158, -0.0441,  0.0197,  ...,  0.0332, -0.0243,  0.0143],\n",
            "        ...,\n",
            "        [ 0.0422,  0.0011, -0.0204,  ...,  0.0363, -0.0171, -0.0395],\n",
            "        [-0.0401,  0.0028, -0.0132,  ...,  0.0338, -0.0433,  0.0285],\n",
            "        [ 0.0232, -0.0104,  0.0040,  ...,  0.0190,  0.0342,  0.0222]],\n",
            "       device='cuda:0')), ('birnn_layers.1.BiGRU.weight_hh_l0_reverse', tensor([[-0.0192, -0.0129,  0.0278,  ..., -0.0219,  0.0387,  0.0121],\n",
            "        [-0.0058,  0.0052,  0.0168,  ...,  0.0310, -0.0336,  0.0561],\n",
            "        [ 0.0057,  0.0124,  0.0360,  ..., -0.0183, -0.0311,  0.0300],\n",
            "        ...,\n",
            "        [ 0.0195,  0.0055,  0.0056,  ..., -0.0089,  0.0015, -0.0538],\n",
            "        [-0.0125,  0.0336, -0.0048,  ...,  0.0323, -0.0067,  0.0391],\n",
            "        [-0.0358, -0.0326, -0.0067,  ..., -0.0294, -0.0298,  0.0251]],\n",
            "       device='cuda:0')), ('birnn_layers.1.BiGRU.bias_ih_l0_reverse', tensor([ 0.0107, -0.0016, -0.0177,  ..., -0.0127,  0.0016, -0.0209],\n",
            "       device='cuda:0')), ('birnn_layers.1.BiGRU.bias_hh_l0_reverse', tensor([ 0.0204,  0.0210, -0.0411,  ..., -0.0160, -0.0148,  0.0166],\n",
            "       device='cuda:0')), ('birnn_layers.1.layer_norm.weight', tensor([0.9936, 0.9916, 0.9710,  ..., 0.9862, 0.9856, 0.9943], device='cuda:0')), ('birnn_layers.1.layer_norm.bias', tensor([ 4.0879e-05,  6.3141e-04, -1.1892e-02,  ..., -6.8895e-03,\n",
            "        -8.0607e-03,  1.4880e-03], device='cuda:0')), ('birnn_layers.2.BiGRU.weight_ih_l0', tensor([[-0.0193,  0.0074, -0.0480,  ..., -0.0075,  0.0171, -0.0472],\n",
            "        [-0.0089, -0.0094, -0.0313,  ...,  0.0014,  0.0262,  0.0081],\n",
            "        [-0.0144,  0.0235, -0.0160,  ..., -0.0161,  0.0105,  0.0063],\n",
            "        ...,\n",
            "        [-0.0215,  0.0162, -0.0233,  ..., -0.0386, -0.0185, -0.0077],\n",
            "        [-0.0391, -0.0427, -0.0272,  ...,  0.0149, -0.0369,  0.0241],\n",
            "        [ 0.0355, -0.0148, -0.0022,  ..., -0.0253,  0.0203,  0.0281]],\n",
            "       device='cuda:0')), ('birnn_layers.2.BiGRU.weight_hh_l0', tensor([[-0.0070,  0.0333, -0.0354,  ...,  0.0228,  0.0307, -0.0027],\n",
            "        [ 0.0148, -0.0340, -0.0117,  ..., -0.0199,  0.0382, -0.0388],\n",
            "        [-0.0019,  0.0322, -0.0335,  ..., -0.0381,  0.0375,  0.0350],\n",
            "        ...,\n",
            "        [-0.0384,  0.0063, -0.0371,  ...,  0.0007, -0.0200, -0.0024],\n",
            "        [ 0.0359,  0.0079, -0.0122,  ..., -0.0019,  0.0080, -0.0306],\n",
            "        [-0.0057, -0.0116,  0.0130,  ..., -0.0122, -0.0396,  0.0299]],\n",
            "       device='cuda:0')), ('birnn_layers.2.BiGRU.bias_ih_l0', tensor([ 0.0186,  0.0083,  0.0101,  ..., -0.0177, -0.0235, -0.0039],\n",
            "       device='cuda:0')), ('birnn_layers.2.BiGRU.bias_hh_l0', tensor([-0.0364, -0.0021, -0.0212,  ...,  0.0373, -0.0091, -0.0302],\n",
            "       device='cuda:0')), ('birnn_layers.2.BiGRU.weight_ih_l0_reverse', tensor([[-0.0097,  0.0074,  0.0080,  ...,  0.0503,  0.0421,  0.0327],\n",
            "        [ 0.0431, -0.0278, -0.0283,  ..., -0.0224,  0.0256,  0.0349],\n",
            "        [ 0.0091,  0.0347,  0.0166,  ...,  0.0078,  0.0137, -0.0183],\n",
            "        ...,\n",
            "        [ 0.0009,  0.0393,  0.0203,  ...,  0.0240,  0.0069,  0.0099],\n",
            "        [ 0.0388, -0.0312, -0.0094,  ..., -0.0025,  0.0359, -0.0319],\n",
            "        [-0.0262, -0.0155, -0.0302,  ..., -0.0165,  0.0164,  0.0434]],\n",
            "       device='cuda:0')), ('birnn_layers.2.BiGRU.weight_hh_l0_reverse', tensor([[-2.1522e-04, -3.5420e-02,  2.3252e-02,  ...,  4.3585e-02,\n",
            "          1.6338e-03, -2.3122e-02],\n",
            "        [ 2.1740e-02, -2.2391e-02,  1.1298e-02,  ...,  5.7559e-02,\n",
            "          2.8755e-02,  3.6602e-02],\n",
            "        [-1.4329e-02,  2.8118e-02,  3.6931e-02,  ..., -2.2001e-03,\n",
            "          3.0543e-02, -1.4645e-02],\n",
            "        ...,\n",
            "        [-2.1399e-02,  2.4923e-02,  2.8506e-03,  ...,  2.2422e-02,\n",
            "          2.0448e-02,  2.5728e-02],\n",
            "        [-2.0849e-02,  4.3129e-02,  2.1550e-02,  ..., -3.7080e-05,\n",
            "          1.5301e-02,  3.5659e-02],\n",
            "        [ 5.1574e-03,  3.3225e-02, -2.3871e-02,  ..., -2.6469e-02,\n",
            "          3.0561e-02,  5.2013e-03]], device='cuda:0')), ('birnn_layers.2.BiGRU.bias_ih_l0_reverse', tensor([-0.0095,  0.0159, -0.0261,  ...,  0.0125,  0.0012,  0.0394],\n",
            "       device='cuda:0')), ('birnn_layers.2.BiGRU.bias_hh_l0_reverse', tensor([ 0.0093,  0.0332, -0.0321,  ...,  0.0401,  0.0109,  0.0400],\n",
            "       device='cuda:0')), ('birnn_layers.2.layer_norm.weight', tensor([0.9888, 0.9895, 0.9822,  ..., 0.9755, 0.9874, 0.9839], device='cuda:0')), ('birnn_layers.2.layer_norm.bias', tensor([-0.0059, -0.0023, -0.0061,  ..., -0.0032, -0.0041, -0.0048],\n",
            "       device='cuda:0')), ('birnn_layers.3.BiGRU.weight_ih_l0', tensor([[ 0.0184,  0.0373,  0.0096,  ..., -0.0140, -0.0048,  0.0221],\n",
            "        [-0.0395,  0.0180,  0.0423,  ...,  0.0266, -0.0055,  0.0146],\n",
            "        [ 0.0089, -0.0198,  0.0194,  ..., -0.0216, -0.0287, -0.0155],\n",
            "        ...,\n",
            "        [ 0.0264, -0.0041, -0.0116,  ...,  0.0312,  0.0439,  0.0101],\n",
            "        [-0.0266,  0.0211,  0.0358,  ..., -0.0058,  0.0348, -0.0070],\n",
            "        [-0.0366,  0.0333, -0.0148,  ..., -0.0525, -0.0299,  0.0157]],\n",
            "       device='cuda:0')), ('birnn_layers.3.BiGRU.weight_hh_l0', tensor([[ 0.0283, -0.0212, -0.0331,  ..., -0.0005,  0.0344, -0.0397],\n",
            "        [-0.0185, -0.0183,  0.0198,  ...,  0.0316, -0.0387, -0.0405],\n",
            "        [-0.0195,  0.0329, -0.0179,  ...,  0.0400, -0.0315,  0.0149],\n",
            "        ...,\n",
            "        [ 0.0038, -0.0355,  0.0368,  ..., -0.0421, -0.0058,  0.0315],\n",
            "        [-0.0239, -0.0175, -0.0091,  ..., -0.0182,  0.0331,  0.0011],\n",
            "        [ 0.0305,  0.0055,  0.0181,  ...,  0.0365,  0.0385, -0.0221]],\n",
            "       device='cuda:0')), ('birnn_layers.3.BiGRU.bias_ih_l0', tensor([-0.0039,  0.0048, -0.0310,  ..., -0.0046,  0.0274, -0.0345],\n",
            "       device='cuda:0')), ('birnn_layers.3.BiGRU.bias_hh_l0', tensor([-0.0025,  0.0082,  0.0249,  ...,  0.0076,  0.0007,  0.0177],\n",
            "       device='cuda:0')), ('birnn_layers.3.BiGRU.weight_ih_l0_reverse', tensor([[ 0.0034, -0.0086, -0.0025,  ..., -0.0261, -0.0322, -0.0005],\n",
            "        [-0.0024, -0.0368,  0.0074,  ...,  0.0236,  0.0218,  0.0249],\n",
            "        [-0.0339,  0.0422, -0.0160,  ..., -0.0058,  0.0195,  0.0265],\n",
            "        ...,\n",
            "        [ 0.0183, -0.0297,  0.0058,  ..., -0.0373, -0.0166,  0.0093],\n",
            "        [-0.0368, -0.0091,  0.0176,  ...,  0.0390,  0.0377,  0.0317],\n",
            "        [ 0.0005, -0.0238,  0.0293,  ...,  0.0411, -0.0322,  0.0021]],\n",
            "       device='cuda:0')), ('birnn_layers.3.BiGRU.weight_hh_l0_reverse', tensor([[-0.0026, -0.0420, -0.0184,  ...,  0.0089,  0.0336, -0.0242],\n",
            "        [ 0.0075,  0.0369, -0.0426,  ..., -0.0228, -0.0111,  0.0008],\n",
            "        [-0.0166,  0.0361, -0.0275,  ..., -0.0141, -0.0177, -0.0381],\n",
            "        ...,\n",
            "        [-0.0340,  0.0435, -0.0002,  ..., -0.0197,  0.0094, -0.0256],\n",
            "        [-0.0021,  0.0203, -0.0059,  ...,  0.0265,  0.0238,  0.0022],\n",
            "        [ 0.0349, -0.0277, -0.0061,  ...,  0.0328,  0.0263,  0.0341]],\n",
            "       device='cuda:0')), ('birnn_layers.3.BiGRU.bias_ih_l0_reverse', tensor([-0.0269,  0.0186,  0.0020,  ...,  0.0302,  0.0040, -0.0254],\n",
            "       device='cuda:0')), ('birnn_layers.3.BiGRU.bias_hh_l0_reverse', tensor([ 0.0187,  0.0381, -0.0207,  ...,  0.0140, -0.0215,  0.0117],\n",
            "       device='cuda:0')), ('birnn_layers.3.layer_norm.weight', tensor([0.9881, 0.9843, 0.9948,  ..., 0.9911, 0.9874, 0.9921], device='cuda:0')), ('birnn_layers.3.layer_norm.bias', tensor([-0.0020, -0.0108, -0.0014,  ..., -0.0019, -0.0023,  0.0011],\n",
            "       device='cuda:0')), ('birnn_layers.4.BiGRU.weight_ih_l0', tensor([[-0.0355, -0.0175, -0.0259,  ..., -0.0014,  0.0034, -0.0487],\n",
            "        [ 0.0368,  0.0131, -0.0114,  ..., -0.0366,  0.0043,  0.0146],\n",
            "        [ 0.0271, -0.0385, -0.0324,  ...,  0.0071,  0.0162, -0.0277],\n",
            "        ...,\n",
            "        [-0.0159,  0.0115, -0.0211,  ..., -0.0431, -0.0256, -0.0359],\n",
            "        [ 0.0009,  0.0350,  0.0172,  ...,  0.0281, -0.0229, -0.0103],\n",
            "        [ 0.0112, -0.0249,  0.0384,  ..., -0.0359,  0.0393,  0.0048]],\n",
            "       device='cuda:0')), ('birnn_layers.4.BiGRU.weight_hh_l0', tensor([[ 0.0212, -0.0226, -0.0295,  ...,  0.0050,  0.0181, -0.0346],\n",
            "        [ 0.0154,  0.0405,  0.0120,  ..., -0.0031,  0.0292, -0.0435],\n",
            "        [ 0.0197, -0.0317,  0.0233,  ..., -0.0392, -0.0087,  0.0363],\n",
            "        ...,\n",
            "        [ 0.0152,  0.0061,  0.0063,  ..., -0.0357, -0.0010, -0.0211],\n",
            "        [-0.0077, -0.0418,  0.0204,  ..., -0.0388, -0.0399, -0.0096],\n",
            "        [-0.0076,  0.0319, -0.0218,  ..., -0.0072, -0.0155,  0.0078]],\n",
            "       device='cuda:0')), ('birnn_layers.4.BiGRU.bias_ih_l0', tensor([-0.0334, -0.0318,  0.0055,  ...,  0.0239, -0.0208,  0.0214],\n",
            "       device='cuda:0')), ('birnn_layers.4.BiGRU.bias_hh_l0', tensor([ 0.0036, -0.0190,  0.0098,  ..., -0.0090,  0.0217,  0.0149],\n",
            "       device='cuda:0')), ('birnn_layers.4.BiGRU.weight_ih_l0_reverse', tensor([[ 0.0071, -0.0329, -0.0230,  ...,  0.0398,  0.0061,  0.0349],\n",
            "        [ 0.0241,  0.0108, -0.0257,  ...,  0.0223, -0.0421,  0.0412],\n",
            "        [ 0.0202,  0.0154, -0.0225,  ..., -0.0165,  0.0098,  0.0028],\n",
            "        ...,\n",
            "        [-0.0302,  0.0220, -0.0291,  ...,  0.0222,  0.0390, -0.0429],\n",
            "        [-0.0262,  0.0250,  0.0097,  ..., -0.0182,  0.0181,  0.0299],\n",
            "        [-0.0220, -0.0042,  0.0035,  ...,  0.0256, -0.0357,  0.0154]],\n",
            "       device='cuda:0')), ('birnn_layers.4.BiGRU.weight_hh_l0_reverse', tensor([[-0.0422,  0.0378,  0.0290,  ..., -0.0186, -0.0195, -0.0410],\n",
            "        [-0.0401, -0.0201, -0.0220,  ..., -0.0134, -0.0242,  0.0216],\n",
            "        [-0.0144,  0.0346,  0.0048,  ..., -0.0362, -0.0264,  0.0105],\n",
            "        ...,\n",
            "        [-0.0096, -0.0228, -0.0114,  ...,  0.0381, -0.0253,  0.0249],\n",
            "        [-0.0084, -0.0191, -0.0380,  ..., -0.0425,  0.0377,  0.0148],\n",
            "        [ 0.0382, -0.0422,  0.0059,  ..., -0.0208, -0.0191,  0.0129]],\n",
            "       device='cuda:0')), ('birnn_layers.4.BiGRU.bias_ih_l0_reverse', tensor([ 0.0069,  0.0179, -0.0218,  ..., -0.0404, -0.0174, -0.0361],\n",
            "       device='cuda:0')), ('birnn_layers.4.BiGRU.bias_hh_l0_reverse', tensor([-0.0269, -0.0068, -0.0268,  ...,  0.0364,  0.0173, -0.0259],\n",
            "       device='cuda:0')), ('birnn_layers.4.layer_norm.weight', tensor([0.9954, 0.9920, 0.9905,  ..., 0.9967, 0.9917, 0.9927], device='cuda:0')), ('birnn_layers.4.layer_norm.bias', tensor([ 1.3054e-03, -2.5459e-03, -3.7015e-04,  ..., -4.2548e-03,\n",
            "        -7.9637e-04,  7.5314e-05], device='cuda:0')), ('classifier.0.weight', tensor([[ 0.0010, -0.0010,  0.0171,  ..., -0.0300, -0.0257,  0.0183],\n",
            "        [-0.0007, -0.0069,  0.0239,  ..., -0.0163,  0.0047,  0.0143],\n",
            "        [-0.0244, -0.0294,  0.0029,  ..., -0.0257, -0.0147, -0.0341],\n",
            "        ...,\n",
            "        [ 0.0204,  0.0159, -0.0291,  ..., -0.0064, -0.0179, -0.0072],\n",
            "        [-0.0261,  0.0065, -0.0052,  ..., -0.0122,  0.0118, -0.0282],\n",
            "        [-0.0095,  0.0079,  0.0281,  ..., -0.0196, -0.0172, -0.0227]],\n",
            "       device='cuda:0')), ('classifier.0.bias', tensor([-9.5953e-03, -1.0111e-02,  7.6566e-03,  7.3766e-03, -4.5552e-03,\n",
            "        -8.5969e-03, -1.8005e-02, -2.6727e-02, -2.1369e-02, -7.0441e-03,\n",
            "        -1.1648e-02, -1.7162e-02,  1.2123e-02,  9.8531e-03,  1.0575e-02,\n",
            "         6.0546e-03, -1.0457e-02,  2.4543e-02,  3.7836e-02,  3.1731e-02,\n",
            "         2.3562e-02,  2.8659e-02,  1.2169e-02, -2.7815e-02,  1.2635e-02,\n",
            "        -2.1692e-02, -1.4963e-02,  2.0912e-02, -1.6919e-02,  1.5690e-03,\n",
            "        -1.2000e-02, -1.5831e-02,  1.2174e-02, -5.5910e-04, -1.3398e-02,\n",
            "        -3.2346e-02, -3.0878e-02, -1.1696e-02,  2.6721e-02,  3.0208e-02,\n",
            "        -2.6576e-02, -8.0346e-04,  1.3318e-02, -1.5035e-02,  1.9573e-02,\n",
            "         2.3620e-02, -1.9850e-02,  2.4426e-02, -1.1362e-02,  3.3552e-02,\n",
            "         1.7243e-02,  2.1129e-02, -3.1659e-02,  3.2461e-03,  2.2937e-02,\n",
            "        -4.4858e-03, -8.3300e-04,  9.2908e-03, -4.8034e-03,  1.2249e-02,\n",
            "         2.1308e-03,  2.5952e-02,  2.2007e-02,  1.2396e-02, -7.4856e-04,\n",
            "        -1.0409e-02, -2.0059e-02, -1.9163e-02, -2.3595e-02, -1.6743e-02,\n",
            "         2.9624e-02,  1.7353e-04, -2.3414e-02,  1.6635e-02, -2.5435e-02,\n",
            "        -9.5061e-03,  2.0592e-02, -7.1334e-03, -2.3606e-02, -1.0293e-02,\n",
            "        -2.2278e-02,  1.2042e-02,  5.4351e-03, -8.3790e-03,  2.3078e-02,\n",
            "         8.1716e-03,  2.0152e-02, -1.6673e-02, -3.3671e-03,  3.0586e-02,\n",
            "         1.7838e-02,  1.7172e-02,  1.8887e-03, -2.1399e-03, -2.0545e-02,\n",
            "        -5.7254e-03, -4.7401e-03,  1.7426e-02, -1.6789e-02,  1.8147e-02,\n",
            "        -1.0449e-02, -1.3283e-02,  1.8398e-02,  7.2952e-03,  1.7856e-02,\n",
            "        -7.6737e-03, -1.3649e-02,  8.6508e-03, -7.2999e-03,  1.4166e-02,\n",
            "        -6.9640e-03,  2.4067e-02,  1.4204e-02, -2.0913e-02, -3.0721e-02,\n",
            "         1.3280e-02, -3.5613e-03, -7.0876e-03, -2.9121e-02, -2.9648e-02,\n",
            "         6.1947e-03,  2.2715e-02,  1.0471e-02, -2.1160e-02,  7.0198e-03,\n",
            "         2.9280e-02, -7.3845e-03, -3.1839e-02, -2.0232e-02,  3.6560e-03,\n",
            "         1.1524e-02, -9.5586e-03,  7.9378e-03, -3.1184e-02,  2.1487e-02,\n",
            "        -1.3533e-05, -1.9989e-02, -2.1726e-02, -1.1498e-02, -2.3948e-02,\n",
            "         3.8953e-03, -1.0469e-02,  3.4472e-03,  2.4107e-02, -2.7983e-02,\n",
            "         2.8886e-02,  1.7223e-02, -2.0381e-02, -8.1792e-03, -1.0832e-02,\n",
            "        -2.0031e-03,  2.5154e-02, -1.0873e-02,  4.2802e-03, -2.2300e-02,\n",
            "         2.6980e-02, -1.4317e-02,  2.9272e-02, -1.6457e-02,  2.7052e-02,\n",
            "        -1.7886e-02, -1.5665e-02, -6.3092e-03,  1.4987e-02, -8.4528e-03,\n",
            "        -5.5190e-03, -2.9171e-03,  2.1424e-02,  9.4078e-03,  1.5349e-02,\n",
            "        -2.2694e-02,  4.0333e-03,  2.8106e-02,  1.2557e-02,  1.5563e-02,\n",
            "         1.0357e-02,  7.5248e-03,  1.2698e-02, -1.0332e-02, -8.8994e-04,\n",
            "        -2.5482e-03,  3.3114e-03,  3.6854e-02,  2.2759e-03,  2.3989e-03,\n",
            "         1.1325e-02, -2.5207e-02,  1.4984e-02, -1.5693e-02, -1.6345e-02,\n",
            "         9.5882e-03,  1.9797e-02,  1.3983e-02, -2.0065e-02, -7.8224e-03,\n",
            "         4.0907e-03,  7.4069e-03,  5.3494e-03, -2.8836e-03,  3.4219e-02,\n",
            "         8.1575e-03,  1.0382e-02, -3.5436e-02,  1.2228e-03, -5.1843e-03,\n",
            "        -1.0314e-02, -1.8373e-02,  2.8016e-02, -2.7789e-02, -7.0697e-03,\n",
            "        -7.8467e-03, -1.3335e-02,  2.7645e-02,  1.7326e-02,  6.8571e-03,\n",
            "        -1.8623e-02,  2.2004e-02, -2.2139e-02, -2.4259e-02, -2.9750e-03,\n",
            "         1.4701e-02, -2.5384e-02, -3.0689e-02, -2.8674e-02, -1.9452e-02,\n",
            "        -8.7698e-03, -1.0630e-02, -4.2234e-04,  2.8180e-02, -1.8781e-02,\n",
            "         1.7128e-02,  1.9372e-02, -1.9777e-02,  3.1784e-03, -2.4192e-02,\n",
            "         2.6696e-02,  2.4239e-02, -1.6218e-02,  3.6929e-02,  2.0547e-02,\n",
            "        -2.8609e-02,  1.1612e-02, -2.4285e-03,  5.1575e-03,  1.1326e-02,\n",
            "        -3.0095e-02, -1.9001e-02,  3.3377e-02,  8.1286e-04, -1.0945e-02,\n",
            "        -1.2995e-02, -2.4741e-02,  2.2274e-02,  1.7412e-02,  1.7589e-02,\n",
            "        -3.6172e-02,  2.5184e-02,  2.6578e-02, -1.2830e-02,  2.1455e-02,\n",
            "        -1.5474e-02,  1.1857e-02, -1.5925e-02,  2.8472e-03, -1.4520e-02,\n",
            "        -1.0422e-02,  5.1877e-03,  1.3782e-02,  1.5882e-02,  1.6674e-02,\n",
            "        -1.7682e-03, -1.4085e-02, -3.1676e-02,  8.9051e-03, -2.8173e-02,\n",
            "        -1.4156e-02, -9.7040e-04,  1.0373e-02, -2.0109e-02,  8.9750e-03,\n",
            "         2.2183e-02,  5.7433e-03, -5.9587e-03, -2.1966e-02, -2.5160e-02,\n",
            "        -8.5620e-03,  1.2851e-02, -6.0473e-03, -2.5377e-02,  3.8022e-02,\n",
            "         6.3164e-03,  2.6213e-02, -1.8694e-02,  1.8433e-02,  9.2419e-03,\n",
            "        -9.1812e-03,  1.3707e-02, -1.1744e-02,  6.9621e-03,  2.4055e-02,\n",
            "        -7.7341e-04, -3.0077e-02, -1.0937e-03,  1.2837e-02, -2.8004e-02,\n",
            "        -2.4904e-02, -2.6348e-02, -2.3996e-02, -6.8500e-03, -7.1999e-03,\n",
            "        -2.2364e-02, -1.6897e-02,  2.1179e-02, -5.1979e-03, -1.0481e-03,\n",
            "        -3.0800e-03,  3.4166e-02,  2.5879e-02, -4.7867e-03, -1.2120e-02,\n",
            "         2.1197e-02,  1.7204e-02, -1.9117e-02,  1.3945e-02,  1.6292e-02,\n",
            "         1.7108e-02, -2.3226e-02,  1.4180e-03,  1.5087e-02,  1.6200e-03,\n",
            "         1.3571e-02, -1.2245e-02,  2.3125e-02, -2.5058e-02,  1.8778e-02,\n",
            "        -2.2312e-02,  2.5498e-02, -1.9843e-02,  1.7104e-02, -7.4513e-03,\n",
            "        -2.8623e-03,  2.5707e-02, -1.7958e-02,  1.7124e-02,  2.2700e-02,\n",
            "         1.0790e-02,  4.5936e-03, -3.4657e-03,  3.2146e-03, -1.2413e-02,\n",
            "        -4.7585e-03, -2.7471e-02, -2.5143e-02, -1.4472e-02,  5.8098e-03,\n",
            "         1.3621e-04, -2.2470e-02,  2.7878e-03, -1.1784e-02, -1.9080e-02,\n",
            "        -2.0540e-02, -1.4793e-02,  2.0926e-02, -3.0147e-02,  1.8470e-02,\n",
            "        -1.9355e-02,  9.7904e-03, -1.8342e-03, -7.9007e-04,  1.5759e-03,\n",
            "        -4.6701e-03,  2.1349e-02,  2.2418e-02, -1.8214e-02, -3.0812e-02,\n",
            "        -2.3094e-02, -1.4963e-02, -1.2653e-02, -4.6694e-04, -7.6361e-03,\n",
            "        -2.4736e-02, -8.8139e-03,  1.7139e-02,  2.5763e-02, -3.0658e-02,\n",
            "         1.6392e-02,  2.3134e-02, -2.8017e-02,  2.1786e-02, -1.3411e-02,\n",
            "         2.9642e-03,  1.0531e-02,  1.3965e-02,  1.3175e-02, -5.9889e-03,\n",
            "         1.9207e-02, -2.6105e-02,  6.6010e-03, -1.2664e-02, -4.8923e-03,\n",
            "         2.2953e-02,  2.6511e-02, -5.5689e-03,  1.5506e-03, -2.4454e-02,\n",
            "        -1.4474e-02,  1.1058e-02,  2.1487e-02, -1.9467e-02, -1.2128e-02,\n",
            "         1.4287e-02, -2.4218e-02, -6.5520e-03,  1.5525e-02,  3.7777e-03,\n",
            "         2.2219e-02, -2.6448e-02, -2.8609e-02, -5.2222e-03,  1.8594e-02,\n",
            "         1.5748e-02,  7.7628e-04,  1.8531e-02, -6.0936e-03, -1.8727e-02,\n",
            "         1.6969e-02,  5.8126e-03,  2.0469e-02, -2.6201e-02,  1.1061e-02,\n",
            "         1.4291e-02,  1.2486e-02, -3.5824e-03,  1.7246e-02,  2.6752e-02,\n",
            "        -2.2939e-02,  1.5894e-02, -2.3969e-02, -1.7275e-02,  3.0473e-04,\n",
            "         6.5688e-03,  1.2262e-02, -6.7049e-03, -2.2300e-02, -2.1766e-02,\n",
            "        -2.0191e-02,  2.6632e-02,  8.1131e-03,  2.2261e-02,  2.2945e-02,\n",
            "        -2.4723e-02,  1.4001e-02,  1.4927e-02, -9.5685e-03, -1.5503e-02,\n",
            "         1.0217e-02,  4.1689e-03,  1.3438e-02,  1.9486e-02,  3.0297e-02,\n",
            "        -8.7071e-03, -2.7833e-03, -8.1099e-03, -4.6329e-03,  1.7170e-02,\n",
            "        -3.0079e-02,  1.8670e-02,  2.4180e-02, -1.9512e-02,  3.2064e-02,\n",
            "         1.0842e-02, -4.5178e-03,  1.6805e-02, -2.4037e-02, -1.9300e-02,\n",
            "        -1.8359e-02,  1.9666e-02,  2.0315e-02, -4.8435e-03,  5.6079e-03,\n",
            "        -1.4953e-02,  2.3707e-02, -6.7544e-03,  6.9870e-03, -1.8382e-02,\n",
            "        -1.1252e-02,  1.1463e-02,  2.2514e-02,  1.8755e-02, -3.1553e-02,\n",
            "        -2.1316e-02,  2.8619e-02,  8.2241e-03,  9.5900e-03,  2.2859e-02,\n",
            "        -2.7285e-02, -7.2273e-03,  2.9882e-03, -7.9099e-03,  1.1099e-02,\n",
            "        -4.2243e-03, -1.1666e-02, -2.4792e-02, -2.9120e-02, -2.2614e-02,\n",
            "        -4.5375e-03,  1.4225e-02, -1.8515e-02,  6.8955e-03, -1.2505e-02,\n",
            "        -1.9032e-03,  2.0624e-02], device='cuda:0')), ('classifier.3.weight', tensor([[-0.0245, -0.0201,  0.0075,  ...,  0.0589, -0.0088, -0.0325],\n",
            "        [-0.0211,  0.0060,  0.0156,  ...,  0.0136,  0.0005,  0.0056],\n",
            "        [-0.0110,  0.0312,  0.0022,  ...,  0.0032, -0.0056,  0.0192],\n",
            "        ...,\n",
            "        [-0.0099,  0.0348, -0.0007,  ...,  0.0122, -0.0378, -0.0349],\n",
            "        [-0.0074,  0.0523,  0.0043,  ...,  0.0143, -0.0166, -0.0292],\n",
            "        [ 0.0095, -0.0438, -0.0285,  ..., -0.0402, -0.0342,  0.0249]],\n",
            "       device='cuda:0')), ('classifier.3.bias', tensor([-0.0409,  0.0067, -0.0290, -0.0195,  0.0137,  0.0330,  0.0025, -0.0172,\n",
            "         0.0325,  0.0202, -0.0191,  0.0062,  0.0114, -0.0036, -0.0473,  0.0247,\n",
            "         0.0216, -0.0305,  0.0080,  0.0296,  0.0075, -0.0134, -0.0345, -0.0134,\n",
            "        -0.0187, -0.0310,  0.0051, -0.0264,  0.0038], device='cuda:0'))])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "toFx1uY0veRq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "b96b0b67-6785-40a6-bf84-c0f2a0b91b33"
      },
      "source": [
        "# my game\n",
        "with torch.no_grad():\n",
        "    for i, _data in enumerate(test_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        print(output.dim())\n",
        "        print(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        print(labels[i][:label_lengths[i]])\n",
        "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "        #print(decoded_preds)\n",
        "        print(decoded_targets[0])\n",
        "\n",
        "\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "is it better than anywhere else\n",
            "tensor([10., 20.,  1., 10., 21.,  1.,  3.,  6., 21., 21.,  6., 19.,  1., 21.,\n",
            "         9.,  2., 15.,  1.,  2., 15., 26., 24.,  9.,  6., 19.,  6.,  1.,  6.,\n",
            "        13., 20.,  6.], device='cuda:0')\n",
            "is it better than anywhere else\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T57YOK2WjsPm"
      },
      "source": [
        "## Original Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XZodve8PGKfS",
        "colab": {}
      },
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set, experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "92kVVEr7GR6j",
        "colab": {}
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ucfQX3qN21az",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c7J9Gf4QtvNs"
      },
      "source": [
        "# My learning code \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOlL3uB6t4ZS",
        "colab": {}
      },
      "source": [
        "if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "        \n",
        "test_url=\"test-clean\"\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Cei39GivZFE",
        "colab": {}
      },
      "source": [
        "print(type(test_dataset))\n",
        "\n",
        "# https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(7)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "print(use_cuda)\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=5, # originaly 20\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "print(type(test_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WIe3KLAwzgZo",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "local_audio_transforms = nn.Sequential(\n",
        "    # https://pytorch.org/audio/transforms.html#melspectrogram\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)#,\n",
        "    # https://pytorch.org/audio/transforms.html#frequencymasking\n",
        "    #torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    # https://pytorch.org/audio/transforms.html#timemasking\n",
        "    #torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "spectrograms = []\n",
        "labels = []\n",
        "input_lengths = []\n",
        "label_lengths = []\n",
        "\n",
        "inx = 0\n",
        "for (waveform, sample_rate, utterance, B, C, D) in test_dataset:\n",
        "    inx += 1\n",
        "    print(type(sample_rate))\n",
        "    print(type(B))\n",
        "    print(type(C))\n",
        "    print(type(D))\n",
        "    print(type(waveform))\n",
        "    print(type(utterance))\n",
        "    print(utterance)\n",
        "    print(f\"sample_rate={sample_rate} B={B} C={C} D={D}\")\n",
        "    \n",
        "    print(type(waveform))\n",
        "    print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "    print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "    if inx == 10:\n",
        "        # define 2 sub plots\n",
        "        fig, axs = plt.subplots(2)\n",
        "        #fig.suptitle('Vertically stacked subplots')\n",
        "        #axs[0].plot(x, y)\n",
        "        #axs[1].plot(x, -y)\n",
        "\n",
        "        #plt.figure()\n",
        "        axs[0].plot(waveform.t().numpy())  \n",
        "\n",
        "        spec = local_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        print(\"type(spec) is \", type(spec)) \n",
        "        print(\"Shape of spec: {}\".format(spec.size()))\n",
        "        #    axs[1].imshow(spec.log2().detach().numpy(), cmap='gray')\n",
        "        axs[1].imshow(torch.transpose(spec, 0, 1).log2().numpy(), cmap='gray')\n",
        "\n",
        "\n",
        "    # the 4 elements of the test_loader are defined by  def data_processing(..)\n",
        "    #           return  spectrograms, labels, input_lengths, label_lengths\n",
        "    spectrograms.append(spec)\n",
        "    print(\"#>\", len(spectrograms))\n",
        "    label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "    print(utterance.lower())\n",
        "    print(text_transform.text_to_int(utterance.lower()))\n",
        "    #labels.append(label)\n",
        "    input_lengths.append(spec.shape[0]//2)\n",
        "    print(spec.shape)\n",
        "    print(spec.shape[0]//2)\n",
        "    #label_lengths.append(len(label))\n",
        "    print(len(label))\n",
        "\n",
        "\n",
        "    if inx == 5:\n",
        "        break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f2Vftj3hfdBD",
        "colab": {}
      },
      "source": [
        "# https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
        "\n",
        "learning_rate=5e-4\n",
        "batch_size=20\n",
        "epochs=10\n",
        "hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "model = SpeechRecognitionModel(\n",
        "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "print(model)\n",
        "print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "oneWhat = next(iter(test_loader))\n",
        "#oneWhat = iter(test_loader)\n",
        "print(type(oneWhat))\n",
        "print(len(oneWhat))\n",
        "print([type(x) for x in oneWhat])\n",
        "print(oneWhat[0].size())\n",
        "print(oneWhat[1].size())\n",
        "print(oneWhat[2])\n",
        "print(sum(oneWhat[2]))\n",
        "print(oneWhat[3])\n",
        "print(sum(oneWhat[3]))\n",
        "\n",
        "#writer.add_graph(model, oneWhat)\n",
        "#writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}