{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Building an end-to-end Speech Recognition model in PyTorch",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8e654655553f4294b99145a9c2040bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9a6f3e8585f34a0c8ec773ee0fc7468a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_33501956a978464ba5321010c990a35d",
              "IPY_MODEL_5dc0c0453ebf44dcb04fb798552ae0a6"
            ]
          }
        },
        "9a6f3e8585f34a0c8ec773ee0fc7468a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "33501956a978464ba5321010c990a35d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_440c732675f44845ad91ef56a0370571",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 346663984,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 346663984,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc0592648c454cc2bbca8ad88c3557ef"
          }
        },
        "5dc0c0453ebf44dcb04fb798552ae0a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fc85d62ddeb647c69bf9231ac0ac523f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 331M/331M [00:05&lt;00:00, 67.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efc97a3acdb24c81864d1d0b07ee297f"
          }
        },
        "440c732675f44845ad91ef56a0370571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc0592648c454cc2bbca8ad88c3557ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc85d62ddeb647c69bf9231ac0ac523f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efc97a3acdb24c81864d1d0b07ee297f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TzurV/TestMe/blob/master/EvalYesNo_to_Building_an_end_to_end_Speech_Recognition_model_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ibD6bsRPl8Qu"
      },
      "source": [
        "# Building an end-to-end Speech Recognition model in PyTorch - [AssemblyAI](https://www.assemblyai.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q1fXgsDQmK09"
      },
      "source": [
        "## installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BiFhpIc68QP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "3fec0373-0982-4209-e9d8-4c2e611cc71a"
      },
      "source": [
        "# mount Google Drive\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re \n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# output file\n",
        "#outputResFile = open(f\"/content/gdrive/My Drive/Colab_files/outputResFile.txt\",\"a\",0) \n",
        "#FORMAT = '%Y%m%d%H%M%S'\n",
        "#TodayDate = datetime.now().strftime(FORMAT)\n",
        "#print(TodayDate)\n",
        "#outputResFile.write(TodayDate+'\\n')\n",
        "#outputResFile.flush()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwfN8o17Bdp2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "a432048d-dc46-454b-cc14-5c20970febb2"
      },
      "source": [
        "!pip install torchaudio==0.4.0 torch==1.4.0 comet-ml==3.0.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/bc/3ebc127162d27bed33dc914606f10117d106680baae7ce83603ea09985fd/torchaudio-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 7.0MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 23kB/s \n",
            "\u001b[?25hCollecting comet-ml==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/c6/fac88f43f2aa61a09fee4ffb769c73fe93fe7de75764246e70967d31da09/comet_ml-3.0.2-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 44.8MB/s \n",
            "\u001b[?25hCollecting comet-git-pure>=0.19.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/7a/483413046e48908986a0f9a1d8a917e1da46ae58e6ba16b2ac71b3adf8d7/comet_git_pure-0.19.16-py3-none-any.whl (409kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 33.9MB/s \n",
            "\u001b[?25hCollecting websocket-client>=0.55.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.4MB/s \n",
            "\u001b[?25hCollecting wurlitzer>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/24/5e/f3bd8443bfdf96d2f5d10097d301076a9eb55637b7864e52d2d1a4d8c72a/wurlitzer-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (7.352.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (1.12.0)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (2.6.0)\n",
            "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\n",
            "Collecting netifaces>=0.10.7\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet-ml==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet-ml==3.0.2) (1.24.3)\n",
            "Collecting configobj; extra == \"ini\"\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (3.0.4)\n",
            "Building wheels for collected packages: configobj\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34546 sha256=4eefdf9dfb0e705fd6abe2eb938f51307718e0c8e52b97cdd8ceeca23aa4b5d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built configobj\n",
            "\u001b[31mERROR: torchvision 0.6.1+cu101 has requirement torch==1.5.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio, comet-git-pure, websocket-client, wurlitzer, configobj, everett, netifaces, comet-ml\n",
            "  Found existing installation: torch 1.5.1+cu101\n",
            "    Uninstalling torch-1.5.1+cu101:\n",
            "      Successfully uninstalled torch-1.5.1+cu101\n",
            "Successfully installed comet-git-pure-0.19.16 comet-ml-3.0.2 configobj-5.0.6 everett-1.0.2 netifaces-0.10.9 torch-1.4.0 torchaudio-0.4.0 websocket-client-0.57.0 wurlitzer-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSKHvy8DmOCQ"
      },
      "source": [
        "## Setting up your data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RVJs4Bk8FjjO",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# The core class of Comet.ml is an Experiment, a specific run of a script that \n",
        "# generated a result such as training a model on a single set of hyper parameters. \n",
        "# An Experiment will automatically log scripts output (stdout/stderr), code, \n",
        "# and command line arguments on any script and for the supported libraries will\n",
        "#  also log hyper parameters, metrics and model configuration.\n",
        "from comet_ml import Experiment\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
        "    between two sequences. Informally, the levenshtein disctance is defined as\n",
        "    the minimum number of single-character edits (substitutions, insertions or\n",
        "    deletions) required to change one word into the other. We can naturally\n",
        "    extend the edits to word level when calculate levenshtein disctance for\n",
        "    two sentences.\n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in word-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Levenshtein distance and word number of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in char-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Levenshtein distance and length of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
        "    hypothesis text in word-level. WER is defined as:\n",
        "    .. math::\n",
        "        WER = (Sw + Dw + Iw) / Nw\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sw is the number of words subsituted,\n",
        "        Dw is the number of words deleted,\n",
        "        Iw is the number of words inserted,\n",
        "        Nw is the number of words in the reference\n",
        "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
        "    that empty items will be removed when splitting sentences by delimiter.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Word error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If word number of reference is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
        "    hypothesis text in char-level. CER is defined as:\n",
        "    .. math::\n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
        "    encoded to unicode. Please draw an attention that the leading and tailing\n",
        "    space characters will be truncated and multiple consecutive space\n",
        "    characters in a sentence will be replaced by one space character.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Character error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If the reference length is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        q 18\n",
        "        r 19\n",
        "        s 20\n",
        "        t 21\n",
        "        u 22\n",
        "        v 23\n",
        "        w 24\n",
        "        x 25\n",
        "        y 26\n",
        "        z 27\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html#torch-nn-utils-rnn-pad-sequence\n",
        "    # pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. \n",
        "    # batch_first (bool, optional) – output will be in B x T x * if True, or in T x B x * otherwise\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4XdSlhAQnDEA"
      },
      "source": [
        "## The Model\n",
        "Base of of Deep Speech 2 with some personal improvements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "65H1-PCjm-FB",
        "colab": {}
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CuguNEzKnMOn"
      },
      "source": [
        "## The Training and Evaluating Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ydkqGeOwnPGY",
        "colab": {}
      },
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    with experiment.train():\n",
        "        for batch_idx, _data in enumerate(train_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "\n",
        "            experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
        "            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            iter_meter.step()\n",
        "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(spectrograms), data_len,\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with experiment.test():\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data \n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
        "    experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n",
        "    experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def main(learning_rate=5e-4, batch_size=20, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\",\n",
        "        experiment=Experiment(api_key='dummy_key', disabled=True)):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    experiment.log_parameters(hparams)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28).to(device)\n",
        "    # torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs\n",
        "    #Sets the learning rate of each parameter group according to the 1cycle learning rate policy. \n",
        "    #The 1cycle policy anneals the learning rate from an initial learning rate to \n",
        "    # some maximum learning rate and then from that maximum learning rate to some \n",
        "    # minimum learning rate much lower than the initial learning rate. \n",
        "    # This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    \n",
        "    iter_meter = IterMeter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
        "        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4qBGdkQSmW3a"
      },
      "source": [
        "## Setting up Comet\n",
        "If you have a comet account, fill in teh api key, project name and experiment name below. You can create an account at [comet.ml](comet.ml)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "edo8shRBFt4V",
        "colab": {}
      },
      "source": [
        "comet_api_key = \"\" # add your api key here\n",
        "project_name = \"speechrecognition\"\n",
        "experiment_name = \"speechrecognition-colab\"\n",
        "\n",
        "if comet_api_key:\n",
        "  experiment = Experiment(api_key=comet_api_key, project_name=project_name, parse_args=False)\n",
        "  experiment.set_name(experiment_name)\n",
        "  experiment.display()\n",
        "else:\n",
        "  experiment = Experiment(api_key='dummy_key', disabled=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HxRIb_WempDq"
      },
      "source": [
        "## GPU runtime\n",
        "If you are using a GPU runtime, this will let you know what GPU and how much memory is available. Adjust your batch_size depending on which GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nlUSuAJwlzo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "95f8d3c4-b248-4d54-c868-f69b7d783a33"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jul  4 10:55:50 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P8    36W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vBT776XweV8R"
      },
      "source": [
        "## ORIGINAL code sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hXvlWZeVpXfX"
      },
      "source": [
        "## Train\n",
        "this will download the data on first run and may take a while. \n",
        "\n",
        "If you have Comet.ml setup, you can start seeing your progress in the comet cell above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rXAaegPIe6TF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7sRfxHNjiGw",
        "colab": {}
      },
      "source": [
        "# my game\n",
        "learning_rate = 5e-4\n",
        "batch_size = 3\n",
        "epochs = 3\n",
        "libri_train_set = \"test-clean\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set, experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "toFx1uY0veRq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "b96b0b67-6785-40a6-bf84-c0f2a0b91b33"
      },
      "source": [
        "# my game\n",
        "with torch.no_grad():\n",
        "    for i, _data in enumerate(test_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        print(output.dim())\n",
        "        print(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        print(labels[i][:label_lengths[i]])\n",
        "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "        #print(decoded_preds)\n",
        "        print(decoded_targets[0])\n",
        "\n",
        "\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "is it better than anywhere else\n",
            "tensor([10., 20.,  1., 10., 21.,  1.,  3.,  6., 21., 21.,  6., 19.,  1., 21.,\n",
            "         9.,  2., 15.,  1.,  2., 15., 26., 24.,  9.,  6., 19.,  6.,  1.,  6.,\n",
            "        13., 20.,  6.], device='cuda:0')\n",
            "is it better than anywhere else\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ_77qLx_iTg",
        "colab_type": "text"
      },
      "source": [
        "Load Yes No "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdONBT7r_g6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "baa1a180-9c76-4c00-891d-d640996b66e6"
      },
      "source": [
        "# C:\\Users\\tzurv\\Google Drive\\Colab_files\\ctc-speech\\vctk-p225\\wav48\n",
        "def vctk_data_processing():\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "\n",
        "    mypath = '/content/gdrive/My Drive/Colab_files/ctc-speech/vctk-p225/wav48'\n",
        "\n",
        "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f)) and f.endswith('.wav')]\n",
        "\n",
        "    print(onlyfiles)\n",
        "\n",
        "    for f in onlyfiles:\n",
        "        # get file content\n",
        "        f1 = re.sub('.wav','',f)\n",
        "        txtFileName = '/content/gdrive/My Drive/ctc-speech/vctk-p225/txt/' + f1\n",
        "        txtFile = open(txtFileName,\"r\") \n",
        "        utterance = txtFile.read()\n",
        "        txtFile.close()\n",
        "\n",
        "        #https://tutorials.pytorch.kr/beginner/audio_preprocessing_tutorial.html\n",
        "        waveformFile = mypath + \"/\" + f\n",
        "        waveform, sample_rate = torchaudio.load(waveformFile)\n",
        "        print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "        print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "        # Since Resample applies to a single channel, we resample first channel here\n",
        "        channel = 0\n",
        "        transformed = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
        "        print(\"Shape of transformed waveform: {}\".format(transformed.size()))\n",
        "\n",
        "        spec = train_audio_transforms(transformed).squeeze(0).transpose(0, 1)\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html#torch-nn-utils-rnn-pad-sequence\n",
        "    # pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. \n",
        "    # batch_first (bool, optional) – output will be in B x T x * if True, or in T x B x * otherwise\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "vctk_eval_set = vctk_data_processing()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-31a59abcc54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mvctk_eval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvctk_data_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-31a59abcc54f>\u001b[0m in \u001b[0;36mvctk_data_processing\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmypath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/ctc-speech/vctk-p225/wav48'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0monlyfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monlyfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/ctc-speech/vctk-p225/wav48'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T57YOK2WjsPm"
      },
      "source": [
        "## Original Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XZodve8PGKfS",
        "colab": {}
      },
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set, experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "92kVVEr7GR6j",
        "colab": {}
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ucfQX3qN21az",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c7J9Gf4QtvNs"
      },
      "source": [
        "# My learning code \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOlL3uB6t4ZS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "8e654655553f4294b99145a9c2040bb1",
            "9a6f3e8585f34a0c8ec773ee0fc7468a",
            "33501956a978464ba5321010c990a35d",
            "5dc0c0453ebf44dcb04fb798552ae0a6",
            "440c732675f44845ad91ef56a0370571",
            "dc0592648c454cc2bbca8ad88c3557ef",
            "fc85d62ddeb647c69bf9231ac0ac523f",
            "efc97a3acdb24c81864d1d0b07ee297f"
          ]
        },
        "outputId": "70f3f8ab-1db1-4ec2-9197-99f76858c131"
      },
      "source": [
        "if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "        \n",
        "test_url=\"test-clean\"\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e654655553f4294b99145a9c2040bb1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=346663984.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8Cei39GivZFE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "892e5ca0-68c9-4782-94d8-e705671ffbd6"
      },
      "source": [
        "print(type(test_dataset))\n",
        "\n",
        "# https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(7)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "print(use_cuda)\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=5, # originaly 20\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "print(type(test_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torchaudio.datasets.librispeech.LIBRISPEECH'>\n",
            "True\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WIe3KLAwzgZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "ac047464-485c-44d6-fe26-bbe6dc201316"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "local_audio_transforms = nn.Sequential(\n",
        "    # https://pytorch.org/audio/transforms.html#melspectrogram\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)#,\n",
        "    # https://pytorch.org/audio/transforms.html#frequencymasking\n",
        "    #torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    # https://pytorch.org/audio/transforms.html#timemasking\n",
        "    #torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "spectrograms = []\n",
        "labels = []\n",
        "input_lengths = []\n",
        "label_lengths = []\n",
        "\n",
        "inx = 0\n",
        "for (waveform, sample_rate, utterance, B, C, D) in test_dataset:\n",
        "    inx += 1\n",
        "    print(type(sample_rate))\n",
        "    print(type(B))\n",
        "    print(type(C))\n",
        "    print(type(D))\n",
        "    print(type(waveform))\n",
        "    print(type(utterance))\n",
        "    print(utterance)\n",
        "    print(f\"sample_rate={sample_rate} B={B} C={C} D={D}\")\n",
        "    \n",
        "    print(type(waveform))\n",
        "    print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "    print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "    if inx == 1:\n",
        "        # define 2 sub plots\n",
        "        fig, axs = plt.subplots(2)\n",
        "        #fig.suptitle('Vertically stacked subplots')\n",
        "        #axs[0].plot(x, y)\n",
        "        #axs[1].plot(x, -y)\n",
        "\n",
        "        #plt.figure()\n",
        "        axs[0].plot(waveform.t().numpy())  \n",
        "\n",
        "        spec = local_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        print(\"type(spec) is \", type(spec)) \n",
        "        print(\"Shape of spec: {}\".format(spec.size()))\n",
        "        #    axs[1].imshow(spec.log2().detach().numpy(), cmap='gray')\n",
        "        axs[1].imshow(torch.transpose(spec, 0, 1).log2().numpy(), cmap='gray')\n",
        "\n",
        "\n",
        "    # the 4 elements of the test_loader are defined by  def data_processing(..)\n",
        "    #           return  spectrograms, labels, input_lengths, label_lengths\n",
        "    spectrograms.append(spec)\n",
        "    print(\"#>\", len(spectrograms))\n",
        "    label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "    print(utterance.lower())\n",
        "    print(text_transform.text_to_int(utterance.lower()))\n",
        "    #labels.append(label)\n",
        "    input_lengths.append(spec.shape[0]//2)\n",
        "    print(spec.shape)\n",
        "    print(spec.shape[0]//2)\n",
        "    #label_lengths.append(len(label))\n",
        "    print(len(label))\n",
        "\n",
        "\n",
        "    if inx == 2:\n",
        "        break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'str'>\n",
            "IT IS THE LANGUAGE OF WRETCHEDNESS\n",
            "sample_rate=16000 B=4507 C=16021 D=19\n",
            "<class 'torch.Tensor'>\n",
            "Shape of waveform: torch.Size([1, 46880])\n",
            "Sample rate of waveform: 16000\n",
            "type(spec) is  <class 'torch.Tensor'>\n",
            "Shape of spec: torch.Size([235, 128])\n",
            "#> 1\n",
            "it is the language of wretchedness\n",
            "[10, 21, 1, 10, 20, 1, 21, 9, 6, 1, 13, 2, 15, 8, 22, 2, 8, 6, 1, 16, 7, 1, 24, 19, 6, 21, 4, 9, 6, 5, 15, 6, 20, 20]\n",
            "torch.Size([235, 128])\n",
            "117\n",
            "34\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'int'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'str'>\n",
            "WHY SHOULD ONE NOT EXPLORE EVERYTHING AND STUDY EVERYTHING\n",
            "sample_rate=16000 B=4507 C=16021 D=11\n",
            "<class 'torch.Tensor'>\n",
            "Shape of waveform: torch.Size([1, 89840])\n",
            "Sample rate of waveform: 16000\n",
            "#> 2\n",
            "why should one not explore everything and study everything\n",
            "[24, 9, 26, 1, 20, 9, 16, 22, 13, 5, 1, 16, 15, 6, 1, 15, 16, 21, 1, 6, 25, 17, 13, 16, 19, 6, 1, 6, 23, 6, 19, 26, 21, 9, 10, 15, 8, 1, 2, 15, 5, 1, 20, 21, 22, 5, 26, 1, 6, 23, 6, 19, 26, 21, 9, 10, 15, 8]\n",
            "torch.Size([235, 128])\n",
            "117\n",
            "58\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f2Vftj3hfdBD",
        "colab": {}
      },
      "source": [
        "# https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter('runs/fashion_mnist_experiment_1')\n",
        "\n",
        "learning_rate=5e-4\n",
        "batch_size=20\n",
        "epochs=10\n",
        "hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "model = SpeechRecognitionModel(\n",
        "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "print(model)\n",
        "print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "oneWhat = next(iter(test_loader))\n",
        "#oneWhat = iter(test_loader)\n",
        "print(type(oneWhat))\n",
        "print(len(oneWhat))\n",
        "print([type(x) for x in oneWhat])\n",
        "print(oneWhat[0].size())\n",
        "print(oneWhat[1].size())\n",
        "print(oneWhat[2])\n",
        "print(sum(oneWhat[2]))\n",
        "print(oneWhat[3])\n",
        "print(sum(oneWhat[3]))\n",
        "\n",
        "#writer.add_graph(model, oneWhat)\n",
        "#writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}