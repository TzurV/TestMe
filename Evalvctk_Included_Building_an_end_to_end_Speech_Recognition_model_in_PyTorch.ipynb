{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Building an end-to-end Speech Recognition model in PyTorch",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c0fe6a941e74d53832a52eb9a875ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d82236b4fa8b4d22aaa8f88e337ffa2a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2c4872d124447f188555dba54e164aa",
              "IPY_MODEL_c576e82e02374a8a99f72106a1f0a2d0"
            ]
          }
        },
        "d82236b4fa8b4d22aaa8f88e337ffa2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2c4872d124447f188555dba54e164aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d7233a637da5457ebb54d5ed7a7becf6",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 346663984,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 346663984,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c8924ef2d234f4bb2560fb84c7da61f"
          }
        },
        "c576e82e02374a8a99f72106a1f0a2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_09ceafb2f9db4ed0bd3e32a0f666f27e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 331M/331M [00:06&lt;00:00, 51.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48ca086489f34bef8a7464fcf79056e0"
          }
        },
        "d7233a637da5457ebb54d5ed7a7becf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c8924ef2d234f4bb2560fb84c7da61f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09ceafb2f9db4ed0bd3e32a0f666f27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48ca086489f34bef8a7464fcf79056e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb1637ff36174cf29dc4266c2f562a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_750ef71953644213b8e8357c46c716e2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7d6fdde7793148bc8ff542d7d1e642ea",
              "IPY_MODEL_521f106c24b741b78dafea6037c03b78"
            ]
          }
        },
        "750ef71953644213b8e8357c46c716e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d6fdde7793148bc8ff542d7d1e642ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6bb4add5c2ea42ab9a1b5dab888140c7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99ddc52da3464cbe8597f4ef7c194443"
          }
        },
        "521f106c24b741b78dafea6037c03b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_341b107a43ee4bddb7148923b04952f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 231/231 [04:12&lt;00:00,  1.09s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbd44fefb5c94705bd9ef143d0e72a22"
          }
        },
        "6bb4add5c2ea42ab9a1b5dab888140c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99ddc52da3464cbe8597f4ef7c194443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "341b107a43ee4bddb7148923b04952f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbd44fefb5c94705bd9ef143d0e72a22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TzurV/TestMe/blob/master/Evalvctk_Included_Building_an_end_to_end_Speech_Recognition_model_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ibD6bsRPl8Qu"
      },
      "source": [
        "# Building an end-to-end Speech Recognition model in PyTorch - [AssemblyAI](https://www.assemblyai.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q1fXgsDQmK09"
      },
      "source": [
        "## installing the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BiFhpIc68QP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "21c293dc-8aa0-40e1-a859-da1898bed157"
      },
      "source": [
        "# mount Google Drive\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import re \n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# output file\n",
        "#outputResFile = open(f\"/content/gdrive/My Drive/Colab_files/outputResFile.txt\",\"a\",0) \n",
        "#FORMAT = '%Y%m%d%H%M%S'\n",
        "#TodayDate = datetime.now().strftime(FORMAT)\n",
        "#print(TodayDate)\n",
        "#outputResFile.write(TodayDate+'\\n')\n",
        "#outputResFile.flush()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwfN8o17Bdp2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "0958ce1d-7cfc-4a07-8b61-68d0733a524d"
      },
      "source": [
        "!pip install torchaudio==0.4.0 torch==1.4.0 comet-ml==3.0.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchaudio==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/bc/3ebc127162d27bed33dc914606f10117d106680baae7ce83603ea09985fd/torchaudio-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 7.0MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 23kB/s \n",
            "\u001b[?25hCollecting comet-ml==3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/c6/fac88f43f2aa61a09fee4ffb769c73fe93fe7de75764246e70967d31da09/comet_ml-3.0.2-py3-none-any.whl (170kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 34.9MB/s \n",
            "\u001b[?25hCollecting netifaces>=0.10.7\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (7.352.0)\n",
            "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (2.6.0)\n",
            "Collecting wurlitzer>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/1e/52f4effa64a447c4ec0fb71222799e2ac32c55b4b6c1725fccdf6123146e/wurlitzer-2.0.1-py2.py3-none-any.whl\n",
            "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet-ml==3.0.2) (1.12.0)\n",
            "Collecting websocket-client>=0.55.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.6MB/s \n",
            "\u001b[?25hCollecting comet-git-pure>=0.19.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/7a/483413046e48908986a0f9a1d8a917e1da46ae58e6ba16b2ac71b3adf8d7/comet_git_pure-0.19.16-py3-none-any.whl (409kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 39.4MB/s \n",
            "\u001b[?25hCollecting configobj; extra == \"ini\"\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet-ml==3.0.2) (3.0.4)\n",
            "Building wheels for collected packages: configobj\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34546 sha256=e4e3fb5b86e808ab20eebdbc3d652cd1ac985d470e0e8076bdb1fa2586109687\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built configobj\n",
            "\u001b[31mERROR: torchvision 0.6.1+cu101 has requirement torch==1.5.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchaudio, netifaces, wurlitzer, configobj, everett, websocket-client, comet-git-pure, comet-ml\n",
            "  Found existing installation: torch 1.5.1+cu101\n",
            "    Uninstalling torch-1.5.1+cu101:\n",
            "      Successfully uninstalled torch-1.5.1+cu101\n",
            "Successfully installed comet-git-pure-0.19.16 comet-ml-3.0.2 configobj-5.0.6 everett-1.0.2 netifaces-0.10.9 torch-1.4.0 torchaudio-0.4.0 websocket-client-0.57.0 wurlitzer-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSKHvy8DmOCQ"
      },
      "source": [
        "## Setting up your data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RVJs4Bk8FjjO",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# The core class of Comet.ml is an Experiment, a specific run of a script that \n",
        "# generated a result such as training a model on a single set of hyper parameters. \n",
        "# An Experiment will automatically log scripts output (stdout/stderr), code, \n",
        "# and command line arguments on any script and for the supported libraries will\n",
        "#  also log hyper parameters, metrics and model configuration.\n",
        "from comet_ml import Experiment\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
        "    between two sequences. Informally, the levenshtein disctance is defined as\n",
        "    the minimum number of single-character edits (substitutions, insertions or\n",
        "    deletions) required to change one word into the other. We can naturally\n",
        "    extend the edits to word level when calculate levenshtein disctance for\n",
        "    two sentences.\n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in word-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Levenshtein distance and word number of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
        "    hypothesis sequence in char-level.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Levenshtein distance and length of reference sentence.\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
        "    hypothesis text in word-level. WER is defined as:\n",
        "    .. math::\n",
        "        WER = (Sw + Dw + Iw) / Nw\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sw is the number of words subsituted,\n",
        "        Dw is the number of words deleted,\n",
        "        Iw is the number of words inserted,\n",
        "        Nw is the number of words in the reference\n",
        "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
        "    that empty items will be removed when splitting sentences by delimiter.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param delimiter: Delimiter of input sentences.\n",
        "    :type delimiter: char\n",
        "    :return: Word error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If word number of reference is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
        "    hypothesis text in char-level. CER is defined as:\n",
        "    .. math::\n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "    where\n",
        "    .. code-block:: text\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
        "    encoded to unicode. Please draw an attention that the leading and tailing\n",
        "    space characters will be truncated and multiple consecutive space\n",
        "    characters in a sentence will be replaced by one space character.\n",
        "    :param reference: The reference sentence.\n",
        "    :type reference: basestring\n",
        "    :param hypothesis: The hypothesis sentence.\n",
        "    :type hypothesis: basestring\n",
        "    :param ignore_case: Whether case-sensitive or not.\n",
        "    :type ignore_case: bool\n",
        "    :param remove_space: Whether remove internal space characters\n",
        "    :type remove_space: bool\n",
        "    :return: Character error rate.\n",
        "    :rtype: float\n",
        "    :raises ValueError: If the reference length is zero.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer\n",
        "\n",
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        q 18\n",
        "        r 19\n",
        "        s 20\n",
        "        t 21\n",
        "        u 22\n",
        "        v 23\n",
        "        w 24\n",
        "        x 25\n",
        "        y 26\n",
        "        z 27\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    # https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html#torch-nn-utils-rnn-pad-sequence\n",
        "    # pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. \n",
        "    # batch_first (bool, optional) – output will be in B x T x * if True, or in T x B x * otherwise\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    #print(f\"len(label_lengths) = {len(label_lengths)}\")\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4XdSlhAQnDEA"
      },
      "source": [
        "## The Model\n",
        "Base of of Deep Speech 2 with some personal improvements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "65H1-PCjm-FB",
        "colab": {}
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMbJUdQ5Slq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "8c0fe6a941e74d53832a52eb9a875ed3",
            "d82236b4fa8b4d22aaa8f88e337ffa2a",
            "d2c4872d124447f188555dba54e164aa",
            "c576e82e02374a8a99f72106a1f0a2d0",
            "d7233a637da5457ebb54d5ed7a7becf6",
            "5c8924ef2d234f4bb2560fb84c7da61f",
            "09ceafb2f9db4ed0bd3e32a0f666f27e",
            "48ca086489f34bef8a7464fcf79056e0"
          ]
        },
        "outputId": "cf34ef8b-4f51-4f58-e2eb-743670d7652a"
      },
      "source": [
        "# https://pytorch.org/audio/datasets.html\n",
        "# All datasets are subclasses of torch.utils.data.Dataset i.e, they have __getitem__ \n",
        "# and __len__ methods implemented. Hence, they can all be passed to a torch.utils.data.DataLoader \n",
        "# which can load multiple samples parallelly using torch.multiprocessing workers.\n",
        "\n",
        "# https://pytorch.org/audio/_modules/torchaudio/datasets/librispeech.html\n",
        "# SOURCE CODE FOR TORCHAUDIO.DATASETS.LIBRISPEECH\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "        \n",
        "test_url=\"test-clean\"\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "print(type(test_dataset))\n",
        "\n",
        "# https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(7)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "print(use_cuda)\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=5, # originaly 20\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "print(type(test_loader))\n",
        "\n",
        "for i, _data in enumerate(test_loader):\n",
        "    spectrograms, labels, input_lengths, label_lengths = _data\n",
        "    print(input_lengths, label_lengths)\n",
        "    if i == 10:\n",
        "        break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c0fe6a941e74d53832a52eb9a875ed3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=346663984.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "<class 'torchaudio.datasets.librispeech.LIBRISPEECH'>\n",
            "True\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "[117, 225, 109, 168, 245] [34, 58, 30, 48, 39]\n",
            "[1265, 196, 119, 254, 209] [347, 61, 32, 54, 66]\n",
            "[104, 80, 157, 251, 88] [18, 20, 35, 58, 20]\n",
            "[453, 248, 124, 206, 572] [101, 64, 34, 45, 177]\n",
            "[607, 84, 301, 131, 247] [176, 15, 83, 26, 71]\n",
            "[127, 222, 155, 579, 324] [27, 78, 33, 148, 92]\n",
            "[85, 369, 184, 160, 156] [13, 96, 39, 48, 48]\n",
            "[592, 617, 452, 1398, 154] [176, 148, 131, 421, 39]\n",
            "[236, 418, 184, 451, 217] [73, 132, 51, 140, 43]\n",
            "[1124, 691, 900, 105, 214] [320, 185, 247, 23, 54]\n",
            "[423, 289, 144, 155, 173] [127, 76, 33, 33, 36]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mD9kOg3e_uN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "cac65da4-884b-4053-cea2-bc7a7d809f31"
      },
      "source": [
        "print(type(test_dataset))\n",
        "print(len(test_dataset))\n",
        "print(test_dataset[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torchaudio.datasets.librispeech.LIBRISPEECH'>\n",
            "2620\n",
            "(tensor([[-0.0011, -0.0014, -0.0002,  ...,  0.0012,  0.0014,  0.0004]]), 16000, 'IT IS THE LANGUAGE OF WRETCHEDNESS', 4507, 16021, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9bd6kNzgi21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183,
          "referenced_widgets": [
            "bb1637ff36174cf29dc4266c2f562a61",
            "750ef71953644213b8e8357c46c716e2",
            "7d6fdde7793148bc8ff542d7d1e642ea",
            "521f106c24b741b78dafea6037c03b78",
            "6bb4add5c2ea42ab9a1b5dab888140c7",
            "99ddc52da3464cbe8597f4ef7c194443",
            "341b107a43ee4bddb7148923b04952f3",
            "fbd44fefb5c94705bd9ef143d0e72a22"
          ]
        },
        "outputId": "5de0a3f4-2c27-4626-dc91-187cb45c5ffe"
      },
      "source": [
        "# create dataset for vctk similart to LIBRISPEECH\n",
        "#     Create a Dataset for LibriSpeech. Each item is a tuple of the form:\n",
        "#     waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "vctk_eval_dataset = []\n",
        "\n",
        "mypath = '/content/gdrive/My Drive/Colab_files/ctc-speech/vctk-p225/wav48/p225'\n",
        "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f)) ] #and f.endswith('.wav')\n",
        "print(onlyfiles[0], \", files number: \", len(onlyfiles))\n",
        "\n",
        "\n",
        "for indx in tqdm(range(len(onlyfiles))):\n",
        "    f = onlyfiles[indx]\n",
        "\n",
        "    # get file content\n",
        "    f1 = re.sub('.wav','.txt',f)\n",
        "    txtFileName = '/content/gdrive/My Drive/Colab_files/ctc-speech/vctk-p225/txt/p225/' + f1\n",
        "    txtFile = open(txtFileName,\"r\") \n",
        "    utterance = txtFile.read()\n",
        "    txtFile.close()\n",
        "    utterance = re.sub('[,|.|\\-|?|\\n]', '', utterance)\n",
        "    #print(utterance)\n",
        "\n",
        "    #https://tutorials.pytorch.kr/beginner/audio_preprocessing_tutorial.html\n",
        "    waveformFile = mypath + \"/\" + f\n",
        "    waveform, sample_rate = torchaudio.load(waveformFile)\n",
        "    #print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "    #print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "    # Since Resample applies to a single channel, we resample first channel here\n",
        "    channel = 0\n",
        "    transformed = torchaudio.transforms.Resample(sample_rate, 16000)(waveform)\n",
        "\n",
        "    #print(type(transformed), type(utterance))\n",
        "    vctk_eval_dataset.append((transformed, 16000, utterance, None, None, f))\n",
        "    #if indx == 10:\n",
        "    #    break\n",
        "\n",
        "print(type(vctk_eval_dataset))\n",
        "print(len(vctk_eval_dataset))\n",
        "print(vctk_eval_dataset[0])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p225_001.wav , files number:  231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb1637ff36174cf29dc4266c2f562a61",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=231.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "<class 'list'>\n",
            "231\n",
            "(tensor([[0.0038, 0.0069, 0.0060,  ..., 0.0079, 0.0088, 0.0056]]), 16000, 'Please call Stella', None, None, 'p225_001.wav')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfM_J7sljGtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7353afdb-17bf-464e-ea6d-8d1fb473aec0"
      },
      "source": [
        "# The right version of loading (6-July-2020)\n",
        "print(use_cuda)\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "vctk_eval_loader = data.DataLoader(dataset=vctk_eval_dataset,\n",
        "                                batch_size=5, # originaly 20\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "print(type(vctk_eval_loader))\n",
        "\n",
        "for i, _data in enumerate(vctk_eval_loader):\n",
        "    spectrograms, labels, input_lengths, label_lengths = _data\n",
        "    print(input_lengths, label_lengths)\n",
        "    if i == 10:\n",
        "        break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n",
            "[82, 162, 312, 176, 261] [18, 53, 99, 66, 101]\n",
            "[311, 248, 323, 224, 142] [85, 67, 113, 62, 36]\n",
            "[374, 341, 230, 266, 314] [123, 74, 63, 74, 110]\n",
            "[205, 254, 287, 287, 514] [54, 89, 116, 57, 156]\n",
            "[302, 526, 298, 142, 103] [99, 175, 99, 37, 16]\n",
            "[125, 126, 72, 181, 266] [19, 22, 14, 61, 69]\n",
            "[249, 160, 159, 155, 256] [50, 19, 41, 29, 71]\n",
            "[101, 338, 215, 152, 91] [23, 52, 68, 34, 23]\n",
            "[104, 188, 126, 135, 140] [27, 63, 22, 31, 28]\n",
            "[259, 152, 212, 124, 200] [58, 33, 44, 46, 51]\n",
            "[135, 126, 72, 120, 137] [41, 34, 23, 26, 27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CuguNEzKnMOn"
      },
      "source": [
        "## The Training and Evaluating Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ydkqGeOwnPGY",
        "colab": {}
      },
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    with experiment.train():\n",
        "        for batch_idx, _data in enumerate(train_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            loss.backward()\n",
        "\n",
        "            experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
        "            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            iter_meter.step()\n",
        "            if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(spectrograms), data_len,\n",
        "                    100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with experiment.test():\n",
        "        with torch.no_grad():\n",
        "            for i, _data in enumerate(test_loader):\n",
        "                spectrograms, labels, input_lengths, label_lengths = _data \n",
        "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "                output = model(spectrograms)  # (batch, time, n_class)\n",
        "                output = F.log_softmax(output, dim=2)\n",
        "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "                test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "                for j in range(len(decoded_preds)):\n",
        "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())\n",
        "    experiment.log_metric('cer', avg_cer, step=iter_meter.get())\n",
        "    experiment.log_metric('wer', avg_wer, step=iter_meter.get())\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def main(learning_rate=5e-4, batch_size=20, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\",\n",
        "        experiment=Experiment(api_key='dummy_key', disabled=True)):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    experiment.log_parameters(hparams)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    # one More evaluation\n",
        "    #vctk_eval_set = vctk_data_processing()\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28).to(device)\n",
        "    # torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs\n",
        "    #Sets the learning rate of each parameter group according to the 1cycle learning rate policy. \n",
        "    #The 1cycle policy anneals the learning rate from an initial learning rate to \n",
        "    # some maximum learning rate and then from that maximum learning rate to some \n",
        "    # minimum learning rate much lower than the initial learning rate. \n",
        "    # This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    \n",
        "    iter_meter = IterMeter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\n",
        "        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)\n",
        "        test(model, device, vctk_eval_loader, criterion, epoch, iter_meter, experiment)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4qBGdkQSmW3a"
      },
      "source": [
        "## Setting up Comet\n",
        "If you have a comet account, fill in teh api key, project name and experiment name below. You can create an account at [comet.ml](comet.ml)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "edo8shRBFt4V",
        "colab": {}
      },
      "source": [
        "comet_api_key = \"\" # add your api key here\n",
        "project_name = \"speechrecognition\"\n",
        "experiment_name = \"speechrecognition-colab\"\n",
        "\n",
        "if comet_api_key:\n",
        "  experiment = Experiment(api_key=comet_api_key, project_name=project_name, parse_args=False)\n",
        "  experiment.set_name(experiment_name)\n",
        "  experiment.display()\n",
        "else:\n",
        "  experiment = Experiment(api_key='dummy_key', disabled=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HxRIb_WempDq"
      },
      "source": [
        "## GPU runtime\n",
        "If you are using a GPU runtime, this will let you know what GPU and how much memory is available. Adjust your batch_size depending on which GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nlUSuAJwlzo8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "13309e45-3447-43bc-a2a4-4b223b0c231c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul  6 12:44:46 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    59W / 149W |    319MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vBT776XweV8R"
      },
      "source": [
        "## ORIGINAL code sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hXvlWZeVpXfX"
      },
      "source": [
        "## Train\n",
        "this will download the data on first run and may take a while. \n",
        "\n",
        "If you have Comet.ml setup, you can start seeing your progress in the comet cell above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rXAaegPIe6TF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y7sRfxHNjiGw",
        "colab": {}
      },
      "source": [
        "# my game\n",
        "learning_rate = 5e-4\n",
        "batch_size = 3\n",
        "epochs = 3\n",
        "libri_train_set = \"test-clean\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set, experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T57YOK2WjsPm"
      },
      "source": [
        "## Original Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XZodve8PGKfS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56891022-f42a-4880-aa20-db575ac8e2ef"
      },
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 10\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set, experiment)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpeechRecognitionModel(\n",
            "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (rescnn_layers): Sequential(\n",
            "    (0): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (2): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (birnn_layers): Sequential(\n",
            "    (0): BidirectionalGRU(\n",
            "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (2): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (3): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (4): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): GELU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "  )\n",
            ")\n",
            "Num Model Parameters 23705373\n",
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 7.081871\n",
            "Train Epoch: 1 [1000/28539 (4%)]\tLoss: 2.944707\n",
            "Train Epoch: 1 [2000/28539 (7%)]\tLoss: 2.857919\n",
            "Train Epoch: 1 [3000/28539 (11%)]\tLoss: 2.838245\n",
            "Train Epoch: 1 [4000/28539 (14%)]\tLoss: 2.846269\n",
            "Train Epoch: 1 [5000/28539 (18%)]\tLoss: 2.885026\n",
            "Train Epoch: 1 [6000/28539 (21%)]\tLoss: 2.820604\n",
            "Train Epoch: 1 [7000/28539 (25%)]\tLoss: 2.857621\n",
            "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 2.848951\n",
            "Train Epoch: 1 [9000/28539 (32%)]\tLoss: 2.927544\n",
            "Train Epoch: 1 [10000/28539 (35%)]\tLoss: 2.860066\n",
            "Train Epoch: 1 [11000/28539 (39%)]\tLoss: 2.844382\n",
            "Train Epoch: 1 [12000/28539 (42%)]\tLoss: 2.864240\n",
            "Train Epoch: 1 [13000/28539 (46%)]\tLoss: 2.854836\n",
            "Train Epoch: 1 [14000/28539 (49%)]\tLoss: 2.851149\n",
            "Train Epoch: 1 [15000/28539 (53%)]\tLoss: 2.807771\n",
            "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 2.620575\n",
            "Train Epoch: 1 [17000/28539 (60%)]\tLoss: 2.471540\n",
            "Train Epoch: 1 [18000/28539 (63%)]\tLoss: 2.235547\n",
            "Train Epoch: 1 [19000/28539 (67%)]\tLoss: 2.123178\n",
            "Train Epoch: 1 [20000/28539 (70%)]\tLoss: 2.032626\n",
            "Train Epoch: 1 [21000/28539 (74%)]\tLoss: 2.007258\n",
            "Train Epoch: 1 [22000/28539 (77%)]\tLoss: 1.933558\n",
            "Train Epoch: 1 [23000/28539 (81%)]\tLoss: 1.768545\n",
            "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 1.862976\n",
            "Train Epoch: 1 [25000/28539 (88%)]\tLoss: 1.876191\n",
            "Train Epoch: 1 [26000/28539 (91%)]\tLoss: 1.680737\n",
            "Train Epoch: 1 [27000/28539 (95%)]\tLoss: 1.684040\n",
            "Train Epoch: 1 [28000/28539 (98%)]\tLoss: 1.717114\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.4730, Average CER: 0.433436 Average WER: 0.9465\n",
            "\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.8786, Average CER: 0.525325 Average WER: 0.9526\n",
            "\n",
            "Train Epoch: 2 [0/28539 (0%)]\tLoss: 1.649067\n",
            "Train Epoch: 2 [1000/28539 (4%)]\tLoss: 1.598149\n",
            "Train Epoch: 2 [2000/28539 (7%)]\tLoss: 1.550065\n",
            "Train Epoch: 2 [3000/28539 (11%)]\tLoss: 1.502260\n",
            "Train Epoch: 2 [4000/28539 (14%)]\tLoss: 1.695182\n",
            "Train Epoch: 2 [5000/28539 (18%)]\tLoss: 1.574072\n",
            "Train Epoch: 2 [6000/28539 (21%)]\tLoss: 1.459165\n",
            "Train Epoch: 2 [7000/28539 (25%)]\tLoss: 1.581314\n",
            "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 1.555330\n",
            "Train Epoch: 2 [9000/28539 (32%)]\tLoss: 1.427900\n",
            "Train Epoch: 2 [10000/28539 (35%)]\tLoss: 1.463814\n",
            "Train Epoch: 2 [11000/28539 (39%)]\tLoss: 1.607295\n",
            "Train Epoch: 2 [12000/28539 (42%)]\tLoss: 1.292960\n",
            "Train Epoch: 2 [13000/28539 (46%)]\tLoss: 1.530624\n",
            "Train Epoch: 2 [14000/28539 (49%)]\tLoss: 1.467056\n",
            "Train Epoch: 2 [15000/28539 (53%)]\tLoss: 1.625368\n",
            "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 1.404928\n",
            "Train Epoch: 2 [17000/28539 (60%)]\tLoss: 1.419679\n",
            "Train Epoch: 2 [18000/28539 (63%)]\tLoss: 1.272411\n",
            "Train Epoch: 2 [19000/28539 (67%)]\tLoss: 1.299514\n",
            "Train Epoch: 2 [20000/28539 (70%)]\tLoss: 1.304320\n",
            "Train Epoch: 2 [21000/28539 (74%)]\tLoss: 1.255257\n",
            "Train Epoch: 2 [22000/28539 (77%)]\tLoss: 1.342046\n",
            "Train Epoch: 2 [23000/28539 (81%)]\tLoss: 1.247303\n",
            "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 1.208407\n",
            "Train Epoch: 2 [25000/28539 (88%)]\tLoss: 1.260836\n",
            "Train Epoch: 2 [26000/28539 (91%)]\tLoss: 1.204697\n",
            "Train Epoch: 2 [27000/28539 (95%)]\tLoss: 1.100295\n",
            "Train Epoch: 2 [28000/28539 (98%)]\tLoss: 1.216080\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.0282, Average CER: 0.313598 Average WER: 0.7929\n",
            "\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.6684, Average CER: 0.439910 Average WER: 0.8609\n",
            "\n",
            "Train Epoch: 3 [0/28539 (0%)]\tLoss: 1.256476\n",
            "Train Epoch: 3 [1000/28539 (4%)]\tLoss: 1.218951\n",
            "Train Epoch: 3 [2000/28539 (7%)]\tLoss: 1.238483\n",
            "Train Epoch: 3 [3000/28539 (11%)]\tLoss: 1.193682\n",
            "Train Epoch: 3 [4000/28539 (14%)]\tLoss: 1.079851\n",
            "Train Epoch: 3 [5000/28539 (18%)]\tLoss: 1.090977\n",
            "Train Epoch: 3 [6000/28539 (21%)]\tLoss: 1.276899\n",
            "Train Epoch: 3 [7000/28539 (25%)]\tLoss: 1.266763\n",
            "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 1.218323\n",
            "Train Epoch: 3 [9000/28539 (32%)]\tLoss: 1.049543\n",
            "Train Epoch: 3 [10000/28539 (35%)]\tLoss: 1.224690\n",
            "Train Epoch: 3 [11000/28539 (39%)]\tLoss: 1.096433\n",
            "Train Epoch: 3 [12000/28539 (42%)]\tLoss: 1.027898\n",
            "Train Epoch: 3 [13000/28539 (46%)]\tLoss: 1.218688\n",
            "Train Epoch: 3 [14000/28539 (49%)]\tLoss: 1.085382\n",
            "Train Epoch: 3 [15000/28539 (53%)]\tLoss: 1.071976\n",
            "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 1.062710\n",
            "Train Epoch: 3 [17000/28539 (60%)]\tLoss: 1.172443\n",
            "Train Epoch: 3 [18000/28539 (63%)]\tLoss: 1.275084\n",
            "Train Epoch: 3 [19000/28539 (67%)]\tLoss: 0.996555\n",
            "Train Epoch: 3 [20000/28539 (70%)]\tLoss: 1.195534\n",
            "Train Epoch: 3 [21000/28539 (74%)]\tLoss: 1.099317\n",
            "Train Epoch: 3 [22000/28539 (77%)]\tLoss: 1.245415\n",
            "Train Epoch: 3 [23000/28539 (81%)]\tLoss: 1.003859\n",
            "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 1.175440\n",
            "Train Epoch: 3 [25000/28539 (88%)]\tLoss: 1.213840\n",
            "Train Epoch: 3 [26000/28539 (91%)]\tLoss: 1.145122\n",
            "Train Epoch: 3 [27000/28539 (95%)]\tLoss: 1.040284\n",
            "Train Epoch: 3 [28000/28539 (98%)]\tLoss: 0.877707\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.8661, Average CER: 0.261533 Average WER: 0.6994\n",
            "\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.5889, Average CER: 0.415746 Average WER: 0.8193\n",
            "\n",
            "Train Epoch: 4 [0/28539 (0%)]\tLoss: 0.929371\n",
            "Train Epoch: 4 [1000/28539 (4%)]\tLoss: 1.025570\n",
            "Train Epoch: 4 [2000/28539 (7%)]\tLoss: 1.067460\n",
            "Train Epoch: 4 [3000/28539 (11%)]\tLoss: 1.096316\n",
            "Train Epoch: 4 [4000/28539 (14%)]\tLoss: 0.963856\n",
            "Train Epoch: 4 [5000/28539 (18%)]\tLoss: 1.115037\n",
            "Train Epoch: 4 [6000/28539 (21%)]\tLoss: 1.017943\n",
            "Train Epoch: 4 [7000/28539 (25%)]\tLoss: 1.294105\n",
            "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 1.057769\n",
            "Train Epoch: 4 [9000/28539 (32%)]\tLoss: 1.013039\n",
            "Train Epoch: 4 [10000/28539 (35%)]\tLoss: 1.101240\n",
            "Train Epoch: 4 [11000/28539 (39%)]\tLoss: 1.212219\n",
            "Train Epoch: 4 [12000/28539 (42%)]\tLoss: 1.206857\n",
            "Train Epoch: 4 [13000/28539 (46%)]\tLoss: 1.033374\n",
            "Train Epoch: 4 [14000/28539 (49%)]\tLoss: 1.098582\n",
            "Train Epoch: 4 [15000/28539 (53%)]\tLoss: 1.004004\n",
            "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 1.160936\n",
            "Train Epoch: 4 [17000/28539 (60%)]\tLoss: 0.984394\n",
            "Train Epoch: 4 [18000/28539 (63%)]\tLoss: 0.948865\n",
            "Train Epoch: 4 [19000/28539 (67%)]\tLoss: 0.979786\n",
            "Train Epoch: 4 [20000/28539 (70%)]\tLoss: 1.153539\n",
            "Train Epoch: 4 [21000/28539 (74%)]\tLoss: 1.006438\n",
            "Train Epoch: 4 [22000/28539 (77%)]\tLoss: 0.848188\n",
            "Train Epoch: 4 [23000/28539 (81%)]\tLoss: 0.976026\n",
            "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 0.870111\n",
            "Train Epoch: 4 [25000/28539 (88%)]\tLoss: 1.006543\n",
            "Train Epoch: 4 [26000/28539 (91%)]\tLoss: 0.947086\n",
            "Train Epoch: 4 [27000/28539 (95%)]\tLoss: 0.923780\n",
            "Train Epoch: 4 [28000/28539 (98%)]\tLoss: 0.944378\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.7404, Average CER: 0.227216 Average WER: 0.6204\n",
            "\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.5009, Average CER: 0.390778 Average WER: 0.7907\n",
            "\n",
            "Train Epoch: 5 [0/28539 (0%)]\tLoss: 0.846223\n",
            "Train Epoch: 5 [1000/28539 (4%)]\tLoss: 0.872638\n",
            "Train Epoch: 5 [2000/28539 (7%)]\tLoss: 0.826274\n",
            "Train Epoch: 5 [3000/28539 (11%)]\tLoss: 0.815418\n",
            "Train Epoch: 5 [4000/28539 (14%)]\tLoss: 0.985900\n",
            "Train Epoch: 5 [5000/28539 (18%)]\tLoss: 1.093660\n",
            "Train Epoch: 5 [6000/28539 (21%)]\tLoss: 0.896870\n",
            "Train Epoch: 5 [7000/28539 (25%)]\tLoss: 1.214561\n",
            "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 0.986534\n",
            "Train Epoch: 5 [9000/28539 (32%)]\tLoss: 0.827276\n",
            "Train Epoch: 5 [10000/28539 (35%)]\tLoss: 1.108377\n",
            "Train Epoch: 5 [11000/28539 (39%)]\tLoss: 0.801180\n",
            "Train Epoch: 5 [12000/28539 (42%)]\tLoss: 0.981536\n",
            "Train Epoch: 5 [13000/28539 (46%)]\tLoss: 0.935621\n",
            "Train Epoch: 5 [14000/28539 (49%)]\tLoss: 0.932910\n",
            "Train Epoch: 5 [15000/28539 (53%)]\tLoss: 1.045659\n",
            "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 0.965334\n",
            "Train Epoch: 5 [17000/28539 (60%)]\tLoss: 0.870096\n",
            "Train Epoch: 5 [18000/28539 (63%)]\tLoss: 0.803194\n",
            "Train Epoch: 5 [19000/28539 (67%)]\tLoss: 0.934654\n",
            "Train Epoch: 5 [20000/28539 (70%)]\tLoss: 1.012982\n",
            "Train Epoch: 5 [21000/28539 (74%)]\tLoss: 0.774691\n",
            "Train Epoch: 5 [22000/28539 (77%)]\tLoss: 0.804970\n",
            "Train Epoch: 5 [23000/28539 (81%)]\tLoss: 0.974550\n",
            "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 0.870160\n",
            "Train Epoch: 5 [25000/28539 (88%)]\tLoss: 0.811360\n",
            "Train Epoch: 5 [26000/28539 (91%)]\tLoss: 0.816558\n",
            "Train Epoch: 5 [27000/28539 (95%)]\tLoss: 1.020283\n",
            "Train Epoch: 5 [28000/28539 (98%)]\tLoss: 0.720846\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.6746, Average CER: 0.205260 Average WER: 0.5862\n",
            "\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.4205, Average CER: 0.367468 Average WER: 0.7666\n",
            "\n",
            "Train Epoch: 6 [0/28539 (0%)]\tLoss: 0.678016\n",
            "Train Epoch: 6 [1000/28539 (4%)]\tLoss: 0.875919\n",
            "Train Epoch: 6 [2000/28539 (7%)]\tLoss: 0.942658\n",
            "Train Epoch: 6 [3000/28539 (11%)]\tLoss: 0.855188\n",
            "Train Epoch: 6 [4000/28539 (14%)]\tLoss: 0.911603\n",
            "Train Epoch: 6 [5000/28539 (18%)]\tLoss: 0.759476\n",
            "Train Epoch: 6 [6000/28539 (21%)]\tLoss: 0.943278\n",
            "Train Epoch: 6 [7000/28539 (25%)]\tLoss: 0.874857\n",
            "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 0.724218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c3864cc04b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlibri_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test-clean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-ad154253688b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, epochs, train_url, test_url, experiment)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0miter_meter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvctk_eval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ad154253688b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "92kVVEr7GR6j",
        "colab": {}
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ucfQX3qN21az",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}